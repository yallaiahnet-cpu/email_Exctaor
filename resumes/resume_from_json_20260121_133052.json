{
  "name": "Yallaiah Onteru",
  "title": "Senior AI/ML Engineer - Agentic AI & Multi-Agent Systems Specialist",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bring 10 years of experience across Insurance, Healthcare, Banking, and Consulting domains, combining Agentic AI solutions with AWS Bedrock, Llama models, and multi-agent frameworks to extract cybersecurity insights from unstructured ticketing data.",
    "Design multi-agent systems using LangGraph, AutoGen, and CrewAI to coordinate intelligent workflows that process Hadoop-based unstructured data sources, enabling automated threat detection and incident response in enterprise environments.",
    "Build RAG pipelines integrating NEO4J graph databases with AWS SageMaker and OpenSearch to retrieve contextual cybersecurity information, improving accuracy of LLM-generated recommendations for security operations teams.",
    "Develop MCP client-server architectures connecting agentic workflows to distributed data stores including S3, DynamoDB, and Hadoop clusters, ensuring seamless information flow between AI agents and enterprise data lakes.",
    "Implement prompt engineering strategies and prompt versioning systems to refine LLM outputs for cybersecurity use cases, reducing false positives in threat classification by iteratively testing prompt variations against historical ticket data.",
    "Create hybrid RAG solutions combining vector search with knowledge graphs stored in NEO4J, allowing agents to traverse relationship networks within security incidents and identify root causes faster than manual investigation methods.",
    "Configure AWS Bedrock foundation models including Llama for natural language understanding tasks on security logs, training agents to parse unstructured ticket descriptions and extract indicators of compromise automatically.",
    "Orchestrate agentic RAG workflows where specialized agents handle retrieval, synthesis, and validation tasks independently, coordinating through LangChain to generate comprehensive security incident reports without human intervention.",
    "Integrate DBT with Airflow to schedule data transformation jobs that prepare Hadoop datasets for machine learning model training, ensuring clean input data flows into multi-agent systems processing cybersecurity events.",
    "Deploy Python-based agent frameworks on AWS infrastructure, managing containerized services that scale horizontally to handle increasing volumes of unstructured ticketing data generated by enterprise security monitoring tools.",
    "Construct knowledge retrieval systems using OpenSearch vector databases that store embeddings of historical security incidents, enabling agents to identify similar past events and recommend proven remediation strategies.",
    "Tune Llama model parameters through experimentation on AWS SageMaker notebooks, adjusting temperature and top-k settings to balance creativity and accuracy when agents generate security assessment narratives from raw log data.",
    "Collaborate with cybersecurity analysts to validate agent outputs against known threat patterns, incorporating feedback loops that retrain models and improve classification accuracy for emerging attack vectors over time.",
    "Establish model evaluation frameworks measuring precision and recall of agent-generated security classifications, using confusion matrices to identify weak areas where additional training data or prompt refinement improves performance.",
    "Apply NLP techniques to preprocess unstructured ticketing text, removing noise and normalizing terminology before feeding documents into retrieval systems, ensuring consistent embedding quality for downstream agent reasoning tasks.",
    "Monitor agent performance metrics through custom dashboards tracking response latency and answer relevance, quickly identifying bottlenecks in multi-agent workflows that require optimization or architectural adjustments.",
    "Recommend optimal agent orchestration patterns based on workload characteristics, choosing between sequential and parallel execution strategies depending on data dependencies and desired throughput for cybersecurity insight extraction.",
    "Work under tight project timelines to deliver proof-of-concept agentic systems, iterating rapidly on multi-agent designs and presenting functional prototypes to stakeholders within weeks rather than months of initial requirement gathering."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "AI/ML Frameworks": [
      "Llama",
      "LangChain",
      "LangGraph",
      "AutoGen",
      "CrewAI",
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "XGBoost",
      "LightGBM",
      "H2O"
    ],
    "Agentic AI & Multi-Agent Systems": [
      "MCP (Model Context Protocol)",
      "Multi-Agent Orchestration",
      "Agent-to-Agent Communication",
      "Agentic RAG",
      "Prompt Engineering",
      "Prompt Versioning",
      "LLM Fine-tuning"
    ],
    "AWS AI Services": [
      "AWS Bedrock",
      "AWS SageMaker",
      "AWS Lambda",
      "AWS Glue",
      "Amazon S3",
      "DynamoDB",
      "Amazon Kinesis",
      "AWS EC2",
      "AWS RDS"
    ],
    "Big Data & Distributed Processing": [
      "Hadoop",
      "Apache Spark",
      "PySpark",
      "Apache Kafka",
      "Hive",
      "HBase",
      "Spark Streaming",
      "Databricks",
      "MapReduce"
    ],
    "Retrieval Augmented Generation": [
      "RAG Pipelines",
      "Hybrid RAG",
      "Agentic RAG",
      "Vector Databases",
      "Semantic Search",
      "Embedding Models",
      "Context Retrieval"
    ],
    "Natural Language Processing": [
      "spaCy",
      "NLTK",
      "Hugging Face Transformers",
      "BERT",
      "GPT",
      "TF-IDF",
      "Named Entity Recognition",
      "Text Classification",
      "Sentiment Analysis"
    ],
    "Graph Databases & Knowledge Graphs": [
      "NEO4J",
      "Graph Query Languages",
      "Relationship Mapping",
      "Knowledge Graph Construction",
      "Graph Embeddings"
    ],
    "Search & Indexing": [
      "OpenSearch",
      "Elasticsearch",
      "Vector Search",
      "Full-Text Search",
      "Index Optimization"
    ],
    "Data Orchestration & Transformation": [
      "Apache Airflow",
      "DBT",
      "ETL Pipelines",
      "Data Quality Checks",
      "Workflow Scheduling"
    ],
    "Cloud Platforms": [
      "AWS (Bedrock, SageMaker, S3, DynamoDB, Lambda, EC2, RDS, Glue)",
      "Azure (ML Studio, Data Factory, Databricks, Cosmos DB)"
    ],
    "Databases & Data Stores": [
      "DynamoDB",
      "S3",
      "OpenSearch",
      "PostgreSQL",
      "MySQL",
      "MongoDB",
      "Redis",
      "Snowflake",
      "Cassandra"
    ],
    "Machine Learning Operations": [
      "MLflow",
      "Model Evaluation",
      "Model Validation",
      "A/B Testing",
      "Model Monitoring",
      "Experimentation Tracking"
    ],
    "Containerization & DevOps": [
      "Docker",
      "Kubernetes",
      "Git",
      "GitHub",
      "Jenkins",
      "CI/CD Pipelines",
      "Terraform"
    ],
    "Development Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "Google Colab",
      "Anaconda"
    ],
    "Security & Compliance": [
      "Cybersecurity Domain Knowledge",
      "Security-Aware AI Design",
      "Data Privacy",
      "HIPAA",
      "PCI-DSS",
      "Threat Detection"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas",
      "responsibilities": [
        "Plan multi-agent architectures using LangGraph and CrewAI to process insurance claim tickets stored in Hadoop, defining agent roles for extraction, classification, and recommendation tasks that coordinate through MCP protocols.",
        "Implement agentic RAG systems connecting AWS Bedrock Llama models with NEO4J knowledge graphs, retrieving relevant policy information and historical claim patterns to assist agents in generating accurate insurance fraud assessments.",
        "Configure MCP servers exposing Hadoop data access APIs to agent clients, establishing secure communication channels that allow distributed AI agents to query unstructured claim descriptions and retrieve customer interaction logs.",
        "Deploy PySpark jobs on Databricks to preprocess insurance ticketing data, cleaning text fields and extracting structured attributes that feed into multi-agent workflows processing fraud detection rules and compliance checks.",
        "Establish prompt versioning workflows tracking LLM instruction iterations, testing different prompt templates against labeled insurance claim datasets to identify phrasings that maximize classification accuracy for policy violation detection.",
        "Construct hybrid RAG pipelines combining S3-stored claim documents with NEO4J relationship graphs, enabling agents to traverse connections between policyholders, agents, and claim histories when assessing risk patterns.",
        "Develop proof-of-concept multi-agent systems demonstrating automated claim adjudication, presenting functional prototypes to insurance operations teams and gathering feedback to refine agent coordination logic before production deployment.",
        "Integrate AWS SageMaker model endpoints with LangChain orchestration layers, allowing agents to invoke custom NLP models for entity extraction from claim narratives while maintaining low-latency response times required by real-time processing.",
        "Apply agent-to-agent communication patterns where specialist agents exchange findings, using one agent to extract claim details and another to cross-reference against fraud databases before synthesizing final risk scores.",
        "Tune Llama foundation models through parameter experimentation on AWS Bedrock, adjusting generation settings to balance response creativity with factual accuracy when agents produce claim summary reports for insurance underwriters.",
        "Coordinate with cybersecurity teams to validate agent outputs against known attack patterns targeting insurance systems, incorporating threat intelligence feeds into retrieval databases that agents query during anomaly detection workflows.",
        "Monitor agentic workflows through custom observability dashboards tracking agent execution times and decision accuracy, identifying bottlenecks where slow knowledge retrieval delays multi-agent coordination and overall throughput.",
        "Recommend optimal data partitioning strategies for S3-based claim archives, organizing unstructured documents by claim type and date ranges to accelerate agent retrieval operations during peak processing periods.",
        "Evaluate multi-agent system performance using precision and recall metrics calculated from test claim datasets, iterating on agent prompts and retrieval strategies until classification accuracy meets insurance regulatory requirements.",
        "Troubleshoot DynamoDB connection issues affecting agent state persistence, debugging timeout errors and implementing retry logic that ensures agents successfully save progress when processing long-running claim analysis tasks.",
        "Facilitate code reviews with data engineers to align Airflow DAG schedules with agent execution windows, ensuring upstream data transformations complete before multi-agent systems begin processing newly arrived insurance claim batches."
      ],
      "environment": [
        "Llama",
        "Python",
        "Hadoop",
        "PySpark",
        "MCP",
        "LangGraph",
        "LangChain",
        "AutoGen",
        "CrewAI",
        "AWS Bedrock",
        "AWS SageMaker",
        "AWS S3",
        "DynamoDB",
        "OpenSearch",
        "NEO4J",
        "Databricks",
        "Airflow",
        "DBT",
        "Docker",
        "Kubernetes",
        "RAG Pipelines",
        "Agentic RAG",
        "Hybrid RAG",
        "Prompt Engineering",
        "Prompt Versioning",
        "Multi-Agent Systems",
        "Agent-to-Agent Communication",
        "Proof of Concepts"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey",
      "responsibilities": [
        "Designed multi-agent frameworks using LangChain and AutoGen to analyze healthcare incident tickets, coordinating specialized agents for patient record extraction, HIPAA compliance validation, and adverse event classification tasks.",
        "Built RAG pipelines integrating AWS SageMaker embeddings with OpenSearch indexes, retrieving relevant medical terminology and FDA guidelines to support agent reasoning when generating incident response recommendations.",
        "Configured PySpark data transformations on Databricks to clean unstructured patient safety reports stored in Hadoop, normalizing medical codes and removing PHI before feeding sanitized data into multi-agent processing workflows.",
        "Developed proof-of-concept agentic systems demonstrating automated adverse event detection, presenting functional prototypes to regulatory affairs teams and incorporating feedback to refine agent classification logic for FDA reporting requirements.",
        "Integrated LangGraph orchestration with AWS Bedrock Llama models, defining agent execution graphs that coordinate retrieval, analysis, and synthesis tasks when processing complex medication error investigations requiring cross-referencing multiple data sources.",
        "Constructed hybrid RAG architectures combining S3-stored clinical documentation with NEO4J knowledge graphs mapping drug interactions, allowing agents to traverse relationship networks and identify potential causes of adverse healthcare events.",
        "Applied prompt engineering techniques to optimize LLM instructions for medical terminology understanding, testing prompt variations against labeled adverse event datasets to minimize misclassification of serious versus non-serious incidents.",
        "Established MCP client-server patterns connecting agentic workflows to DynamoDB state stores, ensuring agents reliably persist investigation progress when analyzing lengthy patient case histories that exceed single-session processing limits.",
        "Validated multi-agent outputs by comparing agent-generated incident classifications against expert clinician reviews, calculating confusion matrices to identify weak areas where additional training data improved healthcare event categorization accuracy.",
        "Collaborated with HIPAA compliance officers to audit agent data access patterns, implementing encryption for agent-to-agent communication channels and ensuring all multi-agent workflows met healthcare privacy regulations throughout development.",
        "Optimized OpenSearch query performance by analyzing slow agent retrieval operations, adjusting index mappings and shard configurations to reduce latency when agents search large repositories of historical patient safety incident reports.",
        "Managed Airflow workflows scheduling nightly ETL jobs that extracted healthcare ticketing data from legacy systems, transforming records into formats compatible with multi-agent processing pipelines before loading into S3 staging buckets.",
        "Evaluated agent response quality through manual review sessions with medical safety analysts, gathering qualitative feedback on answer relevance and incorporating findings into prompt refinement cycles that improved incident summary clarity.",
        "Debugged intermittent failures in agent coordination logic where LangGraph execution graphs deadlocked during concurrent knowledge retrieval, resolving race conditions by implementing proper state locking mechanisms in the orchestration layer."
      ],
      "environment": [
        "Python",
        "Hadoop",
        "PySpark",
        "LangChain",
        "LangGraph",
        "AutoGen",
        "AWS SageMaker",
        "AWS Bedrock",
        "AWS S3",
        "DynamoDB",
        "OpenSearch",
        "NEO4J",
        "Databricks",
        "Airflow",
        "Docker",
        "Kubernetes",
        "RAG Pipelines",
        "Hybrid RAG",
        "Prompt Engineering",
        "Multi-Agent Systems",
        "MCP",
        "Llama",
        "Proof of Concepts",
        "HIPAA Compliance"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine",
      "responsibilities": [
        "Architected machine learning pipelines processing Medicaid claims data stored in Azure Data Lake, extracting features from unstructured provider notes and beneficiary correspondence to detect billing anomalies and potential fraud patterns.",
        "Trained gradient boosting models using XGBoost on Azure ML Studio, classifying healthcare claims as legitimate or suspicious based on historical audit outcomes and provider billing history patterns across Maine state healthcare programs.",
        "Processed large-scale healthcare datasets with PySpark on Azure Databricks, aggregating claim line items and joining with provider registry tables to create comprehensive feature sets for fraud detection model training workflows.",
        "Validated model predictions through cross-validation testing against held-out claim samples, achieving acceptable precision-recall tradeoffs that balanced fraud detection rates with false positive volumes acceptable to state auditors and program administrators.",
        "Deployed trained models as REST APIs hosted on Azure Kubernetes Service, exposing prediction endpoints that batch processing jobs invoked nightly to score newly submitted Medicaid claims and flag suspicious submissions for manual review.",
        "Collaborated with state healthcare compliance teams to interpret model outputs, explaining feature importance rankings and demonstrating how specific claim attributes triggered fraud alerts in ways that supported investigator decision-making processes.",
        "Implemented data quality checks in Azure Data Factory pipelines, validating incoming claim feeds for completeness and consistency before allowing records to flow into feature engineering steps that prepared data for model inference.",
        "Optimized model inference latency by profiling prediction code and identifying bottlenecks in feature calculation logic, refactoring Python functions to leverage NumPy vectorization and reducing per-claim scoring time from seconds to milliseconds.",
        "Monitored deployed model performance through Azure Application Insights dashboards, tracking prediction distribution shifts and alert volumes to detect data drift requiring model retraining with updated historical claim examples.",
        "Documented model development decisions in technical specifications shared with HIPAA compliance auditors, describing data anonymization approaches and access controls protecting beneficiary PHI throughout the machine learning lifecycle.",
        "Troubleshot Azure ML pipeline failures caused by schema changes in upstream claim data extracts, coordinating with state IT teams to stabilize data contracts and implement version checks preventing incompatible data from breaking model training jobs.",
        "Participated in code reviews with fellow ML engineers, providing feedback on feature engineering approaches and suggesting alternative encoding strategies for categorical variables like provider specialties and diagnosis codes."
      ],
      "environment": [
        "Python",
        "PySpark",
        "Azure ML Studio",
        "Azure Databricks",
        "Azure Data Lake",
        "Azure Data Factory",
        "Azure Kubernetes Service",
        "XGBoost",
        "Scikit-Learn",
        "NumPy",
        "Pandas",
        "Docker",
        "REST APIs",
        "HIPAA Compliance"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York",
      "responsibilities": [
        "Analyzed transaction patterns in Azure SQL databases to identify credit card fraud indicators, calculating statistical measures like transaction velocity and geographic dispersion that fed into rule-based detection systems protecting customer accounts.",
        "Visualized fraud detection metrics using Power BI dashboards, creating time-series charts showing daily fraud alert volumes and false positive rates that helped operations teams assess detection system effectiveness and adjust thresholds accordingly.",
        "Cleaned transaction datasets using Pandas DataFrames, handling missing merchant category codes and normalizing currency fields to ensure consistent data quality before calculating features for fraud scoring models.",
        "Tested logistic regression models predicting transaction fraud probability, evaluating classification performance on historical labeled datasets and selecting optimal decision thresholds balancing fraud catch rates with customer friction from false declines.",
        "Queried Azure Cosmos DB to retrieve customer profile attributes, joining account opening dates and spending histories with real-time transaction events to enrich fraud detection signals with behavioral context unavailable in transaction records alone.",
        "Reported model performance findings to risk management stakeholders through monthly review meetings, presenting confusion matrices and ROC curves that quantified detection improvements achieved through feature engineering and algorithm changes.",
        "Scheduled ETL workflows using Azure Data Factory to extract transaction logs from legacy banking systems, transforming records into normalized schemas and loading into Azure SQL data warehouses supporting fraud analytics and reporting needs.",
        "Investigated PCI-DSS compliance requirements for handling cardholder data in analytics pipelines, implementing tokenization approaches that replaced sensitive card numbers with surrogate keys while preserving ability to track transaction sequences per account.",
        "Compared clustering algorithms including K-Means for grouping similar transaction patterns, using silhouette scores to select optimal cluster counts and interpreting cluster characteristics to identify distinct fraud attack vectors targeting different customer segments.",
        "Collaborated with Azure infrastructure teams to resolve database connection timeout issues affecting fraud detection batch jobs, adjusting connection pool settings and retry logic to improve pipeline reliability during peak transaction processing periods."
      ],
      "environment": [
        "Python",
        "Pandas",
        "NumPy",
        "Scikit-Learn",
        "Azure SQL",
        "Azure Cosmos DB",
        "Azure Data Factory",
        "Power BI",
        "Logistic Regression",
        "K-Means Clustering",
        "PCI-DSS Compliance"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra",
      "responsibilities": [
        "Extracted data from relational databases using Sqoop, transferring customer transaction records and product catalog tables from on-premises Oracle systems into Hadoop HDFS for downstream analytics and reporting applications.",
        "Loaded cleansed datasets into Hive tables, defining partition schemes by transaction date and customer region to optimize query performance when business analysts ran ad-hoc SQL queries for sales trend analysis and customer segmentation.",
        "Transformed raw data files using Informatica PowerCenter workflows, applying business rules to standardize product codes and aggregate daily transaction summaries before loading into data warehouse fact tables consumed by reporting tools.",
        "Scheduled nightly ETL jobs through Informatica Workflow Manager, monitoring execution logs and setting up email alerts that notified on-call engineers when data loads failed or ran longer than expected completion windows.",
        "Discovered Hadoop ecosystem tools by reading documentation and experimenting with MapReduce sample programs, gradually building proficiency in distributed data processing concepts through hands-on practice with small test datasets.",
        "Assisted senior engineers in debugging Hive query performance issues, learning to interpret query execution plans and suggest index optimizations that reduced runtimes for frequently executed analytical queries.",
        "Documented data pipeline architecture in Confluence pages, creating diagrams showing data flow from source systems through Hadoop processing stages to final consumption by business intelligence tools and reports.",
        "Attended team meetings discussing project requirements with business stakeholders, taking notes on data transformation logic and clarifying questions about source system behavior to ensure accurate implementation of ETL specifications."
      ],
      "environment": [
        "Hadoop",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "Oracle",
        "HDFS",
        "MapReduce",
        "SQL"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}