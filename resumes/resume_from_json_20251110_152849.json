{
  "name": "Yallaiah Onteru",
  "title": "AI/ML Engineer - GenAI ",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "I'm having 10 years of experience in AI/ML engineering, specializing in building scalable machine learning pipelines using Python, AWS SageMaker, and GenAI APIs like OpenAI, Gemini for enterprise solutions.",
    "Architected end-to-end data ingestion and transformation pipelines using Python and AWS services, integrating GitHub Copilot and Cursor to accelerate development cycles while maintaining code quality standards.",
    "Led implementation of Generative AI APIs including OpenAI, Google Gemini, and Hugging Face models, deploying them through AWS SageMaker with Terraform infrastructure provisioning for automated scaling.",
    "Developed CI/CD pipelines using GitHub Actions and Jenkins, incorporating AI-assisted development tools like Windsurf and Amazon Code Whisperer to optimize deployment workflows and reduce manual errors.",
    "Built robust data orchestration systems handling data ingestion, transformation, and pipeline orchestration using Python, AWS RDS, and Redis Cache for real-time processing in insurance applications.",
    "Implemented Infrastructure as Code using Terraform and AWS CloudFormation, managing EC2 instances, ALB configurations, and IAM policies while leveraging AI coding assistants for faster development.",
    "Designed scalable ML workflows on AWS SageMaker, integrating Hugging Face transformers and OpenAI APIs for natural language processing tasks, deployed through automated GitHub Actions pipelines.",
    "Created secure data pipelines with AWS Secret Manager integration, utilizing Redis Cache for performance optimization and RDS for structured data storage in healthcare compliance environments.",
    "Orchestrated complex data transformation workflows using Python frameworks, AWS ALB for load balancing, and implemented monitoring through CloudFormation templates with AI-assisted debugging tools.",
    "Deployed production-ready GenAI solutions using Google Gemini and OpenAI APIs, managed through Jenkins CI/CD pipelines with Terraform infrastructure automation for banking regulatory compliance.",
    "Established MLOps practices using AWS SageMaker pipelines, GitHub Actions for continuous integration, and leveraged GitHub Copilot to enhance team productivity in model development cycles.",
    "Built data ingestion frameworks processing terabytes of data using Python, AWS EC2 compute clusters, and implemented caching strategies with Redis for optimized pipeline orchestration performance.",
    "Integrated multiple Generative AI APIs including Hugging Face models into production systems, utilizing AWS IAM for secure access control and CloudFormation for reproducible deployments.",
    "Developed automated testing frameworks with Jenkins and GitHub Actions, incorporating Cursor and Windsurf AI tools to generate comprehensive test coverage for data transformation pipelines.",
    "Implemented real-time data orchestration using Python async frameworks, AWS RDS for persistence, and ALB for distributing workloads across multiple SageMaker endpoints serving GenAI models.",
    "Created infrastructure monitoring dashboards using Terraform modules, tracking AWS resource utilization for EC2, RDS, and SageMaker instances while optimizing costs through automated scaling policies.",
    "Established best practices for AI-assisted development using GitHub Copilot, Amazon Code Whisperer, and Cursor, training teams on leveraging these tools for Python ML pipeline development.",
    "Built resilient data transformation services using AWS Secret Manager for credential management, Redis Cache for session handling, and implemented disaster recovery through CloudFormation templates."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "SQL",
      "Bash/Shell",
      "JavaScript",
      "TypeScript",
      "R",
      "Java"
    ],
    "Generative AI & LLM APIs": [
      "OpenAI GPT",
      "Google Gemini",
      "Hugging Face",
      "Claude API",
      "Anthropic",
      "LangChain",
      "LlamaIndex",
      "OpenAI Assistants API",
      "Gemini Pro",
      "Hugging Face Inference API"
    ],
    "AI-Assisted Development Tools": [
      "GitHub Copilot",
      "Cursor",
      "Windsurf",
      "Amazon Code Whisperer",
      "Tabnine",
      "Codeium",
      "Replit AI",
      "CodeGPT",
      "Sourcegraph Cody"
    ],
    "AWS Cloud Services": [
      "AWS SageMaker",
      "EC2",
      "RDS",
      "ALB (Application Load Balancer)",
      "Redis Cache",
      "Secret Manager",
      "IAM",
      "Lambda",
      "S3",
      "CloudWatch",
      "ECS",
      "EKS"
    ],
    "Infrastructure as Code": [
      "Terraform",
      "AWS CloudFormation",
      "Terraform Cloud",
      "Terragrunt",
      "AWS CDK",
      "Pulumi",
      "Ansible",
      "Chef",
      "CloudFormation StackSets"
    ],
    "CI/CD & DevOps": [
      "GitHub Actions",
      "Jenkins",
      "GitLab CI",
      "CircleCI",
      "Travis CI",
      "ArgoCD",
      "Spinnaker",
      "TeamCity",
      "Bamboo",
      "Azure DevOps"
    ],
    "Data Pipeline & Orchestration": [
      "Data Ingestion",
      "Data Transformation",
      "Pipeline Orchestration",
      "Apache Airflow",
      "Apache Kafka",
      "AWS Glue",
      "Dagster",
      "Prefect",
      "Luigi",
      "Apache NiFi"
    ],
    "Machine Learning Frameworks": [
      "AWS SageMaker",
      "TensorFlow",
      "PyTorch",
      "Scikit-learn",
      "XGBoost",
      "LightGBM",
      "Keras",
      "JAX",
      "MLflow",
      "Kubeflow"
    ],
    "Database & Storage": [
      "AWS RDS",
      "Redis",
      "PostgreSQL",
      "MySQL",
      "MongoDB",
      "DynamoDB",
      "ElastiCache",
      "Aurora",
      "Redshift",
      "Cassandra"
    ],
    "Monitoring & Security": [
      "AWS IAM",
      "AWS Secret Manager",
      "CloudWatch",
      "Datadog",
      "Prometheus",
      "Grafana",
      "AWS Security Hub",
      "AWS KMS",
      "HashiCorp Vault"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes",
      "AWS ECS",
      "AWS EKS",
      "Fargate",
      "Helm",
      "Docker Compose",
      "Rancher",
      "OpenShift"
    ],
    "Version Control & Collaboration": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "AWS CodeCommit",
      "Perforce",
      "SVN",
      "Mercurial",
      "Azure Repos"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Architected multi-agent systems using CrewAI and LangGraph frameworks with Python, implementing Model Context Protocol for agent-to-agent communication in insurance claim processing workflows.",
        "Built Generative AI APIs integration layer using OpenAI, Google Gemini, and Hugging Face models, deploying through AWS SageMaker endpoints with Terraform infrastructure for automated scaling.",
        "Implemented CI/CD pipelines using GitHub Actions and Jenkins, leveraging GitHub Copilot and Cursor to accelerate development of Python-based data transformation modules for insurance analytics.",
        "Developed proof-of-concept systems using multi-agent frameworks, orchestrating data ingestion and transformation pipelines with AWS RDS for persistence and Redis Cache for performance optimization.",
        "Created Infrastructure as Code templates using Terraform and CloudFormation, managing EC2 instances, ALB configurations, and IAM policies for secure multi-agent system deployments in production.",
        "Integrated AI-assisted development tools including Windsurf and Amazon Code Whisperer into team workflows, improving Python code quality and reducing debugging time for pipeline orchestration.",
        "Built agent-to-agent communication protocols using Google's multi-agent frameworks, implementing data transformation logic with AWS Secret Manager for secure credential handling in insurance systems.",
        "Orchestrated complex ML workflows on AWS SageMaker, utilizing Hugging Face transformers for document processing while maintaining HIPAA compliance through proper IAM role configurations.",
        "Deployed production GenAI solutions using OpenAI and Gemini APIs, managed through Jenkins pipelines with automated testing using GitHub Actions for continuous integration of insurance models.",
        "Implemented data ingestion frameworks processing insurance claims data, using Python async libraries with AWS ALB for load distribution across multiple SageMaker inference endpoints.",
        "Established MLOps practices using CrewAI for autonomous agent deployment, leveraging GitHub Copilot to generate boilerplate code for data transformation pipelines in insurance fraud detection.",
        "Created resilient pipeline orchestration using LangGraph, implementing retry mechanisms with Redis Cache for state management and RDS for transaction logging in regulatory compliance systems.",
        "Built automated monitoring for multi-agent systems using CloudFormation templates, tracking AWS resource utilization for EC2 clusters running proof-of-concept GenAI insurance applications.",
        "Developed secure API gateways for Hugging Face model serving, implementing AWS IAM policies and Secret Manager integration for protecting sensitive insurance customer data in pipelines.",
        "Integrated Model Context Protocol into existing Python frameworks, enabling seamless communication between CrewAI agents and legacy systems through data transformation middleware layers.",
        "Optimized infrastructure costs using Terraform modules, implementing auto-scaling policies for AWS SageMaker endpoints serving OpenAI and Gemini models based on insurance claim processing loads."
      ],
      "environment": [
        "Python",
        "CrewAI",
        "LangGraph",
        "Model Context Protocol",
        "Multi-Agent Systems",
        "OpenAI API",
        "Google Gemini",
        "Hugging Face",
        "AWS SageMaker",
        "GitHub Copilot",
        "Cursor",
        "Windsurf",
        "Amazon Code Whisperer",
        "GitHub Actions",
        "Jenkins",
        "Terraform",
        "AWS CloudFormation",
        "EC2",
        "RDS",
        "ALB",
        "Redis Cache",
        "Secret Manager",
        "IAM",
        "Data Ingestion",
        "Data Transformation",
        "Pipeline Orchestration"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Architected healthcare data pipelines using Python and AWS SageMaker, integrating OpenAI APIs for medical document analysis while maintaining HIPAA compliance through proper encryption methods.",
        "Implemented CrewAI multi-agent frameworks for clinical trial data processing, utilizing Terraform to provision AWS infrastructure including EC2 instances and RDS databases for secure storage.",
        "Developed proof-of-concept systems using LangGraph for patient data transformation, leveraging GitHub Copilot and Cursor to accelerate Python development for healthcare regulatory workflows.",
        "Built CI/CD pipelines with GitHub Actions and Jenkins, automating deployment of Hugging Face models to AWS SageMaker endpoints for medical image classification in diagnostic systems.",
        "Created data ingestion frameworks processing HIPAA-compliant healthcare records, using AWS ALB for load balancing and Redis Cache for session management in real-time patient monitoring.",
        "Integrated Google Gemini APIs for natural language processing of medical reports, implementing AWS Secret Manager for secure API key storage and IAM policies for access control.",
        "Orchestrated complex data transformation workflows using Python, deploying Infrastructure as Code with CloudFormation templates for reproducible healthcare data processing environments.",
        "Established multi-agent systems for drug discovery pipelines, using CrewAI agents to coordinate between data ingestion, transformation, and analysis phases while ensuring FDA compliance.",
        "Deployed GenAI solutions using OpenAI and Hugging Face models, managed through Jenkins pipelines with automated testing frameworks built using Amazon Code Whisperer assistance.",
        "Implemented pipeline orchestration for clinical trial data using Python async patterns, AWS RDS for structured storage, and Terraform modules for infrastructure management across environments.",
        "Built secure API gateways for healthcare AI services, utilizing AWS IAM for fine-grained access control and implementing data transformation logic for FHIR standard compliance.",
        "Developed monitoring dashboards for ML pipelines using CloudFormation, tracking SageMaker endpoint performance and implementing auto-scaling based on healthcare system load patterns.",
        "Created resilient data processing systems using Redis Cache for state management, implementing retry logic for failed transformations and maintaining audit trails in RDS databases.",
        "Optimized infrastructure costs using Terraform, implementing spot instance strategies for EC2 clusters running proof-of-concept GenAI models while maintaining healthcare service availability."
      ],
      "environment": [
        "Python",
        "CrewAI",
        "LangGraph",
        "Multi-Agent Systems",
        "Proof of Concepts",
        "OpenAI API",
        "Google Gemini",
        "Hugging Face",
        "AWS SageMaker",
        "GitHub Copilot",
        "Cursor",
        "Amazon Code Whisperer",
        "GitHub Actions",
        "Jenkins",
        "Terraform",
        "AWS CloudFormation",
        "EC2",
        "RDS",
        "ALB",
        "Redis Cache",
        "Secret Manager",
        "IAM",
        "Data Ingestion",
        "Data Transformation",
        "Pipeline Orchestration",
        "HIPAA Compliance"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Engineered healthcare data pipelines using Python and Azure integration, implementing Terraform scripts for infrastructure provisioning while maintaining HIPAA compliance standards.",
        "Deployed Generative AI APIs including OpenAI and Hugging Face models, creating data transformation modules that processed patient records through secure pipeline orchestration frameworks.",
        "Implemented CI/CD workflows using GitHub Actions, automating deployment of machine learning models with Jenkins while leveraging AI-assisted development tools for code optimization.",
        "Built data ingestion systems processing state healthcare databases, utilizing Azure services integrated with AWS-compatible tools through Terraform infrastructure management solutions.",
        "Created secure data transformation pipelines using Python, implementing Redis Cache for performance optimization and maintaining audit trails for regulatory compliance requirements.",
        "Developed Infrastructure as Code templates using Terraform and CloudFormation principles, managing cloud resources efficiently while ensuring healthcare data privacy standards.",
        "Orchestrated ML workflows integrating Google Gemini APIs for natural language processing of medical documents, deploying through automated CI/CD pipelines with comprehensive testing.",
        "Established data orchestration frameworks using Python async patterns, implementing retry mechanisms and error handling for resilient healthcare data processing systems.",
        "Integrated Hugging Face transformers for medical text analysis, deploying models through secure endpoints with proper IAM configurations and Secret Manager integration.",
        "Built monitoring solutions for pipeline performance tracking, implementing alerting mechanisms for data transformation failures and resource utilization optimization.",
        "Developed API gateways for healthcare AI services, ensuring secure communication between systems while maintaining HIPAA-compliant data transformation processes.",
        "Optimized infrastructure costs through efficient resource allocation, implementing auto-scaling policies and spot instance strategies for cost-effective ML pipeline execution."
      ],
      "environment": [
        "Python",
        "Azure",
        "OpenAI API",
        "Google Gemini",
        "Hugging Face",
        "GitHub Actions",
        "Jenkins",
        "Terraform",
        "CloudFormation",
        "Redis Cache",
        "Data Ingestion",
        "Data Transformation",
        "Pipeline Orchestration",
        "IAM",
        "Secret Manager",
        "HIPAA Compliance"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Developed fraud detection pipelines using Python and Azure ML services, implementing data transformation logic for real-time transaction monitoring in banking systems.",
        "Built data ingestion frameworks processing financial transactions, utilizing Terraform for infrastructure provisioning and maintaining PCI compliance through secure configurations.",
        "Implemented machine learning models using OpenAI APIs for customer sentiment analysis, deploying through CI/CD pipelines with GitHub Actions and Jenkins automation.",
        "Created pipeline orchestration systems using Python, integrating Redis Cache for session management and implementing secure data transformation for regulatory reporting.",
        "Engineered Infrastructure as Code templates with Terraform, managing Azure resources while ensuring banking security standards and regulatory compliance requirements.",
        "Deployed Hugging Face models for document classification, implementing secure API endpoints with proper authentication and authorization mechanisms for financial data.",
        "Established data transformation workflows for credit risk assessment, utilizing Google Gemini APIs for natural language processing of financial documents and reports.",
        "Developed monitoring dashboards for ML pipeline performance, tracking model accuracy and implementing automated retraining workflows based on drift detection.",
        "Integrated secure credential management using Secret Manager principles, ensuring protection of sensitive banking data throughout the transformation pipeline.",
        "Optimized data processing workflows using Python async patterns, implementing efficient batch processing for large-scale financial transaction analysis systems."
      ],
      "environment": [
        "Python",
        "Azure",
        "OpenAI API",
        "Google Gemini",
        "Hugging Face",
        "GitHub Actions",
        "Jenkins",
        "Terraform",
        "Redis Cache",
        "Data Ingestion",
        "Data Transformation",
        "Pipeline Orchestration",
        "Secret Manager",
        "PCI Compliance"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Collaborated on data ingestion pipelines using Hadoop and Informatica, learning to integrate Python scripts for automated data transformation in consulting projects.",
        "Assisted in implementing Sqoop-based data migration workflows, incorporating basic CI/CD practices with Jenkins while exploring Infrastructure as Code concepts.",
        "Supported development of data transformation modules using Informatica PowerCenter, working with senior engineers to understand pipeline orchestration best practices.",
        "Participated in building ETL workflows with Hadoop ecosystem tools, gaining experience in handling large-scale data ingestion and transformation requirements.",
        "Learned to implement data quality checks using Python scripts, integrating with Informatica workflows for comprehensive data validation in consulting engagements.",
        "Contributed to infrastructure setup using basic Terraform scripts, understanding cloud resource provisioning while maintaining documentation for team reference.",
        "Worked on optimizing Sqoop import jobs for faster data ingestion, implementing parallel processing techniques learned through hands-on consulting project experience.",
        "Developed initial understanding of pipeline orchestration concepts, creating simple Python automation scripts for workflow scheduling and monitoring tasks."
      ],
      "environment": [
        "Hadoop",
        "Informatica",
        "Sqoop",
        "Python",
        "Jenkins",
        "Terraform",
        "Data Ingestion",
        "Data Transformation",
        "Pipeline Orchestration"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Certified Salesforce Developer PD1"
  ]
}