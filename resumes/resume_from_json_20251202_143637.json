{
  "name": "Shivaleela Uppula",
  "title": "Senior Streaming Data Engineer - Healthcare & Financial Domains",
  "contact": {
    "email": "shivaleelauppula@gmail.com",
    "phone": "+12244420531",
    "portfolio": "",
    "linkedin": "https://linkedin.com/in/shivaleela-uppula",
    "github": ""
  },
  "professional_summary": [
    "I am having 10 years of experience in architecting and implementing large-scale, real-time data pipelines using Databricks, Apache Spark, and streaming technologies across regulated healthcare, insurance, government, and financial domains.",
    "Leveraging Databricks and Apache Spark to build a unified healthcare data platform that ingested real-time patient monitoring data from Kafka, achieving HIPAA-compliant streaming enrichment with exactly-once semantics for critical ML feature generation.",
    "Orchestrating complex batch and streaming pipelines using Airflow and Databricks Workflows for Blue Cross Blue Shield, ensuring SLA-driven operations with robust monitoring, alerting dashboards, and root-cause analysis for insurance claim processing systems.",
    "Implementing stateful stream processing with mapGroupsWithState to handle real-time Medicaid eligibility checks for the State of Arizona, incorporating watermarking and checkpointing in Structured Streaming to manage late-arriving government data.",
    "Designing and troubleshooting a low-latency, ACID-compliant transaction pipeline on Delta Lake for Discover Financial Services to process real-time credit card transactions, enforcing schema evolution and data quality validation for PCI-DSS compliance.",
    "Developing feature generation pipelines for ML systems by applying applyInPandasWithState on streaming healthcare data at Medline, creating real-time predictive features for medical supply demand forecasting within a GenAI-assisted development environment.",
    "Architecting a multi-agent proof-of-concept using CrewAI and LangGraph to automate pipeline monitoring and incident response, integrating with Databricks Assistant and GitHub Copilot to accelerate development and documentation for healthcare analytics.",
    "Configuring pipeline observability using Azure Monitor and Datadog for enterprise streaming systems, building operational dashboards that tracked metrics, cost optimization of Databricks compute, and triggered alerts for SLA breaches in insurance domains.",
    "Engineering a Delta Live Tables framework to structure complex batch and streaming pipelines, implementing schema enforcement and data quality checks to ensure reliable ingestion of real-time financial data feeds into feature stores.",
    "Leading the CI/CD implementation for data pipelines using dbx and GitHub Actions, establishing Git branching discipline and automated deployment patterns for Databricks jobs across development, staging, and production environments in Azure.",
    "Optimizing Spark job performance through shuffle reduction, strategic caching, and partition tuning to meet strict latency SLAs for real-time government dashboards, while conducting code reviews and debugging sessions to resolve data skew issues.",
    "Documenting comprehensive runbooks and incident processes for 24/7 streaming systems, detailing troubleshooting steps for Kafka consumer lags, Delta Lake merge conflicts, and Airflow DAG failures to support on-call rotations.",
    "Constructing streaming enrichment pipelines with exactly-once semantics using Kafka and EventHub, ensuring no duplicate or missing records in real-time insurance policy updates and maintaining transaction integrity for audit trails.",
    "Integrating streaming technologies like Kinesis and EventHub with Databricks to process high-volume healthcare device telemetry, implementing Structured Streaming with checkpointing to guarantee state recovery and pipeline resilience.",
    "Applying advanced SQL within Spark SQL to transform and aggregate streaming data for real-time operational dashboards, enabling government agencies to monitor public health metrics with sub-second latency on refreshed data models.",
    "Collaborating with data scientists to productionize ML models by building streaming inference pipelines that scored live financial transaction streams, ensuring low-latency predictions while maintaining model version lineage and governance.",
    "Mentoring junior engineers on streaming architecture patterns, Delta Lake best practices, and performance tuning techniques, sharing lessons from debugging late-night pipeline failures and designing robust retry logic for stateful processing.",
    "Adapting pipeline designs to comply with domain-specific regulations including HIPAA, GDPR, and PCI-DSS, implementing encryption, access logging, and audit controls within Databricks notebooks and Spark configurations for secure data handling."
  ],
  "technical_skills": {
    "Cloud Data Platforms & Orchestration": [
      "Databricks",
      "Azure Databricks",
      "Databricks Workflows",
      "Delta Lake",
      "Delta Live Tables",
      "Unity Catalog",
      "Apache Airflow",
      "dbx"
    ],
    "Streaming & Real-Time Processing": [
      "Apache Spark Structured Streaming",
      "Stateful Processing (mapGroupsWithState)",
      "Apache Kafka",
      "Azure EventHub",
      "Amazon Kinesis",
      "Exactly-Once Semantics",
      "Checkpointing",
      "Watermarking"
    ],
    "Big Data & Distributed Computing": [
      "Apache Spark",
      "PySpark",
      "Spark SQL",
      "Batch Pipeline Development",
      "Streaming Pipeline Development",
      "Performance Tuning",
      "Shuffle Optimization",
      "Partitioning"
    ],
    "Cloud Services (Azure)": [
      "Azure Cloud",
      "Azure Monitor",
      "Azure DevOps",
      "Azure Data Lake Storage",
      "Azure Key Vault",
      "Azure Active Directory"
    ],
    "Cloud Services (AWS)": [
      "AWS Cloud",
      "Amazon S3",
      "AWS IAM",
      "CloudWatch",
      "AWS Lambda",
      "Amazon MSK"
    ],
    "Data Engineering & ETL": [
      "Advanced SQL",
      "Feature Generation for ML",
      "Schema Evolution",
      "ACID Transactions",
      "Data Quality Validation",
      "Data Modeling",
      "CI/CD for Data Pipelines"
    ],
    "MLOps & AI Integration": [
      "Feature Store Concepts",
      "Streaming ML Inference",
      "GenAI-Assisted Development",
      "Cursor",
      "Windsurf",
      "GitHub Copilot",
      "Databricks Assistant",
      "Model Deployment"
    ],
    "Monitoring & Observability": [
      "Pipeline Monitoring",
      "Operational Dashboards",
      "Alerting Systems",
      "Root-Cause Analysis",
      "Logging Frameworks",
      "Datadog",
      "SLA Management"
    ],
    "Development Tools & Practices": [
      "Git",
      "GitHub Actions",
      "Terraform",
      "Infrastructure-as-Code",
      "Runbooks",
      "Incident Processes",
      "Documentation",
      "Code Review"
    ],
    "Programming & Scripting": [
      "Python",
      "Scala",
      "SQL",
      "Bash/Shell",
      "PySpark API",
      "Pandas",
      "Spark DataFrame API"
    ]
  },
  "experience": [
    {
      "role": "Senior Data Engineer-AI/ML with Gen AI",
      "client": "Medline Industries",
      "duration": "2023-Dec - Present",
      "location": "Illinois",
      "responsibilities": [
        "Architected a HIPAA-compliant real-time data platform on Databricks using Delta Lake and Structured Streaming to ingest patient supply chain events from Kafka, implementing exactly-once semantics to ensure accurate inventory tracking across healthcare facilities.",
        "Engineered stateful stream processing pipelines with mapGroupsWithState to compute rolling inventory levels and generate ML features for demand forecasting, tackling initial state management challenges through iterative debugging and checkpointing strategies.",
        "Orchestrated complex DAGs using Databricks Workflows and Airflow to coordinate batch historical loads with streaming real-time data, designing workflow dependencies that met strict SLAs for hospital replenishment reports and operational dashboards.",
        "Developed a proof-of-concept multi-agent system using CrewAI and LangGraph to autonomously monitor pipeline health, where agents collaborated to diagnose Kafka lag incidents and generate Databricks notebook fixes, reducing initial response time.",
        "Implemented schema evolution and enforcement on Delta Live Tables managing streaming healthcare datasets, carefully adding new columns for diagnostic codes without breaking downstream BI reports consumed by clinical stakeholders.",
        "Utilized Databricks Assistant and GitHub Copilot extensively during development to accelerate writing PySpark UDFs for PHI data masking, while conducting peer code reviews to ensure logic matched complex business rules for medical device tracking.",
        "Constructed feature generation pipelines that consumed real-time streaming data from EventHub, applying applyInPandasWithState to create time-windowed aggregates for ML models predicting regional supply shortages, integrating results into a feature store.",
        "Configured comprehensive pipeline observability with Azure Monitor and custom Databricks dashboards, setting up alerts for watermark staleness and SLA breaches, which involved late-night troubleshooting of a sudden EventHub throughput spike.",
        "Led the CI/CD migration for all Databricks pipelines using dbx and GitHub Actions, establishing Git feature branch discipline and automated deployment rings, which required resolving several tricky library dependency conflicts in the staging environment.",
        "Optimized performance of critical Spark jobs by analyzing UI stages, implementing broadcast joins and salting to reduce shuffle skew in large healthcare provider dimension tables, achieving required sub-minute latency for streaming enrichment.",
        "Authored detailed runbooks and incident response playbooks for the new streaming platform, documenting steps to handle Delta merge conflicts and Kafka consumer rebalancing, which we tested during a chaotic but informative simulated outage drill.",
        "Designed and deployed a GenAI-assisted documentation system using a custom agent with Model Context Protocol, where it parsed pipeline logs and auto-generated troubleshooting sections, saving the team hours each week on manual updates.",
        "Spearheaded cost optimization initiatives for Databricks compute by analyzing cluster usage patterns, rightsizing instance types, and implementing auto-termination policies, presenting findings and trade-offs to healthcare finance leadership.",
        "Integrated streaming data quality validation frameworks that performed continuous checks on incoming HIPAA-protected data, raising alerts for anomalous null rates in critical patient identifier fields and triggering quarantine workflows for review.",
        "Mentored two junior engineers on streaming architecture and Delta Lake best practices, pairing with them to debug a perplexing watermark advancement issue that turned out to be a misconfigured event time column in the source Kafka topic.",
        "Collaborated with security teams to embed granular access controls and audit logging into all Databricks notebooks and job configurations, ensuring the real-time platform complied with stringent healthcare data governance and HIPAA regulations."
      ],
      "environment": [
        "Databricks",
        "Apache Spark",
        "Delta Lake",
        "Kafka",
        "Structured Streaming",
        "mapGroupsWithState",
        "Azure",
        "Airflow",
        "Databricks Workflows",
        "Delta Live Tables",
        "Exactly-Once Semantics",
        "Azure EventHub",
        "CrewAI",
        "LangGraph",
        "GitHub Copilot",
        "Databricks Assistant",
        "CI/CD",
        "dbx",
        "Azure Monitor",
        "Checkpointing",
        "Watermarking",
        "Schema Evolution",
        "ACID Transactions",
        "SLA",
        "Alerting",
        "Runbooks",
        "Performance Tuning",
        "Feature Generation"
      ]
    },
    {
      "role": "Senior Data Engineer",
      "client": "Blue Cross Blue Shield Association",
      "duration": "2022-Sep - 2023-Nov",
      "location": "St. Louis",
      "responsibilities": [
        "Built a real-time claims adjudication pipeline using Databricks and Spark Structured Streaming from Kafka, implementing checkpointing and watermarking to handle delayed insurance eligibility updates and ensure processing completeness.",
        "Developed batch and streaming feature pipelines for ML models predicting claim fraud, leveraging Delta Lake's ACID transactions to reliably update features in place while maintaining audit trails required for insurance regulatory compliance.",
        "Established pipeline monitoring and observability using Datadog dashboards and custom metrics, configuring alerts for SLA misses on claim processing times and leading the root-cause analysis for a major weekend degradation incident.",
        "Created orchestrated workflows with Airflow to manage dependencies between batch member enrollment data and streaming claim events, designing resilient DAGs with retry logic and task sensors that waited for upstream file arrivals in Azure Data Lake.",
        "Applied advanced Spark SQL to perform complex aggregations on streaming healthcare data for real-time provider dashboards, optimizing queries with partition pruning on Delta tables to meet performance targets for concurrent user loads.",
        "Implemented a data quality validation framework for continuous ingestion of policyholder data, writing PySpark tests that checked for referential integrity and business rule compliance, quarantining problematic records for manual review by insurance analysts.",
        "Utilized Delta Lake's schema enforcement and evolution capabilities to safely add new insurance plan attributes to existing streaming datasets without disrupting downstream reporting applications used by actuarial teams.",
        "Configured and troubleshot a stateful streaming job using applyInPandasWithState to calculate rolling member deductibles, overcoming initial hurdles with state serialization and debugging out-of-memory errors during peak claim volumes.",
        "Participated in on-call rotations for critical streaming systems, documenting incident responses in runbooks and refining alert thresholds after a false positive storm caused by a holiday-related dip in claim submission volume.",
        "Championed the adoption of GitHub Copilot within the data engineering team, demonstrating its use to generate boilerplate code for new Delta merge operations and helping teammates overcome initial skepticism about the tool's utility.",
        "Integrated the pipeline with Azure Monitor logging, ensuring all Spark driver and executor logs were captured for debugging and that cost attribution tags were applied to all Databricks clusters for departmental chargeback reporting.",
        "Assisted in designing a feature store integration pattern for machine learning teams, prototyping the generation of real-time features from streaming claim data and establishing versioning practices for model training and inference.",
        "Conducted performance tuning sessions on several Spark streaming jobs, identifying and resolving a data skew issue in a large join with static provider tables by implementing a broadcast hint and salting the streaming key.",
        "Collaborated with enterprise architecture to define governance standards for the new Databricks environment, contributing to policies on notebook naming, cluster tagging, and the use of Unity Catalog for managing insurance data assets."
      ],
      "environment": [
        "Databricks",
        "Apache Spark",
        "Structured Streaming",
        "Delta Lake",
        "Kafka",
        "Azure",
        "Airflow",
        "Checkpointing",
        "Watermarking",
        "ACID Transactions",
        "Data Quality Validation",
        "Schema Enforcement",
        "applyInPandasWithState",
        "Pipeline Monitoring",
        "Datadog",
        "SLA",
        "Alerting",
        "Root-Cause Analysis",
        "Spark SQL",
        "Performance Tuning",
        "Feature Generation",
        "GitHub Copilot",
        "Azure Monitor"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "State of Arizona",
      "duration": "2020-Apr - 2022-Aug",
      "location": "Arizona",
      "responsibilities": [
        "Designed and implemented a streaming data pipeline on AWS using Kinesis and Spark Streaming to process real-time public health data for COVID-19 dashboarding, ensuring timely ingestion and aggregation for government decision-makers.",
        "Constructed batch data pipelines with Apache Spark on Databricks to consolidate historical Medicaid enrollment data from disparate legacy systems, performing complex joins and transformations to create a unified member golden record.",
        "Developed operational dashboards in Databricks to monitor pipeline health and data quality metrics for government programs, setting up CloudWatch alerts for job failures and data freshness breaches against mandated SLAs.",
        "Engineered a Delta Lake architecture on S3 to serve as the curated data layer for analytics, implementing schema evolution strategies to accommodate changing data formats from various state agency source systems over time.",
        "Orchestrated pipeline executions using Apache Airflow, building DAGs that coordinated weekly batch loads with daily incremental updates and managed dependencies between data preparation jobs and downstream tableau extracts.",
        "Wrote advanced SQL queries and PySpark transformations to calculate eligibility metrics and program utilization rates, optimizing logic to handle large volumes of historical data within constrained AWS batch processing windows.",
        "Performed troubleshooting and performance tuning on Spark jobs processing census and demographic data, resolving a persistent memory overhead issue by switching to lighter-weight data structures and adjusting executor configurations.",
        "Implemented a logging framework using CloudWatch Logs and custom metrics to track pipeline execution times and data record counts, creating visibility for managers and supporting audits for federal reporting requirements.",
        "Documented data lineage and pipeline designs for government audit compliance, creating runbooks that outlined recovery procedures for common failure scenarios like source file format changes or temporary network outages.",
        "Collaborated with other state agency teams to integrate new data sources, participating in lengthy requirement meetings to understand complex business rules for social service program eligibility and data sharing agreements.",
        "Assisted in the migration of several on-premise SSIS ETL packages to cloud-native Spark jobs on Databricks, mapping legacy transformation logic and ensuring parity validation for critical financial allocation calculations.",
        "Supported the development of real-time dashboards for emergency response teams by building streaming aggregations of 911 call data, working to reduce end-to-end latency to meet the stringent requirements for situational awareness."
      ],
      "environment": [
        "AWS",
        "Apache Spark",
        "Databricks",
        "Spark Streaming",
        "Amazon Kinesis",
        "Delta Lake",
        "S3",
        "Apache Airflow",
        "Advanced SQL",
        "Batch Pipeline",
        "Streaming Pipeline",
        "Pipeline Monitoring",
        "CloudWatch",
        "SLA",
        "Alerting",
        "Performance Tuning",
        "Troubleshooting",
        "Documentation"
      ]
    },
    {
      "role": "Big Data Engineer",
      "client": "Discover Financial Services",
      "duration": "2018-Jan - 2020-Mar",
      "location": "Houston, Texas",
      "responsibilities": [
        "Developed batch data pipelines using Apache Spark on AWS EMR to process daily credit card transaction data for fraud detection models, transforming and enriching raw records into features suitable for machine learning consumption.",
        "Built a real-time streaming prototype using Kinesis and Spark Streaming to score transactions for potential fraud, implementing basic watermarking to handle out-of-order events and exploring stateful aggregation for customer spending patterns.",
        "Participated in designing the data architecture for a new customer analytics platform, advocating for the use of Delta Lake format on S3 to provide ACID transaction guarantees and reliable time travel for financial audit trails.",
        "Wrote and optimized complex SQL queries to generate aggregated financial reports and customer segmentation datasets, ensuring calculations adhered to strict PCI-DSS compliance standards for handling sensitive cardholder data.",
        "Assisted in setting up initial pipeline monitoring using CloudWatch alarms and custom dashboards, tracking job success rates and data volumes to identify trends and potential issues before they impacted downstream reporting schedules.",
        "Conducted performance analysis on Spark jobs, identifying and resolving a skew issue in a large join between transaction and merchant data by implementing salting techniques, which improved cluster utilization and reduced runtimes.",
        "Collaborated with data scientists to productionize fraud detection models, building the batch inference pipeline that scored transactions and delivered results to the operational systems with required low latency.",
        "Documented data lineage and transformation logic for key financial metrics, contributing to the enterprise data dictionary and ensuring clarity for audit and compliance reviews of the new big data platform.",
        "Supported the migration of several legacy Teradata workloads to the new Spark-based platform, carefully validating output to ensure financial reporting accuracy and reconciling discrepancies with business analysts.",
        "Participated in an on-call rotation for the production data platform, responding to alerts, troubleshooting failed jobs, and escalating issues related to data quality or infrastructure problems following established incident processes."
      ],
      "environment": [
        "AWS",
        "Apache Spark",
        "EMR",
        "Spark Streaming",
        "Kinesis",
        "Batch Pipeline",
        "Streaming Pipeline",
        "Advanced SQL",
        "Delta Lake",
        "ACID Transactions",
        "Pipeline Monitoring",
        "CloudWatch",
        "Performance Tuning",
        "Troubleshooting",
        "PCI-DSS"
      ]
    },
    {
      "role": "Data Analyst",
      "client": "Sig Tuple",
      "duration": "2015-May - 2017-Nov",
      "location": "Bengaluru, India",
      "responsibilities": [
        "Analyzed healthcare diagnostic image metadata and patient records using SQL and Python, writing queries to extract, transform, and load data into analysis-ready formats for medical research and product development teams.",
        "Developed scripts to automate the validation and cleansing of incoming lab test data, checking for completeness and consistency against defined business rules to ensure high-quality inputs for AI model training in a healthcare context.",
        "Created interactive dashboards and reports using Python visualization libraries and Power BI to track key metrics related to diagnostic accuracy and operational throughput, presenting findings to clinical and engineering stakeholders.",
        "Assisted in building foundational data pipelines that consolidated data from various source systems including Oracle and PostgreSQL databases, learning the importance of data governance and auditability in a regulated healthcare environment.",
        "Documented data definitions, lineage, and standard operating procedures for the analytics team, contributing to knowledge sharing and ensuring consistent handling of sensitive patient health information under privacy guidelines.",
        "Participated in requirements gathering meetings with pathologists and clinicians to understand their data needs for research studies, translating those needs into technical specifications for data extraction and aggregation.",
        "Performed exploratory data analysis on large sets of medical images and associated metadata, identifying patterns, outliers, and data quality issues that needed to be addressed before downstream machine learning processes.",
        "Supported senior data engineers in testing and validating new ETL processes, writing validation queries to compare source and target data counts and values, gaining initial exposure to data pipeline concepts and best practices."
      ],
      "environment": [
        "Python",
        "SQL",
        "Oracle",
        "PostgreSQL",
        "Power BI",
        "Data Analysis",
        "ETL",
        "Healthcare Data",
        "HIPAA",
        "Data Validation",
        "Data Visualization"
      ]
    }
  ],
  "education": [
    {
      "institution": "VMTW",
      "degree": "Bachelor of Technology",
      "field": "Computer science",
      "year": "July 2011 - May 2015"
    }
  ],
  "certifications": []
}