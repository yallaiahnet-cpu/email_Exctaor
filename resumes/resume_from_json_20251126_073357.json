{
  "name": "Yallaiah Onteru",
  "title": "Senior Knowledge Graph Engineer & AI Developer",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "I am having 10 years of experience in knowledge graph development, semantic systems, graph database optimization, and AI-driven data engineering across Insurance, Healthcare, Banking, and Consulting sectors.",
    "Built enterprise Neo4j knowledge graphs using Cypher queries to connect policy holder entities with claim nodes, reducing query response time through graph algorithms like PageRank and Connected Components for insurance risk analysis.",
    "Created RDF triple stores with SPARQL endpoints to validate healthcare data quality using SHACL constraints, ensuring HIPAA compliance while processing patient records through ETL pipelines orchestrated via Airflow on AWS Cloud infrastructure.",
    "Designed Graph RAG systems integrating Neo4j with vector databases and LangChain frameworks to retrieve contextual insurance policy information, implementing multi-agent architectures using LangGraph for automated claim processing workflows.",
    "Developed Java Spring Boot microservices exposing REST APIs that query Neo4j graph databases, deploying containerized services via Docker and Kubernetes to handle real-time graph traversals for banking transaction monitoring under PCI-DSS standards.",
    "Configured AWS Cloud infrastructure including S3 for graph data storage, Lambda functions for ETL triggers, and RDS for relational metadata, managing infrastructure as code using Terraform while monitoring system health through CloudWatch and Prometheus.",
    "Implemented data quality checks within ETL pipelines using Pandas and NumPy to validate graph node properties before ingestion into Neo4j, writing Python scripts that flag anomalies in healthcare records based on state regulatory requirements.",
    "Constructed knowledge graph schemas modeling insurance domain ontologies with entities representing policies, claims, and customer relationships, applying graph algorithms to identify fraud patterns through connected component analysis across claim networks.",
    "Integrated Claude AI models via MCP protocols within multi-agent systems, building POC demonstrations where agents collaborate using structured outputs and streaming callbacks to process insurance documentation and extract policy terms into graph nodes.",
    "Automated graph database ETL workflows using Airflow DAGs that extract source data from SQL databases, transform records applying business rules, and load cleaned data into Neo4j while maintaining version control through Git repositories.",
    "Optimized Cypher query performance analyzing execution plans and creating appropriate indexes on Neo4j node properties, reducing healthcare patient lookup times when traversing relationship paths between diagnosis nodes and treatment entities.",
    "Deployed microservices architecture on AWS ECS clusters running containerized Java Spring Boot applications, implementing CI/CD pipelines via Jenkins that test graph query endpoints before production deployment to maintain service reliability.",
    "Applied SHACL validation rules to RDF graphs ensuring semantic data integrity for banking customer profiles, catching schema violations before data propagation across downstream systems while maintaining audit trails for regulatory compliance reviews.",
    "Participated in daily standups discussing graph modeling challenges with cross-functional teams including software engineers and data scientists, troubleshooting Cypher query bottlenecks and proposing index strategies during code review sessions.",
    "Monitored Neo4j cluster health using Grafana dashboards tracking query latency and memory usage, responding to alerts by analyzing slow query logs and coordinating with infrastructure teams to scale graph database resources during peak loads.",
    "Tested REST API endpoints using PyTest frameworks validating graph query responses match expected node relationships, writing unit tests for Python ETL functions that transform raw insurance data before loading into knowledge graph structures.",
    "Collaborated with product managers to translate business requirements into graph data models, documenting ontology decisions and relationship semantics in technical specifications shared with quality assurance teams for validation planning.",
    "Researched emerging graph technologies attending team knowledge-sharing sessions about GraphOps practices and semantic modeling patterns, experimenting with RDF serialization formats during POC phases to evaluate integration options with existing systems."
  ],
  "technical_skills": {
    "Graph Technologies": [
      "Neo4j",
      "Cypher",
      "Knowledge Graphs",
      "Graph RAG",
      "RDF",
      "SPARQL",
      "SHACL",
      "Graph Algorithms (PageRank, Connected Components)"
    ],
    "Programming Languages": [
      "Python",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "AI & Agent Frameworks": [
      "LangChain",
      "LangGraph",
      "CrewAI",
      "Multi-Agent Systems",
      "Claude AI Models",
      "MCP (Model Context Protocol)",
      "Structured Outputs",
      "Callbacks",
      "Streaming"
    ],
    "Data Engineering & ETL": [
      "Apache Airflow",
      "ETL Pipelines",
      "Data Quality Checks",
      "Apache Spark",
      "PySpark",
      "Informatica",
      "Sqoop",
      "Apache NiFi",
      "AWS Glue"
    ],
    "Cloud Platforms": [
      "AWS (S3, Lambda, EC2, RDS, Redshift, CloudWatch, ECS)",
      "Azure (Data Factory, Databricks, Cosmos DB, ML Studio)"
    ],
    "Backend & Microservices": [
      "Java Spring Boot",
      "REST APIs",
      "GraphQL",
      "Microservices Architecture",
      "Flask",
      "FastAPI"
    ],
    "Vector & Traditional Databases": [
      "Vector Databases",
      "PostgreSQL",
      "MySQL",
      "MongoDB",
      "Cassandra",
      "Snowflake",
      "Elasticsearch",
      "Redis"
    ],
    "Data Science Libraries": [
      "Pandas",
      "NumPy",
      "Scikit-learn",
      "TensorFlow",
      "PyTorch",
      "XGBoost",
      "Hugging Face Transformers"
    ],
    "DevOps & Orchestration": [
      "Docker",
      "Kubernetes",
      "Terraform",
      "CloudFormation",
      "Jenkins",
      "Git",
      "GitHub Actions"
    ],
    "Monitoring & Testing": [
      "Prometheus",
      "Grafana",
      "CloudWatch",
      "PyTest",
      "JUnit",
      "Linux/Unix"
    ],
    "Compliance & Security": [
      "HIPAA",
      "PCI-DSS",
      "GDPR",
      "FDA Regulations",
      "Data Privacy Standards"
    ],
    "Big Data Frameworks": [
      "Apache Hadoop",
      "Apache Kafka",
      "Spark Streaming",
      "Hive",
      "HBase",
      "MapReduce"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Build Neo4j knowledge graphs modeling insurance policy entities with Cypher queries that traverse claim relationships using graph algorithms like PageRank to rank fraud risk scores, connecting policyholder nodes to incident entities for compliance reporting.",
        "Design Graph RAG architectures integrating Neo4j with vector databases, implementing LangGraph multi-agent systems where specialized agents retrieve policy documentation and Claude AI models generate contextual responses via MCP protocols with structured outputs.",
        "Develop Java Spring Boot microservices exposing REST APIs that execute Cypher queries against Neo4j clusters, deploying containerized services using Docker on AWS ECS while managing infrastructure through Terraform scripts for auto-scaling configurations.",
        "Configure ETL pipelines in Apache Airflow extracting insurance claims from SQL databases, transforming data using Python Pandas to validate against business rules, and loading cleaned records into Neo4j while implementing data quality checks that flag anomalies.",
        "Construct RDF triple stores applying SHACL validation rules to ensure semantic consistency across insurance ontologies, writing SPARQL queries that retrieve policy terms and verify compliance with state insurance regulations during quarterly audits.",
        "Implement multi-agent POC demonstrations using LangGraph where agents coordinate through callbacks and streaming responses, processing claim documents to extract entities and relationships that populate knowledge graph nodes representing policy coverage details.",
        "Optimize Cypher query performance analyzing execution plans and creating composite indexes on Neo4j node properties, reducing claim lookup times from minutes to seconds when traversing relationship paths between customer entities and policy nodes.",
        "Monitor Neo4j cluster health using Grafana dashboards tracking query latency and memory consumption, responding to CloudWatch alerts by scaling AWS EC2 instances and coordinating with infrastructure teams during peak claim processing periods.",
        "Integrate Claude AI models within PySpark jobs that analyze claim text documents, extracting structured insurance entities and feeding results into knowledge graphs while maintaining audit logs in AWS S3 for regulatory compliance reviews.",
        "Collaborate with data science teams debugging graph traversal issues during daily standups, reviewing Cypher query logic and proposing relationship modeling changes that better represent insurance domain concepts like policy endorsements and riders.",
        "Test REST API endpoints using PyTest frameworks validating graph query responses return correct node relationships, writing unit tests for Python ETL functions that transform raw claim data before loading into Neo4j databases.",
        "Participate in code reviews examining Java Spring Boot service implementations, suggesting improvements to exception handling and database connection pooling strategies that increase microservice reliability during high-traffic scenarios.",
        "Automate CI/CD pipelines using Jenkins deploying knowledge graph applications to AWS environments, configuring deployment stages that run integration tests against Neo4j test instances before promoting services to production clusters.",
        "Apply Connected Components graph algorithm identifying clusters of related insurance claims that indicate potential fraud rings, visualizing results for investigation teams using Python matplotlib libraries that highlight suspicious relationship patterns.",
        "Coordinate with product managers translating business requirements into graph data models, documenting entity-relationship schemas and ontology design decisions in Confluence pages shared with quality assurance teams for test case development.",
        "Research GraphOps practices attending team workshops about semantic modeling patterns, experimenting with different RDF serialization formats during POC phases to evaluate performance trade-offs when integrating with existing AWS data lake architectures."
      ],
      "environment": [
        "Neo4j",
        "Cypher",
        "Knowledge Graphs",
        "Graph RAG",
        "RDF",
        "SPARQL",
        "SHACL",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "Claude AI",
        "MCP",
        "Java Spring Boot",
        "Python",
        "PySpark",
        "REST APIs",
        "Microservices",
        "Apache Airflow",
        "ETL Pipelines",
        "Pandas",
        "NumPy",
        "AWS S3",
        "AWS Lambda",
        "AWS EC2",
        "AWS ECS",
        "AWS RDS",
        "Docker",
        "Kubernetes",
        "Terraform",
        "Jenkins",
        "Git",
        "PyTest",
        "Grafana",
        "Prometheus",
        "CloudWatch",
        "Vector Databases",
        "PostgreSQL",
        "Graph Algorithms (PageRank, Connected Components)"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Architected Neo4j knowledge graphs representing healthcare patient journeys with Cypher queries traversing diagnosis nodes to treatment entities, applying graph algorithms to identify care pathway patterns while ensuring HIPAA compliance through encrypted data storage on AWS.",
        "Established Graph RAG systems combining Neo4j with vector databases using LangChain frameworks to retrieve patient medical histories, implementing retrieval pipelines that query graph relationships and generate clinical summaries through Claude AI model integration.",
        "Engineered Java Spring Boot microservices providing REST APIs for clinical data access, deploying containerized applications via Docker on AWS ECS clusters while configuring load balancers and implementing OAuth authentication to meet healthcare security requirements.",
        "Orchestrated ETL workflows using Apache Airflow extracting patient records from hospital SQL databases, transforming data with Python scripts applying HIPAA anonymization rules, and loading validated records into Neo4j while logging data quality metrics.",
        "Validated RDF healthcare ontologies using SHACL constraint definitions ensuring semantic integrity across medical terminologies, writing SPARQL federated queries that retrieve patient information from distributed graph databases while maintaining FDA regulatory compliance.",
        "Prototyped multi-agent systems using LangGraph where medical coding agents processed clinical notes through streaming callbacks, extracting ICD codes and medication entities that populated knowledge graph nodes representing patient treatment histories.",
        "Tuned Cypher query performance adding strategic indexes on patient identifier properties, reducing medication history lookups from seconds to milliseconds when querying relationship paths connecting patient nodes to prescription entities across hospital systems.",
        "Tracked Neo4j database metrics using Prometheus exporters and Grafana dashboards monitoring query throughput, troubleshooting slow queries during on-call rotations by analyzing database logs and adjusting memory allocation settings based on workload patterns.",
        "Processed clinical documents using PySpark jobs integrated with LangChain document loaders, extracting medical entities through NLP pipelines and storing structured results in Neo4j while maintaining comprehensive audit trails required for healthcare compliance.",
        "Assisted data engineering teams resolving ETL pipeline failures during morning standups, debugging Python transformation logic and fixing data type mismatches that caused Neo4j ingestion errors when loading patient demographic information.",
        "Verified REST API functionality writing PyTest test suites that validated graph query results matched expected patient relationships, creating mock data fixtures representing various clinical scenarios to ensure comprehensive endpoint coverage.",
        "Reviewed Java code submissions evaluating error handling approaches and database transaction management, suggesting retry logic improvements that increased microservice resilience when Neo4j cluster nodes experienced temporary network connectivity issues.",
        "Maintained CI/CD pipelines using GitHub Actions automating deployment of knowledge graph services to AWS environments, configuring pipeline stages that executed integration tests against staging Neo4j instances before releasing to production.",
        "Analyzed patient care networks using Connected Components algorithm identifying isolated patient groups requiring care coordination, generating reports for healthcare administrators using Python seaborn visualizations highlighting care delivery gaps across facilities."
      ],
      "environment": [
        "Neo4j",
        "Cypher",
        "Knowledge Graphs",
        "Graph RAG",
        "RDF",
        "SPARQL",
        "SHACL",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "Claude AI",
        "Java Spring Boot",
        "Python",
        "PySpark",
        "REST APIs",
        "Microservices",
        "Apache Airflow",
        "ETL Pipelines",
        "Pandas",
        "NumPy",
        "AWS S3",
        "AWS Lambda",
        "AWS ECS",
        "AWS RDS",
        "Docker",
        "Kubernetes",
        "Jenkins",
        "Git",
        "PyTest",
        "GitHub Actions",
        "Grafana",
        "Prometheus",
        "CloudWatch",
        "Vector Databases",
        "PostgreSQL",
        "HIPAA Compliance",
        "FDA Regulations",
        "Graph Algorithms"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Modeled Azure Cosmos DB graph databases representing Medicaid beneficiary relationships using Gremlin queries, connecting patient nodes to provider entities while implementing HIPAA data encryption and access controls for state healthcare programs.",
        "Generated RDF datasets from legacy healthcare systems using Python scripts, transforming relational data into semantic triples and validating graph structure with SHACL rules before loading into Azure-hosted triple stores for public health reporting.",
        "Assembled ETL pipelines using Azure Data Factory orchestrating data flows from SQL Server databases, applying Python Pandas transformations to standardize provider records, and loading cleaned data into Cosmos DB while monitoring pipeline execution logs.",
        "Queried graph databases using SPARQL endpoints to extract Medicaid enrollment statistics for state reporting requirements, writing complex federated queries that joined patient demographics with service utilization data across distributed healthcare systems.",
        "Programmed Python microservices exposing REST APIs that executed Gremlin graph traversals, deploying applications as Azure App Services with automated scaling policies that handled variable query loads during monthly enrollment processing periods.",
        "Certified data quality implementing validation checks within ETL workflows using NumPy arrays to flag missing patient identifiers, creating exception reports for data stewards highlighting records requiring manual review before graph database ingestion.",
        "Indexed Cosmos DB graph collections optimizing query performance for common access patterns, reducing beneficiary lookup times when traversing relationship edges between enrollment nodes and service provider entities across state healthcare networks.",
        "Observed Azure Monitor dashboards tracking Cosmos DB request units and query latency, investigating performance degradation incidents during weekly operations reviews and recommending partition key adjustments to improve data distribution across database partitions.",
        "Collaborated with state healthcare administrators during requirements gathering sessions, documenting graph data models representing Medicaid program rules and translating policy requirements into entity-relationship schemas stored in Azure DevOps wikis.",
        "Debugged ETL pipeline failures during team meetings examining Python stack traces, identifying root causes related to API rate limits when calling external healthcare data sources and implementing exponential backoff retry logic.",
        "Validated graph query results writing Python unit tests comparing actual traversal outputs against expected relationship paths, using PyTest fixtures that mocked Azure Cosmos DB responses to test service logic without requiring live database connections.",
        "Attended weekly architecture reviews presenting graph modeling decisions to technical leads, discussing trade-offs between different graph database options and defending schema choices based on state regulatory requirements and query performance benchmarks."
      ],
      "environment": [
        "Azure Cosmos DB",
        "Gremlin",
        "Graph Databases",
        "RDF",
        "SPARQL",
        "SHACL",
        "Python",
        "REST APIs",
        "Azure Data Factory",
        "ETL Pipelines",
        "Pandas",
        "NumPy",
        "Azure App Services",
        "Azure Monitor",
        "SQL Server",
        "Azure DevOps",
        "Git",
        "PyTest",
        "HIPAA Compliance",
        "Data Quality Checks",
        "Microservices"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Mapped financial transaction networks using Azure Cosmos DB graph structures with Gremlin queries identifying suspicious money flow patterns, connecting account holder nodes to transaction entities while enforcing PCI-DSS data protection standards.",
        "Extracted banking data from SQL Server databases using Python scripts, transforming transactional records into graph-friendly formats with Pandas dataframes, and loading normalized data into Cosmos DB collections following established ETL patterns.",
        "Composed SPARQL queries against RDF financial ontologies retrieving customer account relationships for regulatory reporting, joining semantic data from multiple banking systems to generate consolidated views of customer holdings across product lines.",
        "Deployed Python Flask APIs serving graph database query results to internal banking applications, containerizing services using Docker and running on Azure Kubernetes clusters with network policies restricting access to approved internal IP ranges.",
        "Inspected data quality running validation scripts that checked referential integrity between graph nodes, using NumPy operations to identify orphaned transaction records missing corresponding account nodes and generating cleanup reports for operations teams.",
        "Adjusted Cosmos DB indexing policies improving query response times for fraud detection workflows, analyzing query execution statistics and creating selective indexes on transaction timestamp properties to accelerate temporal range scans.",
        "Reviewed Azure Application Insights telemetry monitoring API request patterns, investigating latency spikes during month-end processing windows and coordinating with database administrators to allocate additional throughput during peak transaction volumes.",
        "Documented graph schema designs capturing banking domain entities like accounts, customers, and transactions, creating entity-relationship diagrams stored in Confluence that guided development teams implementing new financial product features.",
        "Participated in sprint planning sessions estimating effort for graph database features, breaking down user stories into technical tasks and discussing implementation approaches with software engineers during collaborative design sessions.",
        "Troubleshot production incidents during on-call shifts analyzing application logs, identifying database connection pool exhaustion issues and implementing connection retry logic to improve service reliability during network disruptions."
      ],
      "environment": [
        "Azure Cosmos DB",
        "Gremlin",
        "Graph Databases",
        "RDF",
        "SPARQL",
        "Python",
        "Flask",
        "REST APIs",
        "ETL Pipelines",
        "Pandas",
        "NumPy",
        "Azure Kubernetes Service",
        "Docker",
        "SQL Server",
        "Azure Application Insights",
        "Git",
        "PCI-DSS Compliance",
        "Data Quality Checks",
        "Microservices"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Transferred data from relational databases using Sqoop importing MySQL tables into Hadoop HDFS clusters, writing shell scripts that automated daily data ingestion jobs and logged execution status for monitoring purposes.",
        "Transformed raw data files using Informatica PowerCenter mapping workflows, applying business rules to cleanse customer records and standardize data formats before loading into enterprise data warehouse tables.",
        "Processed large datasets using Hadoop MapReduce jobs written in Java, aggregating sales transactions across regional databases and generating summary reports stored in Hive tables for business intelligence consumption.",
        "Scheduled ETL workflows using Unix cron jobs triggering Informatica sessions during off-peak hours, monitoring job completion status through email notifications and investigating failures by examining session logs.",
        "Validated data accuracy comparing source record counts against target table row counts using SQL queries, documenting discrepancies in issue tracking systems and working with source system owners to resolve data quality problems.",
        "Learned Hadoop ecosystem components attending internal training sessions, practicing HDFS commands and Hive query syntax in development environments to build foundational skills in distributed data processing.",
        "Assisted senior engineers troubleshooting Informatica mapping errors during team meetings, reviewing transformation logic and suggesting fixes for null value handling issues that caused workflow failures.",
        "Contributed to team documentation updating data lineage diagrams showing data flow from source systems through ETL pipelines into target databases, helping new team members understand existing data integration architecture."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "MySQL",
        "Java",
        "Shell Scripting",
        "Unix/Linux",
        "ETL Pipelines",
        "Data Quality Checks"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Certified Salesforce Developer PD1"
  ]
}