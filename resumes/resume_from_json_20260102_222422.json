{
  "name": "Yallaiah Onteru",
  "title": "GenAI Agentic Solutions Engineer ",
  "contact": {
    "email": "yonteru.ai.engineer@gmail.com",
    "phone": "7372310791",
    "portfolio": "",
    "linkedin": "Linked In",
    "github": ""
  },
  "professional_summary": [
    "Having 6 years of experience specializing in GenAI agentic solutions, production ML systems, and enterprise-scale automation across Insurance, Technology, Transportation, and Banking domains with proven expertise in operational excellence.",
    "Built LLM-powered AI agents using LangChain, LangGraph, and MCP protocol to diagnose production incidents, integrated RAG pipelines with vector databases for automated troubleshooting, reducing mean time to resolution across large-scale environments.",
    "Deployed tool-using agents on AWS ECS and Lambda with function calling capabilities, orchestrated multi-agent systems using Step Functions, applied prompt engineering with Claude, OpenAI, Gemini, and Llama models for production support automation.",
    "Developed Python and Java microservices on AWS infrastructure leveraging S3, DynamoDB, Redshift, and SageMaker, implemented Terraform and CloudFormation for infrastructure as code, established CI/CD pipelines for continuous model deployment and evaluation.",
    "Configured vector retrieval systems with embedding models, architected RAG solutions integrating observability platforms for real-time diagnostics, utilized PySpark and Databricks for large-scale data processing supporting agentic workflows and ML pipelines.",
    "Implemented finetuning workflows for domain-specific LLMs including Qwen models, applied statistical methods and ML algorithms for anomaly detection, built evaluation frameworks measuring agent performance against business metrics focused on risk reduction and cost efficiency.",
    "Integrated Docker containerization for EKS deployments, established MLOps practices for model serving and monitoring, partnered with production engineers translating operational pain points into measurable agentic AI roadmaps aligned with reliability objectives.",
    "Collaborated across global teams to design policy checks and safety guardrails for AI agent actions, enforced least-privilege access patterns, ensured compliance requirements while maintaining explainable agent reasoning for auditable production operations.",
    "Utilized data structures and algorithms to optimize agent decision-making latency, implemented validator models for production action verification, contributed to design reviews establishing best-in-class engineering standards for autonomous system deployments.",
    "Executed incident management integrations with ServiceNow and PagerDuty, automated diagnostic workflows using SQL queries against operational databases, delivered cost-efficient solutions through intelligent workload orchestration and resource optimization strategies.",
    "Facilitated adoption of embedding techniques for semantic search in troubleshooting knowledge bases, mentored team members on agent architectures and safe deployment patterns, participated in experiment rigor ensuring objective functions linked to operational excellence.",
    "Coordinated with application teams to establish agentic system evaluation methodologies, analyzed time-series data for predictive maintenance models, supported production runtime experience improvements through data-driven insights and continuous learning mechanisms.",
    "Enhanced infrastructure reliability by deploying auto-remediation agents that reason over observability metrics, integrated GraphQL APIs for real-time system state queries, maintained high availability architectures supporting mission-critical production management services.",
    "Secured cloud resources following OWASP guidelines, optimized vector database queries for sub-second retrieval performance, produced technical documentation for agent behaviors ensuring transparency in automated decision-making processes within enterprise technology operations."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "Java",
      "TypeScript",
      "SQL",
      "Bash"
    ],
    "LLM & AI Frameworks": [
      "LangChain",
      "LangGraph",
      "LlamaIndex",
      "Guardrails AI",
      "Model Context Protocol (MCP)",
      "Agents SDK",
      "Hugging Face Transformers"
    ],
    "Large Language Models": [
      "Claude",
      "OpenAI GPT-4",
      "Gemini",
      "Llama",
      "Qwen"
    ],
    "AI/ML Techniques": [
      "RAG (Retrieval-Augmented Generation)",
      "Prompt Engineering",
      "Function Calling",
      "Tool-using Agents",
      "Finetuning",
      "Embedding Models",
      "Multi-agent Systems",
      "Agentic Workflows"
    ],
    "Data Processing & Analytics": [
      "PySpark",
      "Databricks",
      "Pandas",
      "NumPy",
      "Apache Airflow",
      "Applied Statistics",
      "Time-series Analysis"
    ],
    "Vector & Databases": [
      "Pinecone",
      "Weaviate",
      "Chroma",
      "DynamoDB",
      "Redshift",
      "PostgreSQL",
      "MongoDB",
      "Redis"
    ],
    "AWS Cloud Services": [
      "ECS",
      "EKS",
      "Lambda",
      "S3",
      "SageMaker",
      "Step Functions",
      "CloudWatch",
      "IAM",
      "API Gateway"
    ],
    "Infrastructure & DevOps": [
      "Terraform",
      "CloudFormation",
      "Docker",
      "Kubernetes",
      "CI/CD Pipelines",
      "Jenkins",
      "GitHub Actions"
    ],
    "API Development": [
      "FastAPI",
      "Flask",
      "RESTful APIs",
      "GraphQL",
      "Microservices Architecture"
    ],
    "MLOps & Monitoring": [
      "MLflow",
      "Weights & Biases",
      "LangSmith",
      "Prometheus",
      "Grafana",
      "CloudWatch"
    ],
    "Testing & Quality": [
      "Pytest",
      "Unit Testing",
      "Integration Testing",
      "A/B Testing",
      "Model Evaluation (BLEU, ROUGE, F1)"
    ],
    "Incident Management": [
      "PagerDuty",
      "ServiceNow",
      "Splunk",
      "Datadog"
    ],
    "Version Control & Collaboration": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket"
    ],
    "Security & Compliance": [
      "OWASP",
      "Least-privilege Access",
      "IAM Policies",
      "Data Encryption"
    ]
  },
  "experience": [
    {
      "client": "Northwestern Mutual",
      "role": "AI Developer",
      "duration": "2025-Feb - Present",
      "location": "Irving, Texas.",
      "responsibilities": [
        "Constructed LLM-powered diagnostic agents using LangChain and LangGraph frameworks on AWS Lambda, implementing function calling with Claude and OpenAI models to automatically triage insurance policy processing incidents, reducing manual investigation time significantly.",
        "Integrate RAG pipelines with Pinecone vector database for regulatory compliance knowledge retrieval, applied prompt engineering techniques to ensure insurance-specific context accuracy, deployed embedding models on SageMaker for semantic search across policy documentation repositories.",
        "Established multi-agent systems orchestrated via Step Functions, designed tool-using agents with MCP protocol for automated database queries against DynamoDB and Redshift, enabled agents to reason over operational metrics and execute remediation actions following safety guardrails.",
        "Operate Python microservices on ECS clusters with Docker containerization, configured Terraform infrastructure provisioning for scalable agent deployments, maintained CI/CD pipelines using GitHub Actions ensuring continuous evaluation of agent performance against reliability metrics.",
        "Synthesized observability data from CloudWatch and Prometheus into agent decision-making workflows, built validator models verifying production actions before execution, partnered with insurance application teams translating compliance pain points into agentic automation roadmaps.",
        "Automated incident response workflows integrating PagerDuty APIs, utilized PySpark on Databricks for analyzing historical incident patterns, implemented statistical models predicting failure modes within insurance claims processing systems improving operational excellence.",
        "Designed policy checks enforcing least-privilege access for agent actions, integrated Guardrails AI framework preventing unsafe operations in production environments, collaborated with global engineering teams ensuring agent behaviors align with insurance regulatory requirements and audit trails.",
        "Refined LLM finetuning processes for Llama and Qwen models using domain-specific insurance data, applied evaluation frameworks measuring agent accuracy with BLEU and F1 scores, mentored peers on vector retrieval optimization techniques reducing query latency below threshold targets.",
        "Streamlined data pipelines extracting features from S3 data lakes, implemented time-series forecasting models detecting anomalies in policy transaction volumes, contributed to design reviews establishing standards for explainable AI reasoning in insurance operations supporting risk reduction.",
        "Configured Redis caching layers optimizing agent response times, developed FastAPI endpoints exposing agent capabilities to downstream systems, facilitated A/B testing comparing agent architectures to quantify improvements in mean time to resolution and cost efficiency.",
        "Validated agent outputs against insurance compliance frameworks, established MLOps monitoring dashboards using Weights & Biases tracking model drift, coordinated with production engineers documenting agent decision logic for regulatory audits and stakeholder transparency.",
        "Transformed legacy manual runbooks into autonomous agent workflows, utilized Java services for real-time event processing from insurance core systems, integrated GraphQL APIs enabling agents to query policy state across distributed microservices architecture.",
        "Troubleshot production agent failures analyzing CloudWatch logs, implemented rollback mechanisms ensuring system stability during agent updates, supported incident management procedures maintaining high availability for insurance policy administration platforms.",
        "Optimized embedding model selection for insurance domain terminology, evaluated different LLMs comparing cost-performance trade-offs, delivered measurable business impact through automated diagnostics reducing operational risk and improving insurance customer experience metrics."
      ],
      "environment": [
        "Python",
        "Java",
        "TypeScript",
        "LangChain",
        "LangGraph",
        "LlamaIndex",
        "Guardrails AI",
        "Model Context Protocol (MCP)",
        "Agents SDK",
        "Tool calling",
        "LLM-powered AI agents",
        "Agentic workflows",
        "Prompt orchestration",
        "RAG (Retrieval-Augmented Generation)",
        "Long-horizon task execution",
        "Agent design patterns",
        "Multi-agent systems",
        "Claude",
        "OpenAI",
        "Gemini",
        "Llama",
        "Qwen",
        "Function calling",
        "Finetuning",
        "Vector databases",
        "Embedding",
        "Pinecone",
        "Weaviate",
        "PySpark",
        "Databricks",
        "AWS",
        "ECS",
        "EKS",
        "Lambda",
        "S3",
        "DynamoDB",
        "Redshift",
        "Step Functions",
        "SageMaker",
        "Terraform",
        "CloudFormation",
        "Docker",
        "Kubernetes",
        "FastAPI",
        "CI/CD",
        "GitHub Actions",
        "Prometheus",
        "Grafana",
        "CloudWatch",
        "PagerDuty",
        "ServiceNow",
        "MLflow",
        "Weights & Biases",
        "Redis",
        "Pytest",
        "Applied Statistics",
        "ML algorithms",
        "Data structures",
        "OWASP",
        "IAM",
        "Git"
      ]
    },
    {
      "client": "Spartex AI",
      "role": "LLM Developer",
      "duration": "2024-Jun - 2025-Feb",
      "location": "Remote",
      "responsibilities": [
        "Assembled RAG systems using LangChain framework with Chroma vector database for technology documentation retrieval, implemented prompt engineering strategies with Claude and OpenAI models, deployed Python services on AWS Lambda processing embedding vectors for semantic similarity search.",
        "Produced multi-agent architectures coordinated through Step Functions workflows, applied MCP protocol enabling tool-using agents to query DynamoDB tables and execute AWS SDK operations, integrated function calling capabilities allowing agents to interact with observability APIs.",
        "Architected CI/CD pipelines using Terraform and CloudFormation deploying LLM applications on ECS, containerized Python microservices with Docker, established MLOps practices monitoring model performance through CloudWatch dashboards tracking latency and token usage metrics.",
        "Processed large-scale datasets with PySpark on Databricks extracting training data for finetuning Llama models, applied statistical techniques validating model outputs, collaborated with technology teams identifying automation opportunities for software development workflows reducing manual effort.",
        "Trained domain-specific LLMs through finetuning techniques, evaluated Gemini and Qwen models comparing accuracy on technology-specific tasks, utilized LlamaIndex for document indexing, configured vector retrieval systems with Weaviate optimizing query performance through embedding tuning.",
        "Analyzed time-series metrics from technology infrastructure detecting performance degradation patterns, implemented predictive models forecasting system capacity needs, integrated Prometheus observability enabling agents to access real-time operational data for decision-making.",
        "Standardized safety guardrails using Guardrails AI framework preventing unintended agent behaviors, enforced least-privilege IAM policies for agent AWS access, documented agent architectures ensuring transparency in automated actions for technology operations audit requirements.",
        "Accelerated agentic workflow development by creating reusable agent templates, mentored developers on LangGraph state management patterns, facilitated design sessions translating technology pain points into agent capabilities aligned with operational risk reduction objectives.",
        "Measured agent effectiveness using evaluation metrics including BLEU and ROUGE scores, conducted A/B testing comparing prompt strategies, leveraged Redis caching reducing redundant LLM API calls improving cost efficiency and response latency.",
        "Migrated legacy rule-based automation to LLM-powered agents with reasoning capabilities, integrated FastAPI endpoints exposing agent services, partnered with DevOps teams embedding agents into incident response playbooks automating initial diagnostic steps."
      ],
      "environment": [
        "Python",
        "TypeScript",
        "LangChain",
        "LangGraph",
        "LlamaIndex",
        "Guardrails AI",
        "Model Context Protocol (MCP)",
        "Agents SDK",
        "Tool calling",
        "LLM-powered AI agents",
        "Agentic workflows",
        "Prompt orchestration",
        "RAG (Retrieval-Augmented Generation)",
        "Long-horizon task execution",
        "Agent design patterns",
        "Multi-agent systems",
        "Claude",
        "OpenAI",
        "Gemini",
        "Llama",
        "Qwen",
        "Function calling",
        "Finetuning",
        "Vector databases",
        "Embedding",
        "Chroma",
        "Weaviate",
        "PySpark",
        "Databricks",
        "AWS",
        "ECS",
        "Lambda",
        "S3",
        "DynamoDB",
        "Step Functions",
        "SageMaker",
        "Terraform",
        "CloudFormation",
        "Docker",
        "FastAPI",
        "CI/CD",
        "Prometheus",
        "CloudWatch",
        "MLflow",
        "Redis",
        "Applied Statistics",
        "ML algorithms",
        "OWASP",
        "IAM",
        "Git"
      ]
    },
    {
      "client": "Ola",
      "role": "Machine Learning Engineer",
      "duration": "2020-Oct - 2023-Sep",
      "location": "Banglore, India.",
      "responsibilities": [
        "Launched predictive ML models on Azure Machine Learning analyzing transportation demand patterns, utilized Python and PySpark processing ride-sharing telemetry data, applied statistical forecasting techniques improving fleet allocation accuracy.",
        "Generated feature engineering pipelines transforming geospatial data from Azure Data Lake, implemented time-series algorithms detecting anomalies in driver availability metrics, collaborated with transportation operations teams identifying efficiency improvement opportunities.",
        "Deployed containerized ML services on Azure Kubernetes Service (AKS), configured Azure DevOps CI/CD pipelines automating model retraining workflows, monitored prediction accuracy through Azure Application Insights dashboards tracking performance degradation.",
        "Extracted insights from transportation logs stored in Azure SQL Database, developed classification models predicting ride cancellation likelihood, utilized data structures optimizing real-time scoring latency for millions of daily ride requests.",
        "Programmed REST APIs using Flask exposing ML predictions to ride-matching services, integrated Azure Functions for event-driven model inference, maintained data quality checks ensuring training datasets reflected current transportation usage patterns.",
        "Investigated feature importance analysis identifying key transportation demand drivers, conducted experimentation comparing ML algorithms including gradient boosting and neural networks, documented model behaviors supporting transportation regulatory compliance reporting.",
        "Tuned hyperparameters for transportation demand forecasting models, established evaluation frameworks measuring MAE and RMSE metrics, coordinated with data engineering teams ensuring reliable data pipelines feeding ML training processes.",
        "Maintained Azure infrastructure using ARM templates, troubleshot model serving issues analyzing logs and metrics, supported transportation analytics initiatives providing ML expertise for business intelligence dashboard development."
      ],
      "environment": [
        "Python",
        "PySpark",
        "Azure Machine Learning",
        "Azure Kubernetes Service (AKS)",
        "Azure Data Lake",
        "Azure SQL Database",
        "Azure Functions",
        "Azure DevOps",
        "Azure Application Insights",
        "Flask",
        "Docker",
        "ARM Templates",
        "Applied Statistics",
        "ML algorithms",
        "Data structures",
        "Time-series Analysis",
        "Git"
      ]
    },
    {
      "client": "ICICI Bank",
      "role": "Azure Data Engineer",
      "duration": "2019-Feb - 2020-Sep",
      "location": "Mumbai, India.",
      "responsibilities": [
        "Populated Azure SQL Data Warehouse with banking transaction data from multiple source systems, created ETL pipelines using Azure Data Factory orchestrating data flows, ensured compliance with banking data governance policies and audit requirements.",
        "Organized data lake architecture on Azure Blob Storage structuring customer account information, implemented incremental data loading strategies optimizing storage costs, validated data quality through automated testing procedures.",
        "Modeled dimensional schemas for banking analytics supporting regulatory reporting, configured Azure Analysis Services for OLAP cubes, collaborated with business analysts translating banking metrics requirements into data models.",
        "Upgraded legacy on-premises ETL processes to cloud-based Azure workflows, optimized query performance for banking transaction analysis, supported migration initiatives moving critical banking datasets to Azure infrastructure.",
        "Secured sensitive banking data implementing encryption at rest and in transit, enforced access controls through Azure Active Directory, documented data lineage ensuring compliance with banking regulations.",
        "Tested data pipeline reliability conducting validation checks, monitored Azure Data Factory pipeline executions addressing failures, partnered with database administrators optimizing Azure SQL performance for banking workloads."
      ],
      "environment": [
        "Python",
        "Azure Data Factory",
        "Azure SQL Data Warehouse",
        "Azure Blob Storage",
        "Azure Analysis Services",
        "Azure Active Directory",
        "SQL",
        "ETL",
        "Data Modeling",
        "Azure DevOps",
        "Git"
      ]
    }
  ],
  "education": [
    {
      "institution": "University of Wisconsin-Milwaukee",
      "degree": "Master's Degree",
      "field": "Information Technology, AI & Data Analytics",
      "year": "2024"
    }
  ],
  "certifications": [
    "Azure Data Engineer (DP-203)",
    "Azure AI Engineer (AI-101)",
    "Salesforce Developer-Associate"
  ]
}