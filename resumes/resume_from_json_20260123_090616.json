{
  "name": "Yallaiah Onteru",
  "title": "GenAI/NLP Developer | LLM Application Architect",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bring 10 years of hands-on experience across Insurance, Healthcare, Banking, and Consulting domains, with 3 years focused on Generative AI, NLP, and LLM-based application development using Python and prompt engineering techniques.",
    "Architect multi-agent systems and agentic workflows using LangChain, LangGraph, AutoGen, and Model Context Protocol (MCP) to orchestrate LLM interactions, enabling dynamic task delegation and agent-to-agent communication on GCP Vertex AI.",
    "Build production-ready LLM applications integrating vector databases like Pinecone and Chroma for embeddings and similarity search, powering RAG pipelines that retrieve contextual information for text generation and summarization tasks.",
    "Develop NLP solutions for text classification, sentiment analysis, and text summarization using Hugging Face Transformers, BERT, GPT, and fine-tuned models deployed through GCP AI services and Vertex AI endpoints.",
    "Design cloud-native AI/ML pipelines on Google Cloud Platform, utilizing BigQuery for data warehousing, Vertex AI for model training and deployment, and GCP's managed services to scale deep learning workloads efficiently.",
    "Construct APIs and interactive prototypes with FastAPI, Flask, and Streamlit, exposing LLM capabilities through REST endpoints and enabling rapid iteration on generative AI features for stakeholders and end-users.",
    "Implement prompt engineering strategies to optimize LLM outputs, experimenting with few-shot learning, chain-of-thought prompting, and retrieval-augmented generation to improve accuracy in domain-specific text generation scenarios.",
    "Train and fine-tune large language models using TensorFlow, PyTorch, and Hugging Face Transformers, adjusting hyperparameters and applying transfer learning to adapt pre-trained models for insurance claims processing and healthcare documentation.",
    "Integrate vector database-driven workflows with GCP Vertex AI, indexing document embeddings for semantic search and enabling similarity-based retrieval to support question-answering systems and knowledge management applications.",
    "Collaborate within Agile cross-functional teams, participating in sprint planning, daily stand-ups, and retrospectives to deliver GenAI features iteratively, ensuring alignment between data science, engineering, and product management stakeholders.",
    "Deploy deep learning models on GCP using Vertex AI Pipelines, automating model training, validation, and serving workflows with Kubeflow and Docker containers, ensuring scalable inference for real-time NLP tasks.",
    "Apply text classification and sentiment analysis models to analyze customer feedback, insurance claims, and healthcare records, extracting actionable insights and automating document categorization to reduce manual review efforts.",
    "Utilize LangChain and LangGraph for LLM orchestration, chaining multiple model calls, memory modules, and external tools to create conversational agents and multi-step reasoning systems that handle complex user queries.",
    "Process large-scale datasets with PySpark on Databricks, performing data preprocessing, feature engineering, and distributed training to support NLP model development and improve training efficiency on GCP infrastructure.",
    "Establish proof-of-concept (POC) projects for agentic architectures, demonstrating feasibility of multi-agent systems that coordinate specialized LLMs for tasks like document summarization, information extraction, and automated report generation.",
    "Monitor deployed LLM applications using GCP Cloud Monitoring and Logging, tracking inference latency, token usage, and error rates to identify performance bottlenecks and optimize model serving configurations.",
    "Troubleshoot NLP model behavior by analyzing edge cases, debugging prompt templates, and refining embeddings to address issues like hallucinations, off-topic responses, and low relevance scores in retrieval-augmented generation workflows.",
    "Optimize cloud-based AI/ML pipelines on GCP, tuning batch sizes, adjusting compute resources, and implementing caching strategies to reduce inference costs while maintaining low-latency responses for interactive LLM-based applications."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "Generative AI & LLM Technologies": [
      "Large Language Models (LLMs)",
      "Generative AI (GenAI)",
      "Prompt Engineering",
      "Fine-tuning LLMs",
      "Retrieval-Augmented Generation (RAG)",
      "OpenAI GPT",
      "Claude AI",
      "Hugging Face Transformers",
      "Transfer Learning",
      "Few-shot Learning"
    ],
    "Natural Language Processing": [
      "Text Classification",
      "Sentiment Analysis",
      "Text Summarization",
      "Named Entity Recognition (NER)",
      "BERT",
      "GPT",
      "spaCy",
      "NLTK",
      "Stanford NLP",
      "TF-IDF",
      "LSI"
    ],
    "LLM Orchestration & Agent Frameworks": [
      "LangChain",
      "LangGraph",
      "LlamaIndex",
      "AutoGen",
      "Model Context Protocol (MCP)",
      "Multi-agent Systems",
      "Agentic Architectures",
      "Agent-to-Agent Communication",
      "Crew AI"
    ],
    "Vector Databases & Embeddings": [
      "Pinecone",
      "Chroma",
      "FAISS",
      "Weaviate",
      "Embeddings",
      "Similarity Search",
      "Semantic Search",
      "Document Indexing"
    ],
    "Deep Learning Frameworks": [
      "TensorFlow",
      "PyTorch",
      "Keras",
      "Hugging Face Transformers",
      "Convolutional Neural Networks (CNNs)",
      "Recurrent Neural Networks (RNNs)",
      "LSTMs",
      "Transformers",
      "Attention Mechanisms"
    ],
    "Machine Learning & Statistical Models": [
      "Scikit-Learn",
      "XGBoost",
      "LightGBM",
      "H2O",
      "AutoML",
      "MLlib",
      "A/B Testing",
      "ANOVA",
      "Hypothesis Testing",
      "PCA",
      "Regression (Linear, Logistic)",
      "Clustering (K-Means)",
      "Time Series (Prophet)"
    ],
    "Cloud Platforms & AI Services": [
      "Google Cloud Platform (GCP)",
      "GCP Vertex AI",
      "GCP BigQuery",
      "GCP Cloud Storage",
      "GCP AI Platform",
      "GCP Cloud Functions",
      "GCP Cloud Run",
      "GCP Dataflow",
      "AWS (S3, SageMaker, Lambda, EC2, RDS, Redshift, Bedrock)",
      "Azure (ML Studio, Data Factory, Databricks)"
    ],
    "API & Application Development": [
      "FastAPI",
      "Flask",
      "Streamlit",
      "Django",
      "REST APIs",
      "React.js",
      "WebSocket",
      "Microservices"
    ],
    "Big Data & Distributed Computing": [
      "Apache Spark",
      "PySpark",
      "Databricks",
      "Apache Hadoop",
      "Apache Kafka",
      "Apache Airflow",
      "Apache Flink",
      "Hive",
      "MapReduce",
      "dbt"
    ],
    "Data Manipulation & Visualization": [
      "Pandas",
      "NumPy",
      "SciPy",
      "Dask",
      "Apache Arrow",
      "Matplotlib",
      "Seaborn",
      "Plotly",
      "Bokeh",
      "Tableau",
      "Power BI",
      "D3.js"
    ],
    "Databases & Data Warehousing": [
      "PostgreSQL",
      "MySQL",
      "Oracle",
      "MongoDB",
      "Cassandra",
      "Redis",
      "Snowflake",
      "Elasticsearch",
      "Google BigQuery",
      "SQL Server",
      "Teradata",
      "AWS RDS"
    ],
    "MLOps & Model Deployment": [
      "MLflow",
      "DVC",
      "Kubeflow",
      "Docker",
      "Kubernetes",
      "Vertex AI Pipelines",
      "Model Serving",
      "CI/CD for ML",
      "Model Monitoring"
    ],
    "DevOps & Version Control": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Jenkins",
      "GitHub Actions",
      "Terraform",
      "CI/CD Pipelines"
    ],
    "Development & Collaboration Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "RStudio",
      "Google Colab",
      "Anaconda",
      "Jira",
      "Confluence",
      "Slack"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Coordinate planning sessions with cross-functional teams to define requirements for LLM-based insurance claims automation, translating business needs into technical specs for multi-agent systems using LangGraph and Model Context Protocol on GCP Vertex AI.",
        "Prototype agentic architectures with AutoGen and agent-to-agent communication frameworks, setting up proof-of-concept demos that route complex queries to specialized LLMs for claims validation, risk assessment, and summarization workflows.",
        "Configure vector databases like Pinecone on GCP to index policy documents and claims history, implementing similarity search and embeddings to power retrieval-augmented generation pipelines that provide context-aware responses to customer inquiries.",
        "Develop multi-agent systems using LangChain and LangGraph to orchestrate LLM interactions, chaining prompt templates, memory stores, and external APIs, which enables dynamic task delegation for document classification and sentiment analysis in insurance operations.",
        "Build FastAPI endpoints that expose fine-tuned Hugging Face Transformers models for text classification and sentiment analysis, processing incoming claims data in real-time and returning structured JSON responses for downstream policy management systems.",
        "Integrate PySpark on Databricks with GCP BigQuery to preprocess millions of insurance records, performing feature engineering and data cleaning before training NLP models for text summarization and entity extraction from unstructured claims narratives.",
        "Deploy LLM applications on GCP Vertex AI, containerizing models with Docker and orchestrating inference pipelines through Kubernetes, ensuring scalable serving of generative AI features that handle peak loads during claims filing periods.",
        "Conduct prompt engineering experiments to refine LLM outputs for domain-specific insurance terminology, testing few-shot learning and chain-of-thought techniques that reduce hallucinations and improve accuracy in claims categorization tasks.",
        "Establish MLOps workflows using MLflow and Vertex AI Pipelines to automate model training, validation, and deployment cycles, tracking hyperparameters and performance metrics for text classification models trained on PyTorch and TensorFlow frameworks.",
        "Collaborate in daily Agile stand-ups to review sprint progress on GenAI features, discussing blockers with data engineers, product managers, and compliance teams to ensure LLM applications meet insurance regulatory standards and NAIC guidelines.",
        "Troubleshoot model inference issues by analyzing GCP Cloud Logging outputs, identifying token limit breaches and timeout errors in LangChain workflows, then adjusting batch sizes and implementing caching to reduce latency for customer-facing chatbots.",
        "Optimize vector database queries by tuning embedding dimensions and indexing strategies in Chroma, improving retrieval speed for similarity search operations that support question-answering systems built with Hugging Face BERT and GPT models.",
        "Enhance NLP pipelines by fine-tuning pre-trained language models on insurance-specific corpora, adjusting learning rates and applying transfer learning techniques to adapt GPT and BERT for summarizing policy documents and extracting coverage details.",
        "Monitor deployed GenAI applications using GCP Cloud Monitoring dashboards, tracking inference latency, API throughput, and error rates to identify performance degradation, then scaling compute resources and adjusting Vertex AI serving configurations.",
        "Streamline data ingestion workflows by connecting GCP Cloud Storage buckets to Databricks, automating ETL processes that feed cleaned datasets into NLP model training pipelines, reducing manual preprocessing time for text classification projects.",
        "Present POC results on multi-agent systems to stakeholders, demonstrating how agentic workflows using Model Context Protocol can automate claims triage and escalate complex cases to human reviewers, aligning LLM capabilities with business objectives."
      ],
      "environment": [
        "Python",
        "LangChain",
        "LangGraph",
        "AutoGen",
        "Model Context Protocol (MCP)",
        "Multi-agent Systems",
        "GCP Vertex AI",
        "GCP BigQuery",
        "Databricks",
        "PySpark",
        "Pinecone",
        "Chroma",
        "Hugging Face Transformers",
        "PyTorch",
        "TensorFlow",
        "FastAPI",
        "Streamlit",
        "Docker",
        "Kubernetes",
        "MLflow",
        "Prompt Engineering",
        "Text Classification",
        "Sentiment Analysis",
        "Text Summarization",
        "Vector Databases",
        "Embeddings",
        "RAG Pipelines",
        "LLM Orchestration",
        "GCP Cloud Storage",
        "GCP Cloud Monitoring",
        "Git",
        "Jira"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Initiated planning for LLM-powered clinical documentation systems, gathering requirements from healthcare analysts to design multi-agent architectures using LangChain and LangGraph that automate patient record summarization while ensuring HIPAA compliance.",
        "Constructed proof-of-concept applications with AutoGen and LangGraph to demonstrate agent-to-agent communication for medical data extraction, routing patient notes to specialized NLP models for diagnosis coding and treatment recommendation summarization.",
        "Implemented vector database solutions using Pinecone on GCP Vertex AI to index clinical trial reports and drug monographs, enabling semantic search and embeddings-based retrieval that supports question-answering systems for medical research teams.",
        "Created FastAPI microservices that serve fine-tuned Hugging Face BERT models for sentiment analysis on patient feedback and text classification of adverse event reports, integrating these endpoints with existing healthcare data platforms on GCP.",
        "Trained deep learning models using PyTorch and TensorFlow on GCP Vertex AI, fine-tuning GPT and BERT transformers on de-identified electronic health records to improve accuracy in clinical text summarization and entity recognition tasks.",
        "Processed large-scale healthcare datasets with PySpark on Databricks, performing ETL operations and feature engineering on patient records stored in GCP BigQuery, preparing clean inputs for NLP model training and prompt engineering experiments.",
        "Configured LangChain workflows to orchestrate multi-step LLM interactions, chaining retrieval-augmented generation pipelines with vector databases, memory modules, and external APIs to generate context-aware summaries of patient treatment histories.",
        "Deployed GenAI applications on GCP using Docker and Kubernetes, containerizing LLM inference endpoints and orchestrating autoscaling policies through Vertex AI to handle variable loads from clinical documentation workflows.",
        "Refined prompt engineering techniques by testing few-shot and zero-shot learning approaches on domain-specific medical terminology, reducing hallucinations in LLM-generated summaries of radiology reports and lab results.",
        "Collaborated with cross-functional Agile teams including data engineers, compliance officers, and clinicians during sprint reviews, ensuring LLM applications met FDA, HIPAA, and GDPR regulatory requirements for patient data handling.",
        "Debugged NLP model outputs by analyzing edge cases where text classification models misidentified drug interactions, adjusting training datasets and retraining Hugging Face Transformers to improve precision on pharmaceutical terminology.",
        "Optimized vector search performance in Chroma by experimenting with embedding dimensions and distance metrics, accelerating similarity-based retrieval for clinical decision support systems that query large medical knowledge bases.",
        "Monitored production LLM applications using GCP Cloud Logging and Monitoring, tracking API response times and token usage patterns, then tuning Vertex AI serving configurations to reduce inference costs during peak usage periods.",
        "Streamlined data pipelines by automating data ingestion from GCP Cloud Storage to Databricks, reducing manual intervention in ETL workflows that feed cleaned patient records into NLP training pipelines for text summarization models."
      ],
      "environment": [
        "Python",
        "LangChain",
        "LangGraph",
        "AutoGen",
        "Multi-agent Systems",
        "GCP Vertex AI",
        "GCP BigQuery",
        "Databricks",
        "PySpark",
        "Pinecone",
        "Chroma",
        "Hugging Face Transformers",
        "BERT",
        "GPT",
        "PyTorch",
        "TensorFlow",
        "FastAPI",
        "Flask",
        "Streamlit",
        "Docker",
        "Kubernetes",
        "MLflow",
        "Prompt Engineering",
        "Text Classification",
        "Sentiment Analysis",
        "Text Summarization",
        "Vector Databases",
        "Embeddings",
        "RAG Pipelines",
        "NLP",
        "LLM Orchestration",
        "GCP Cloud Storage",
        "GCP Cloud Monitoring",
        "HIPAA Compliance",
        "FDA Regulations",
        "Git",
        "Jira"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Mapped out requirements for NLP-driven healthcare eligibility verification systems, working with state officials to define text classification workflows that process Medicaid applications and ensure HIPAA compliance in AWS cloud infrastructure.",
        "Designed sentiment analysis models using Scikit-Learn and TensorFlow to evaluate citizen feedback on public health services, training classifiers on labeled datasets and deploying them via Flask APIs to AWS Lambda for serverless inference.",
        "Prepared training datasets by extracting text from PDF healthcare forms using Python libraries, performing data cleaning and tokenization with NLTK and spaCy before feeding structured inputs into text classification and summarization pipelines.",
        "Developed REST APIs with Flask to expose NLP models for text summarization of patient case notes, integrating these endpoints with AWS API Gateway and connecting to RDS databases that store de-identified healthcare records.",
        "Trained Recurrent Neural Networks and LSTM models using TensorFlow on AWS SageMaker, applying transfer learning techniques to fine-tune pre-trained embeddings for extracting medical entities from unstructured clinical documents.",
        "Executed batch processing jobs with PySpark on AWS EMR to transform millions of Medicaid claims records, performing feature engineering and aggregations before loading cleaned data into AWS Redshift for downstream analytics and reporting.",
        "Launched machine learning models on AWS SageMaker endpoints, configuring autoscaling policies and monitoring inference performance through CloudWatch metrics, ensuring low-latency responses for real-time eligibility determination applications.",
        "Validated NLP model accuracy by conducting A/B tests on different text classification algorithms, comparing performance of Naive Bayes, Logistic Regression, and deep learning approaches to select the best model for production deployment.",
        "Addressed data quality issues by writing Python scripts to detect missing values and inconsistent formatting in healthcare records, implementing validation checks that flagged anomalies before data entered model training pipelines.",
        "Participated in Agile sprint planning and retrospectives with data engineers and public health analysts, ensuring alignment on project milestones and discussing technical trade-offs for balancing model accuracy with inference speed.",
        "Adjusted hyperparameters for text classification models by running grid search experiments on AWS SageMaker, tuning learning rates and regularization parameters to improve precision and recall scores on state healthcare datasets.",
        "Visualized NLP model performance metrics using Matplotlib and Seaborn in Jupyter notebooks, creating confusion matrices and ROC curves to communicate classification results to non-technical stakeholders during project reviews."
      ],
      "environment": [
        "Python",
        "TensorFlow",
        "PyTorch",
        "Scikit-Learn",
        "NLTK",
        "spaCy",
        "Flask",
        "REST APIs",
        "AWS SageMaker",
        "AWS Lambda",
        "AWS EMR",
        "AWS Redshift",
        "AWS S3",
        "AWS RDS",
        "AWS API Gateway",
        "CloudWatch",
        "PySpark",
        "Text Classification",
        "Sentiment Analysis",
        "Text Summarization",
        "RNNs",
        "LSTMs",
        "NLP",
        "Pandas",
        "NumPy",
        "Matplotlib",
        "Seaborn",
        "Jupyter Notebook",
        "HIPAA Compliance",
        "Git",
        "Jira"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Investigated business requirements for fraud detection and customer sentiment models, collaborating with risk analysts to define machine learning use cases that classify transaction patterns and analyze customer feedback in PCI-DSS compliant environments.",
        "Assembled training datasets by querying transactional data from AWS RDS and Redshift, applying SQL aggregations and Pandas transformations to create feature sets for supervised learning models focused on anomaly detection and risk scoring.",
        "Experimented with text classification techniques using Scikit-Learn and TensorFlow to categorize customer support tickets, training Naive Bayes and Logistic Regression models that automated ticket routing and reduced manual triage efforts.",
        "Produced Flask-based APIs to serve machine learning models for real-time fraud scoring, deploying these services on AWS EC2 instances behind load balancers, ensuring high availability for transaction monitoring systems.",
        "Analyzed sentiment in customer reviews using NLTK and spaCy for text preprocessing, applying TF-IDF vectorization and training classifiers that identified negative feedback patterns, enabling proactive customer retention strategies.",
        "Automated ETL workflows using Apache Airflow to schedule data extraction from AWS S3, transformation with Pandas, and loading into Redshift data warehouse, maintaining data pipelines that fed predictive models for credit risk assessment.",
        "Evaluated model performance by calculating precision, recall, F1-scores, and AUC-ROC metrics, presenting findings to business stakeholders through visualizations created with Matplotlib and Tableau dashboards.",
        "Collaborated in Agile sprints with software engineers and compliance teams, ensuring machine learning models adhered to financial regulations and PCI-DSS standards while delivering fraud detection features on tight timelines.",
        "Resolved data inconsistencies by writing Python scripts to detect duplicates and outliers in transaction logs, implementing validation rules that improved data quality before model training and reduced false positive rates.",
        "Tuned hyperparameters for XGBoost and Random Forest models by conducting cross-validation experiments, adjusting tree depth and regularization settings to optimize classification accuracy on imbalanced fraud detection datasets."
      ],
      "environment": [
        "Python",
        "Scikit-Learn",
        "TensorFlow",
        "XGBoost",
        "Pandas",
        "NumPy",
        "NLTK",
        "spaCy",
        "Flask",
        "REST APIs",
        "AWS EC2",
        "AWS S3",
        "AWS RDS",
        "AWS Redshift",
        "Apache Airflow",
        "SQL",
        "Text Classification",
        "Sentiment Analysis",
        "NLP",
        "Matplotlib",
        "Seaborn",
        "Tableau",
        "Jupyter Notebook",
        "PCI-DSS Compliance",
        "Git",
        "Jira"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Absorbed foundational skills in big data processing by working with Hadoop and Informatica, extracting data from relational databases using Sqoop and loading transformed records into HDFS for downstream analytics workflows.",
        "Wrote MapReduce jobs in Java to process large-scale datasets, performing aggregations and data cleaning operations on clickstream logs, then storing results in Hive tables for reporting and business intelligence queries.",
        "Scheduled ETL pipelines using Informatica PowerCenter to extract transactional data from Oracle databases, applying transformation logic and loading cleansed records into data warehouses, supporting monthly reporting cycles for client projects.",
        "Discovered data quality issues by profiling source systems, identifying missing values and schema mismatches, then documenting findings and collaborating with database administrators to resolve inconsistencies before ETL execution.",
        "Assisted in building Hive queries to aggregate data from partitioned tables, learning how to optimize query performance by adjusting partition schemes and tuning Hadoop cluster configurations under guidance from senior engineers.",
        "Participated in code reviews and pair programming sessions, receiving feedback on MapReduce logic and SQL query optimization, which helped refine technical skills and understand best practices for data engineering tasks.",
        "Tested ETL workflows by running end-to-end data validation scripts in Python, comparing source and target record counts, detecting discrepancies, and fixing transformation bugs before promoting code to production environments.",
        "Supported troubleshooting efforts when Sqoop import jobs failed due to database connectivity issues, working with infrastructure teams to diagnose network problems and adjust connection parameters to restore data ingestion processes."
      ],
      "environment": [
        "Hadoop",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "Java",
        "Python",
        "SQL",
        "Oracle",
        "HDFS",
        "ETL",
        "Data Warehousing",
        "Data Quality",
        "Shell Scripting",
        "Git"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}