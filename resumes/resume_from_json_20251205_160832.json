{
  "name": "Yallaiah Onteru",
  "title": "Senior AI Engineer - Agentic AI & Enterprise GenAI Solutions",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "I am having 10 years of experience in building agentic AI systems, multi-agent orchestration, advanced RAG pipelines, and enterprise-grade GenAI solutions across Insurance, Healthcare, Banking, and Consulting domains.",
    "Designed LangGraph multi-agent workflows for Insurance claims automation, integrating Anthropic Claude and OpenAI GPT models with AWS Bedrock Agents to reduce manual processing time while maintaining compliance with regulatory standards.",
    "Built hierarchical RAG pipelines using Pinecone and Weaviate vector databases, applying semantic chunking strategies and hybrid search with BM25 reranking to improve retrieval accuracy for Healthcare patient record systems under HIPAA constraints.",
    "Implemented OpenAI MCP tool servers for Banking transaction monitoring, creating custom agent workflows that connected to AWS Lambda functions and API Gateway endpoints to automate fraud detection processes while ensuring PCI-DSS compliance.",
    "Developed fine-tuning workflows using LoRA and QLoRA techniques on Amazon Titan and Meta Llama models through SageMaker pipelines, optimizing model performance for domain-specific Insurance risk assessment tasks with reduced inference latency.",
    "Configured AWS Bedrock Knowledge Bases with OpenSearch vector storage, implementing adaptive chunking and ColBERT hybrid retrieval to support Healthcare clinical decision support systems while maintaining strict data security and patient privacy protocols.",
    "Integrated CrewAI multi-agent collaboration patterns for Banking loan processing workflows, coordinating specialized agents with distinct roles to handle document verification, risk scoring, and approval routing through event-driven Step Functions orchestration.",
    "Applied prompt engineering techniques including ReAct, CoT, and Maieutic Prompting to improve Insurance underwriting agent reasoning, reducing hallucination rates through structured output control and grounding strategies with external knowledge base validation.",
    "Established LangSmith observability pipelines for Healthcare AI systems, tracking agent performance metrics, token usage, and response quality across distributed workflows while implementing guardrails for PII masking and policy enforcement.",
    "Deployed AutoGen multi-agent reasoning frameworks for Consulting analytics platforms, enabling agent-to-agent delegation and tool-use design patterns that processed complex business queries across multiple data sources with Redis caching for latency reduction.",
    "Constructed FastAPI microservices for AI agent tooling, exposing RESTful endpoints that integrated with SQS message queues and DynamoDB state storage to support scalable Insurance claims processing workflows across distributed teams.",
    "Optimized LLM inference through prompt compression and distillation techniques, implementing batching strategies in AWS Lambda functions to reduce API costs for high-volume Healthcare patient inquiry systems while maintaining sub-second response times.",
    "Orchestrated CI/CD pipelines using GitHub Actions and CodePipeline for automated Bedrock model evaluations, incorporating regression tests and prompt validation workflows to ensure consistent agent behavior across Banking compliance requirements.",
    "Maintained vector database performance through embedding optimization experiments with E5, bge-large, and Cohere models, conducting A/B tests to identify optimal configurations for Insurance document retrieval systems with ChromaDB and Qdrant storage.",
    "Secured agentic AI systems through AWS KMS encryption, VPC isolation, and jailbreak prevention mechanisms, implementing multi-layer guardrails that validated agent outputs against Healthcare regulatory requirements and Responsible AI standards.",
    "Collaborated with Cloud Architects and Platform Engineering teams to design scalable AI infrastructure using ECS containerization, implementing monitoring through CloudWatch and Prometheus for real-time agent health tracking across enterprise environments.",
    "Troubleshot production agent failures by analyzing LangSmith traces and OpenTelemetry data, identifying bottlenecks in tool server response times and resolving issues through code reviews and debugging sessions with Backend Developers and Data Engineering teams.",
    "Participated in model evaluation workflows using RAGAS and TruLens frameworks, assessing RAG pipeline quality metrics and iterating on retrieval strategies to meet enterprise accuracy standards for Banking financial analysis agents and Insurance underwriting systems."
  ],
  "technical_skills": {
    "LLM Development & Fine-Tuning": [
      "OpenAI GPT-4",
      "Anthropic Claude",
      "Meta Llama",
      "Amazon Titan",
      "LoRA",
      "QLoRA",
      "SFT",
      "RLHF",
      "DPO",
      "Model Distillation"
    ],
    "Prompt Engineering & Agent Design": [
      "Zero-shot Prompting",
      "Few-shot Prompting",
      "Chain-of-Thought (CoT)",
      "ReAct Prompting",
      "PAL Prompting",
      "Maieutic Prompting",
      "Structured Output Control",
      "Grounding Strategies"
    ],
    "Multi-Agent Orchestration Frameworks": [
      "LangChain",
      "LangGraph",
      "OpenAI MCP",
      "CrewAI",
      "AutoGen",
      "Agent-to-Agent Communication",
      "Multi-Agent Reasoning",
      "Tool Server Development"
    ],
    "RAG Pipelines & Vector Databases": [
      "Hierarchical RAG",
      "Agentic RAG",
      "Multi-Vector Retrieval",
      "Pinecone",
      "Weaviate",
      "ChromaDB",
      "OpenSearch",
      "Milvus",
      "Qdrant"
    ],
    "Embedding & Search Optimization": [
      "Amazon Titan Embeddings",
      "E5 Models",
      "bge-large",
      "Cohere Embeddings",
      "Instructor Models",
      "Hybrid Search",
      "BM25",
      "ColBERT",
      "Semantic Chunking",
      "Adaptive Chunking"
    ],
    "AWS AI & ML Services": [
      "AWS Bedrock Agents",
      "Bedrock Knowledge Bases",
      "Bedrock Guardrails",
      "SageMaker",
      "Lambda",
      "Step Functions",
      "API Gateway",
      "S3",
      "DynamoDB",
      "RDS",
      "ECS",
      "KMS",
      "VPC"
    ],
    "LLM Performance & Security": [
      "Prompt Compression",
      "Batching",
      "Caching",
      "Hallucination Reduction",
      "Guardrails",
      "PII Masking",
      "Policy Enforcement",
      "Jailbreak Prevention"
    ],
    "MLOps & Observability": [
      "LangSmith",
      "OpenTelemetry",
      "Bedrock Model Evaluation",
      "Model Drift Detection",
      "Traceability",
      "CloudWatch",
      "Prometheus",
      "Grafana"
    ],
    "Backend Development & APIs": [
      "Python",
      "FastAPI",
      "Flask",
      "Node.js",
      "TypeScript",
      "REST APIs",
      "Event-Driven Architecture",
      "SQS",
      "Redis"
    ],
    "Infrastructure & DevOps": [
      "Terraform",
      "CloudFormation",
      "AWS CDK",
      "Docker",
      "ECS",
      "EKS",
      "Kubernetes",
      "GitHub Actions",
      "CodePipeline"
    ],
    "AI Testing & Evaluation": [
      "RAGAS",
      "TruLens",
      "DeepEval",
      "Unit Testing",
      "Integration Testing",
      "Prompt Regression Tests",
      "A/B Testing"
    ],
    "Programming & Data Tools": [
      "Python",
      "SQL",
      "Scala",
      "TypeScript",
      "Bash",
      "PySpark",
      "Pandas",
      "NumPy"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Build LangGraph multi-agent workflows for Insurance claims automation, coordinating specialized agents that handle document parsing, risk evaluation, and approval routing through AWS Bedrock Agents with custom tool integrations and state persistence.",
        "Create PySpark data pipelines that preprocess Insurance policy documents from S3 buckets, transforming unstructured claim text into structured formats before feeding into hierarchical RAG systems with Pinecone vector storage for semantic retrieval operations.",
        "Integrate OpenAI MCP tool servers with AWS Lambda functions, exposing custom APIs that allow Insurance underwriting agents to query external compliance databases and regulatory knowledge bases while maintaining secure VPC network isolation and encryption.",
        "Develop proof-of-concept multi-agent systems using CrewAI frameworks, demonstrating agent-to-agent collaboration patterns where document reviewers delegate complex risk assessments to specialized financial analysis agents before routing to human underwriters for final approval.",
        "Configure AWS Bedrock Guardrails with PII masking rules and policy enforcement mechanisms, ensuring Insurance customer data remains protected during LLM processing while maintaining compliance with state-level data privacy regulations and industry standards.",
        "Implement prompt engineering strategies including ReAct and Chain-of-Thought reasoning for Insurance fraud detection agents, structuring prompts that guide Anthropic Claude models through systematic evidence evaluation before generating classification decisions with confidence scores.",
        "Deploy FastAPI microservices that serve as tool servers for Insurance agents, providing RESTful endpoints for claims validation, policy lookup, and risk scoring operations with Redis caching layers to minimize API latency during high-volume processing periods.",
        "Optimize embedding models for Insurance document retrieval by testing Amazon Titan, E5, and bge-large variants, conducting experiments to identify configurations that maximize semantic similarity matching for policy clause identification within Weaviate vector databases.",
        "Monitor agent performance through LangSmith telemetry pipelines, tracking token usage, response times, and tool call patterns across distributed Insurance workflows while setting up CloudWatch alarms for anomaly detection and automated incident response procedures.",
        "Troubleshoot production issues in multi-agent systems by analyzing OpenTelemetry traces, identifying bottlenecks in agent communication flows and resolving errors through debugging sessions where I worked closely with Backend Developers to patch API integration problems.",
        "Automate model evaluation workflows using GitHub Actions, running regression tests on Insurance agent prompts and validating output consistency across Bedrock model versions before promoting changes to production environments through CodePipeline deployment stages.",
        "Participate in code reviews with ML Engineers, providing feedback on agent orchestration logic and suggesting improvements to error handling patterns while learning new techniques for managing agent state transitions in complex Insurance workflow scenarios.",
        "Compress prompts for Insurance policy summary agents using distillation techniques, reducing token counts while preserving semantic meaning to lower API costs for high-frequency operations that process thousands of policy documents daily across multiple Insurance product lines.",
        "Establish CI/CD automation for agent tool servers, containerizing Python FastAPI services with Docker and deploying to ECS clusters with auto-scaling policies that adjust capacity based on Insurance claims volume during peak business hours and seasonal fluctuations.",
        "Collaborate with Enterprise Architecture teams on designing secure AI infrastructure, implementing KMS encryption for sensitive Insurance data at rest and in transit while configuring IAM roles that enforce least-privilege access principles for agent service accounts.",
        "Conduct A/B testing on hybrid search configurations, comparing BM25 sparse retrieval against dense vector matching with ColBERT reranking to identify optimal strategies for Insurance knowledge base queries that balance retrieval speed with accuracy requirements for compliance teams."
      ],
      "environment": [
        "LangGraph",
        "OpenAI GPT-4",
        "Anthropic Claude",
        "AWS Bedrock Agents",
        "AWS Bedrock Knowledge Bases",
        "AWS Bedrock Guardrails",
        "OpenAI MCP",
        "CrewAI",
        "PySpark",
        "FastAPI",
        "Python",
        "AWS Lambda",
        "AWS Step Functions",
        "API Gateway",
        "S3",
        "DynamoDB",
        "Pinecone",
        "Weaviate",
        "Redis",
        "Amazon Titan Embeddings",
        "E5",
        "bge-large",
        "ReAct Prompting",
        "Chain-of-Thought",
        "LangSmith",
        "OpenTelemetry",
        "CloudWatch",
        "GitHub Actions",
        "CodePipeline",
        "Docker",
        "ECS",
        "KMS",
        "VPC",
        "Terraform"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Constructed LangChain agent workflows for Healthcare clinical documentation, processing HIPAA-compliant patient records through AWS Bedrock models with guardrails that automatically masked sensitive medical identifiers before storing embeddings in OpenSearch vector databases.",
        "Assembled multi-agent systems using LangGraph for Healthcare patient inquiry routing, where triage agents classified incoming requests and delegated to specialized medical information agents that retrieved relevant clinical guidelines from hierarchical RAG pipelines with semantic chunking.",
        "Tested proof-of-concept agentic RAG implementations for Healthcare drug interaction checking, combining AutoGen reasoning frameworks with AWS Lambda tool servers that queried pharmaceutical databases and returned structured responses with confidence scores for clinical decision support.",
        "Fine-tuned Meta Llama models on Healthcare medical terminology using SageMaker pipelines with LoRA adapters, training on anonymized clinical notes to improve entity recognition accuracy while maintaining HIPAA compliance through secure data handling procedures and audit logging.",
        "Prototyped CrewAI collaboration patterns for Healthcare insurance verification workflows, coordinating multiple agents that validated patient coverage, checked prior authorization requirements, and generated approval documents through event-driven Step Functions state machines with DynamoDB tracking.",
        "Validated RAG pipeline quality using RAGAS evaluation metrics, measuring retrieval precision and answer relevance for Healthcare knowledge bases that provided clinical staff with evidence-based treatment recommendations while documenting performance baselines for regulatory audits.",
        "Configured adaptive chunking strategies for Healthcare clinical guidelines, experimenting with recursive splitting and semantic boundary detection to optimize document segmentation before embedding with Cohere models and indexing in ChromaDB for fast similarity search operations.",
        "Established hybrid search patterns combining BM25 lexical matching with dense vector retrieval, implementing ColBERT reranking to improve Healthcare medical literature search accuracy for research teams querying internal knowledge repositories and external PubMed abstracts.",
        "Debugged agent hallucination issues in Healthcare patient education systems by analyzing LangSmith traces, identifying prompts that generated unsupported medical claims and implementing grounding strategies that required agents to cite specific source documents from approved databases.",
        "Attended weekly meetings with Healthcare compliance officers to discuss AI system governance, presenting agent output samples and explaining how Bedrock Guardrails prevented generation of harmful medical advice while maintaining useful clinical information availability for providers.",
        "Reduced LLM inference costs for Healthcare chatbot systems by implementing batching strategies in AWS Lambda functions, grouping patient inquiries and processing them through shared Bedrock API calls with prompt compression techniques that maintained response quality while lowering token usage.",
        "Upgraded legacy Healthcare data pipelines to support vector embeddings, modifying existing ETL workflows to extract clinical text from RDS databases, generate embeddings using Amazon Titan models, and load vectors into Pinecone indexes with metadata filtering for department-specific access.",
        "Reviewed agent architecture designs with ML Engineers during sprint planning sessions, providing input on tool selection and discussing tradeoffs between framework capabilities while learning about emerging patterns in multi-agent Healthcare applications from experienced team members.",
        "Investigated performance bottlenecks in Healthcare appointment scheduling agents by profiling API response times, discovering inefficient database queries in tool servers and working with Backend Developers to optimize SQL statements and add Redis caching for frequently accessed patient records."
      ],
      "environment": [
        "LangChain",
        "LangGraph",
        "AutoGen",
        "CrewAI",
        "OpenAI GPT-4",
        "Anthropic Claude",
        "Meta Llama",
        "AWS Bedrock",
        "AWS Bedrock Guardrails",
        "SageMaker",
        "LoRA",
        "AWS Lambda",
        "AWS Step Functions",
        "API Gateway",
        "S3",
        "DynamoDB",
        "RDS",
        "OpenSearch",
        "Pinecone",
        "ChromaDB",
        "Redis",
        "Amazon Titan Embeddings",
        "Cohere Embeddings",
        "E5",
        "Hierarchical RAG",
        "Agentic RAG",
        "Semantic Chunking",
        "Adaptive Chunking",
        "Hybrid Search",
        "BM25",
        "ColBERT",
        "RAGAS",
        "LangSmith",
        "CloudWatch",
        "Python",
        "FastAPI",
        "HIPAA Compliance",
        "PII Masking",
        "Terraform",
        "Docker",
        "ECS"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Trained scikit-learn classification models for Healthcare Medicaid eligibility determination, processing applicant data from Azure SQL databases and deploying models through Azure ML Studio endpoints that predicted approval likelihood while maintaining HIPAA-compliant audit trails for state regulators.",
        "Migrated legacy Healthcare data pipelines to Azure Data Factory, orchestrating workflows that extracted patient records from on-premise systems, transformed data using PySpark in Databricks clusters, and loaded cleansed datasets into Azure Cosmos DB for downstream analytics applications.",
        "Prepared embedding vectors for Healthcare policy document search by fine-tuning BERT models on medical terminology, generating dense representations that captured semantic relationships between regulations and storing them in Azure Cognitive Search indexes with custom scoring profiles.",
        "Analyzed Healthcare claims patterns using time series forecasting with Prophet models, identifying seasonal trends in emergency room visits and presenting findings to state health officials through interactive Tableau dashboards that informed resource allocation decisions for rural clinics.",
        "Collaborated with public health teams on COVID-19 contact tracing systems, building graph databases in Azure Cosmos DB that tracked exposure networks and implementing shortest-path algorithms to identify high-risk individuals requiring priority testing and quarantine notifications.",
        "Evaluated machine learning model fairness for Healthcare resource allocation, conducting bias audits on prediction outputs across demographic groups and recommending adjustments to training data sampling strategies that reduced disparities in service recommendations for underserved populations.",
        "Refactored Healthcare prescription monitoring systems from batch to near-real-time processing, implementing Azure Stream Analytics jobs that analyzed incoming pharmacy records and triggered alerts for potential opioid abuse patterns while respecting patient privacy under state regulations.",
        "Participated in disaster recovery planning exercises, testing backup procedures for Healthcare ML models and verifying that Azure Blob Storage replication maintained data integrity across regions to ensure continuity of critical public health prediction systems during emergencies.",
        "Documented technical architectures for Healthcare AI systems, creating detailed diagrams of data flows and model dependencies that helped new team members understand system components while satisfying state requirements for technology documentation in government procurement processes.",
        "Attended training sessions on Azure security features, learning about network isolation through Virtual Networks and applying those concepts to Healthcare systems by configuring private endpoints that restricted ML model access to authorized state employee IP ranges only.",
        "Consulted with HIPAA compliance specialists on data handling procedures, revising ML pipeline code to implement field-level encryption for sensitive patient attributes and adding comprehensive logging that tracked all data access events for regulatory audit trails and incident investigations.",
        "Performed exploratory data analysis on Healthcare utilization metrics using Python Pandas and NumPy, identifying data quality issues in claims records and working with Data Engineering teams to correct upstream ETL processes that introduced errors in diagnosis code mappings."
      ],
      "environment": [
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure Databricks",
        "Azure Cosmos DB",
        "Azure SQL Database",
        "Azure Cognitive Search",
        "Azure Stream Analytics",
        "Azure Blob Storage",
        "Scikit-Learn",
        "BERT",
        "Prophet",
        "PySpark",
        "Python",
        "Pandas",
        "NumPy",
        "Tableau",
        "HIPAA Compliance",
        "Graph Databases",
        "Time Series Forecasting",
        "Model Fairness Auditing",
        "Disaster Recovery",
        "Virtual Networks",
        "Field-Level Encryption"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Predicted credit card default probabilities using XGBoost and LightGBM models, training on historical transaction data from Azure SQL Server and deploying through Azure ML endpoints that generated risk scores for Banking underwriting teams while ensuring PCI-DSS compliance for cardholder information.",
        "Visualized Banking customer segmentation results using Plotly and Seaborn, creating interactive dashboards that displayed cluster characteristics based on transaction patterns and presenting findings to marketing teams who used insights to design targeted product campaigns for different demographic groups.",
        "Processed Banking transaction logs through Azure Data Factory pipelines, extracting features from JSON event streams in Azure Blob Storage and applying dimensionality reduction with PCA before training anomaly detection models that flagged suspicious account activity for fraud investigation teams.",
        "Tuned hyperparameters for Banking churn prediction models using grid search, experimenting with different regularization strengths in logistic regression and random forest depth settings while tracking experiment results in spreadsheets before learning about automated ML tracking tools later in career.",
        "Joined cross-functional meetings with Banking compliance officers to discuss model governance, explaining how prediction confidence thresholds balanced false positive rates against regulatory requirements for fair lending practices and documenting decision rationale in technical specification documents.",
        "Cleaned Banking customer demographic data using Pandas, handling missing values through imputation strategies and removing duplicate records before merging datasets from multiple source systems that tracked account openings, credit scores, and transaction histories for modeling purposes.",
        "Implemented A/B testing frameworks for Banking website recommendations, calculating statistical significance of conversion rate differences between control and treatment groups using hypothesis testing techniques and presenting results to product managers who decided on feature rollout strategies.",
        "Extracted Banking financial statement data using SQL queries against Azure SQL databases, aggregating quarterly metrics and calculating derived features like debt-to-income ratios that improved credit risk model accuracy when combined with traditional credit bureau information.",
        "Learned Azure DevOps for Banking ML deployment, initially struggled with YAML pipeline syntax but eventually automated model training jobs that ran on scheduled intervals and published updated prediction endpoints without manual intervention from Data Science team members.",
        "Contributed to Banking fraud detection rule engines, analyzing false negative cases where existing heuristics missed fraudulent transactions and proposing new pattern-matching logic that reduced financial losses while minimizing customer friction from overly aggressive transaction blocking."
      ],
      "environment": [
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure SQL Server",
        "Azure Blob Storage",
        "Azure DevOps",
        "XGBoost",
        "LightGBM",
        "Scikit-Learn",
        "Pandas",
        "NumPy",
        "Plotly",
        "Seaborn",
        "PCA",
        "Logistic Regression",
        "Random Forest",
        "Anomaly Detection",
        "A/B Testing",
        "Hypothesis Testing",
        "SQL",
        "Python",
        "PCI-DSS Compliance",
        "Credit Risk Modeling",
        "Fraud Detection"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Loaded Consulting client transaction data into Hadoop clusters using Sqoop, scheduling incremental imports from Oracle databases that transferred daily sales records and customer information into HDFS storage for batch processing by downstream analytics teams.",
        "Transformed Consulting financial datasets with Informatica PowerCenter, mapping source columns to standardized schemas and applying business rules that cleansed address fields, validated email formats, and enriched records with geographic codes before loading into data warehouse tables.",
        "Monitored Hadoop job execution logs through command-line tools, identifying failed MapReduce tasks and restarting workflows after resolving input data format issues that caused parsing errors when processing CSV files with inconsistent delimiter characters or unexpected line breaks.",
        "Queried Consulting client databases using SQL to investigate data quality issues, comparing record counts between source systems and Hadoop landing zones to verify that Sqoop transfers completed successfully without missing rows during nightly ETL batch windows.",
        "Participated in daily standup meetings with Consulting project teams, reporting progress on data pipeline development tasks and asking senior engineers for guidance when encountering unfamiliar Informatica transformation components that required parameter configuration.",
        "Documented Consulting data integration workflows by creating flowcharts that illustrated source-to-target mappings, helping business analysts understand how customer information flowed from transactional systems through Hadoop processing stages into reporting databases used by executives.",
        "Learned Hadoop administration basics from infrastructure teams, gaining exposure to cluster configuration files and understanding namenode responsibilities while observing how senior engineers troubleshot disk space issues and rebalanced data across cluster nodes during maintenance windows.",
        "Assisted Consulting teams with ad-hoc data extracts, writing SQL queries that joined multiple tables and exported result sets to CSV files that business users imported into Excel spreadsheets for financial analysis and client reporting deliverables."
      ],
      "environment": [
        "Hadoop",
        "Sqoop",
        "Informatica PowerCenter",
        "MapReduce",
        "HDFS",
        "Oracle",
        "SQL",
        "CSV Processing",
        "Data Quality Validation",
        "ETL Workflows",
        "Data Warehousing",
        "Batch Processing"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}