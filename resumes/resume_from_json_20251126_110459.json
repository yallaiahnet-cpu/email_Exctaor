{
  "name": "Yallaiah Onteru",
  "title": "Senior AI Developer & GCP Data Engineer",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "I am having 10 years of experience in enterprise Java full-stack development with cloud-native data engineering on GCP, delivering scalable applications across Insurance, Healthcare, Banking, and Consulting domains.",
    "Build REST APIs using Spring Boot and Java that connect Angular frontends to BigQuery datasets, ensuring smooth data flow for business analytics and compliance reporting in regulated industries.",
    "Design DAG workflows in Cloud Composer to orchestrate Dataflow pipelines that process insurance claims data, transforming raw files from Cloud Storage into structured BigQuery tables for downstream consumption.",
    "Collaborate with offshore teams and product owners to define microservices architecture, balancing technical feasibility with business requirements while maintaining code quality through peer reviews.",
    "Optimize BigQuery SQL queries for performance, reducing query costs and runtime by rewriting complex joins and partitioning large fact tables to support real-time dashboards in healthcare analytics.",
    "Configure CI/CD pipelines using Cloud Build and Jenkins to automate Java application deployments on GKE, integrating unit tests and static code analysis to catch bugs before production releases.",
    "Implement GCP IAM policies to secure access to BigQuery datasets and Dataflow jobs, ensuring compliance with HIPAA and PCI-DSS standards by restricting permissions based on role and data sensitivity.",
    "Develop Angular components and HTML/CSS layouts that consume RESTful services, creating responsive user interfaces for claims processing and patient data management systems used by business analysts.",
    "Troubleshoot Dataflow job failures by analyzing Cloud Logging output, identifying bottlenecks in data transformations, and adjusting worker configurations to handle peak loads during month-end processing.",
    "Mentor junior developers through code reviews and pair programming sessions, sharing best practices for Spring Boot dependency injection and React state management to improve team productivity.",
    "Integrate Pub/Sub messaging with Dataflow pipelines to enable event-driven architectures, decoupling data ingestion from processing to improve system resilience and scalability in banking applications.",
    "Refactor monolithic Java applications into microservices deployed on Kubernetes, containerizing services with Docker and defining health checks to enable zero-downtime deployments on GKE clusters.",
    "Coordinate with distributed engineering teams across time zones to align on API contracts and data schemas, documenting decisions in Confluence and using Git for version control and branching strategies.",
    "Analyze business requirements with product owners and translate them into technical designs, creating sequence diagrams and data flow charts to communicate architecture decisions to offshore developers.",
    "Monitor production systems using Cloud Monitoring and Stackdriver, setting up alerts for API latency and Dataflow job errors to ensure SLA compliance and quickly respond to incidents.",
    "Write Python scripts for Airflow DAGs that schedule batch jobs, calling BigQuery stored procedures and triggering downstream Dataflow pipelines to automate nightly ETL processes for insurance underwriting.",
    "Test Java services using JUnit and Mockito to achieve code coverage targets, catching edge cases in business logic before deployment and reducing production defects through comprehensive test suites.",
    "Deploy infrastructure as code using Terraform to provision GCP resources like Cloud Storage buckets and BigQuery datasets, ensuring consistent environments across development, staging, and production."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Java",
      "Python",
      "JavaScript",
      "TypeScript",
      "SQL",
      "Bash/Shell",
      "Scala"
    ],
    "Cloud Platforms & Services": [
      "Google Cloud Platform (GCP)",
      "BigQuery",
      "Cloud Composer (Airflow)",
      "Dataflow",
      "Cloud Storage",
      "GCP IAM",
      "Cloud Build",
      "Cloud Monitoring",
      "Stackdriver",
      "Cloud Logging",
      "Pub/Sub",
      "AWS (S3, EC2, RDS, Lambda, Glue, Redshift)"
    ],
    "Backend Frameworks & APIs": [
      "Spring Boot",
      "REST API",
      "Microservices Architecture",
      "Spring MVC",
      "Spring Data JPA",
      "Hibernate",
      "JAX-RS",
      "Flask",
      "Fast API"
    ],
    "Frontend Technologies": [
      "Angular",
      "React",
      "HTML",
      "CSS",
      "JavaScript",
      "TypeScript",
      "Bootstrap",
      "Material UI"
    ],
    "Data Engineering & Orchestration": [
      "Apache Airflow",
      "DAG Development",
      "Apache Spark",
      "PySpark",
      "Apache Kafka",
      "ETL Pipelines",
      "Data Warehousing",
      "Batch Processing",
      "Stream Processing"
    ],
    "Databases & Data Storage": [
      "BigQuery",
      "PostgreSQL",
      "MySQL",
      "Oracle",
      "Cloud Storage",
      "MongoDB",
      "Redis",
      "Snowflake",
      "AWS RDS"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes (GKE)",
      "Container Registry",
      "Helm Charts",
      "Pod Management"
    ],
    "CI/CD & DevOps": [
      "Jenkins",
      "Cloud Build",
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Terraform",
      "Infrastructure as Code",
      "Deployment Automation"
    ],
    "Testing & Quality Assurance": [
      "JUnit",
      "Mockito",
      "Integration Testing",
      "Code Reviews",
      "Unit Testing",
      "Test-Driven Development",
      "Static Code Analysis"
    ],
    "Big Data Technologies": [
      "Apache Hadoop",
      "Hive",
      "Sqoop",
      "MapReduce",
      "HBase",
      "Informatica"
    ],
    "AI/ML Frameworks": [
      "LangChain",
      "LangGraph",
      "Multi-Agent Systems",
      "Model Context Protocol",
      "Crew AI",
      "AutoGen",
      "RAG Pipelines"
    ],
    "Regulatory & Compliance": [
      "HIPAA",
      "PCI-DSS",
      "GDPR",
      "Data Security",
      "Access Control"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Prototype multi-agent systems using LangGraph to automate insurance claims processing, coordinating specialized agents for document extraction, fraud detection, and policy validation to reduce manual review time.",
        "Establish proof-of-concept workflows with Model Context Protocol to enable agent-to-agent communication, allowing claim assessment agents to query policy database agents and retrieve coverage details dynamically.",
        "Configure BigQuery datasets and partitioned tables to store claims history and customer data, writing SQL queries that feed into Dataflow pipelines for nightly batch aggregation used in actuarial risk models.",
        "Create Spring Boot microservices that expose REST APIs for claims submission, integrating with Angular frontends to provide real-time status updates while ensuring compliance with insurance regulatory standards.",
        "Orchestrate data pipelines in Cloud Composer with custom Python DAGs that extract claims files from Cloud Storage, transform them using Dataflow, and load results into BigQuery for business intelligence reporting.",
        "Review code submitted by offshore developers, checking for proper error handling in Java services and validating that BigQuery table schemas match API response models to prevent data type mismatches.",
        "Diagnose Dataflow job errors by examining Cloud Logging traces, discovering that worker memory limits caused OOM crashes during peak claim volumes and adjusting autoscaling policies to handle seasonal spikes.",
        "Mentor team members on GCP IAM best practices, explaining how service accounts and role bindings control access to BigQuery and Cloud Composer, and demonstrating secure key management for API authentication.",
        "Attend daily standups with product owners to prioritize features, clarifying business requirements for new claims workflows and translating them into technical tasks for offshore development teams to implement.",
        "Integrate PySpark transformations within Dataflow jobs to cleanse and deduplicate claims data, handling edge cases where missing fields or inconsistent formats caused downstream analytics dashboards to display incorrect totals.",
        "Deploy Docker containers to GKE clusters running Java microservices, defining Kubernetes deployments with health checks and resource limits to ensure stable performance under load during open enrollment periods.",
        "Experiment with LangGraph agent architectures in sandbox environments, testing different prompt strategies and tool integrations to determine optimal configurations before proposing production implementations.",
        "Collaborate with architects to design event-driven workflows using Pub/Sub, decoupling claims intake from processing to improve system resilience when backend services experience temporary outages.",
        "Maintain CI/CD pipelines in Cloud Build that compile Java applications, run JUnit tests, and push Docker images to Container Registry, automating deployments to staging environments for QA validation.",
        "Query BigQuery tables to analyze claims processing metrics, identifying bottlenecks in approval workflows and recommending schema changes and indexing strategies to improve query performance for reporting dashboards.",
        "Participate in code reviews where I sometimes struggle to balance thoroughness with speed, learning to focus feedback on critical issues like security vulnerabilities and data validation rather than minor style preferences."
      ],
      "environment": [
        "GCP",
        "BigQuery",
        "Cloud Composer",
        "Dataflow",
        "Cloud Storage",
        "GCP IAM",
        "Java",
        "Spring Boot",
        "REST API",
        "Angular",
        "HTML",
        "CSS",
        "JavaScript",
        "Python",
        "PySpark",
        "LangGraph",
        "Multi-Agent Systems",
        "Model Context Protocol",
        "Docker",
        "Kubernetes (GKE)",
        "Git",
        "Cloud Build",
        "Cloud Logging",
        "Cloud Monitoring",
        "JUnit",
        "Mockito",
        "Pub/Sub",
        "Terraform"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Constructed LangChain pipelines for clinical trial data extraction, parsing PDF documents to identify patient eligibility criteria and adverse events, feeding structured results into BigQuery tables for regulatory submissions.",
        "Delivered proof-of-concept multi-agent workflows that automated HIPAA compliance checks, using specialized agents to scan patient records for PII exposure and flag violations before data entered analytics pipelines.",
        "Structured REST APIs with Spring Boot to serve patient health records, implementing OAuth authentication and field-level encryption to meet HIPAA requirements while supporting React-based clinician dashboards.",
        "Automated ETL processes in Cloud Composer by authoring Python DAGs that scheduled Dataflow jobs to ingest EHR data from Cloud Storage, transform it for FHIR compliance, and load it into BigQuery datasets.",
        "Resolved production incidents where Dataflow pipeline failures caused missing patient data in reporting, tracing issues to malformed JSON payloads and adding validation logic to reject invalid records at ingestion.",
        "Conducted code reviews for offshore team members, catching improper handling of PHI in Java services and guiding developers to use encryption at rest and in transit to satisfy HIPAA audit requirements.",
        "Tuned BigQuery performance by partitioning patient encounter tables by date and clustering by patient ID, reducing scan volumes for common queries used in population health analytics dashboards.",
        "Tested LangChain retrieval-augmented generation setups in development environments, experimenting with different embedding models and vector stores to improve accuracy of medical document question-answering systems.",
        "Coordinated with product owners during sprint planning to prioritize features for clinical decision support tools, breaking down epics into user stories and estimating effort for offshore developers to pick up.",
        "Migrated legacy Java applications to microservices on GKE, containerizing services with Docker and defining Kubernetes manifests that included HIPAA-compliant logging and monitoring for FDA-regulated systems.",
        "Investigated Crew AI and AutoGen frameworks for orchestrating healthcare-specific agent workflows, comparing their capabilities for multi-step reasoning tasks like treatment plan recommendation generation.",
        "Managed GCP IAM policies to restrict BigQuery access based on data classification, ensuring only authorized personnel could view sensitive patient information and auditing access logs for compliance reporting.",
        "Wrote integration tests using Mockito to validate API contracts between frontend React components and backend Spring Boot services, catching breaking changes before merging feature branches into main.",
        "Monitored Cloud Monitoring dashboards for API latency and error rates, setting up alerts that triggered PagerDuty incidents when patient data sync jobs exceeded acceptable failure thresholds during nightly runs."
      ],
      "environment": [
        "GCP",
        "BigQuery",
        "Cloud Composer",
        "Dataflow",
        "Cloud Storage",
        "GCP IAM",
        "Java",
        "Spring Boot",
        "REST API",
        "React",
        "HTML",
        "CSS",
        "JavaScript",
        "Python",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "Crew AI",
        "AutoGen",
        "Docker",
        "Kubernetes (GKE)",
        "Git",
        "Cloud Build",
        "Cloud Logging",
        "Cloud Monitoring",
        "JUnit",
        "Mockito",
        "OAuth",
        "HIPAA",
        "FHIR",
        "Terraform"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Trained machine learning models using Scikit-Learn to predict Medicaid eligibility based on applicant demographics, deploying models to AWS Lambda endpoints that served predictions to case management web applications.",
        "Assembled data pipelines on AWS using Apache Airflow to orchestrate Glue ETL jobs that extracted beneficiary records from RDS, transformed them for analytics, and loaded results into Redshift for reporting.",
        "Refined SQL queries in Redshift to generate monthly enrollment reports for state auditors, joining eligibility tables with claims data to calculate program costs while ensuring HIPAA-compliant data masking.",
        "Validated data quality by writing Python scripts that scanned S3 buckets for incoming enrollment files, flagging records with missing required fields and alerting data stewards before processing continued.",
        "Partnered with business analysts to understand state healthcare program rules, translating eligibility requirements into feature engineering logic for ML models that automated manual case review workflows.",
        "Debugged AWS Glue job failures caused by schema drift in source databases, modifying PySpark transformations to handle new columns and backward compatibility without breaking downstream Redshift views.",
        "Documented ML model performance in Confluence, tracking accuracy and recall metrics across demographic segments to identify bias and recommend retraining strategies to improve fairness in eligibility decisions.",
        "Secured AWS resources by configuring IAM roles that restricted S3 bucket access to specific Glue jobs and Lambda functions, implementing least-privilege principles to protect sensitive beneficiary data.",
        "Joined sprint retrospectives where I learned to balance technical debt with feature delivery, advocating for refactoring brittle Airflow DAGs even when product owners prioritized new enrollment workflows.",
        "Presented model results to program managers, explaining confusion matrices and feature importance in non-technical terms to build trust in automated eligibility recommendations.",
        "Monitored CloudWatch logs for Lambda function errors, identifying cold start latency issues that caused timeout failures during peak application submission periods and adjusting memory configurations.",
        "Collaborated with QA teams to define test cases for eligibility prediction APIs, ensuring edge cases like backdated applications and retroactive coverage changes were handled correctly in production."
      ],
      "environment": [
        "AWS",
        "S3",
        "Lambda",
        "RDS",
        "Redshift",
        "Glue",
        "Apache Airflow",
        "Python",
        "PySpark",
        "Scikit-Learn",
        "Pandas",
        "SQL",
        "IAM",
        "CloudWatch",
        "HIPAA",
        "Git",
        "Confluence"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Analyzed transaction data using Python and Pandas to detect fraudulent credit card activity, building logistic regression models that flagged suspicious patterns for manual review by fraud analysts.",
        "Prepared datasets for model training by querying AWS RDS databases, aggregating transaction features like velocity and geographic anomalies, and storing cleaned data in S3 for consumption by SageMaker jobs.",
        "Visualized fraud detection metrics in Tableau dashboards, showing precision-recall tradeoffs to stakeholders and helping risk managers calibrate model thresholds to balance false positives and customer experience.",
        "Evaluated multiple classification algorithms including XGBoost and Random Forest, comparing AUC scores and selecting the best-performing model for production deployment as a SageMaker endpoint.",
        "Ensured PCI-DSS compliance by encrypting cardholder data at rest in S3 and in transit via HTTPS, documenting security controls in audit reports required for quarterly assessments.",
        "Participated in on-call rotation where I sometimes felt overwhelmed troubleshooting production model failures at 2am, gradually building confidence in debugging deployed endpoints under pressure.",
        "Extracted insights from exploratory data analysis, discovering that transaction declines peaked on weekends due to international travel, leading to recommendations for dynamic risk scoring adjustments.",
        "Communicated findings to business stakeholders in quarterly reviews, explaining model performance degradation over time and proposing retraining schedules to maintain accuracy as fraud tactics evolved.",
        "Verified model fairness by analyzing predictions across customer demographics, identifying bias in decline rates for certain zip codes and working with compliance teams to adjust feature weights.",
        "Stored model artifacts and training logs in S3 versioned buckets, implementing lifecycle policies to archive old experiments while retaining metadata for reproducibility during regulatory audits."
      ],
      "environment": [
        "AWS",
        "S3",
        "RDS",
        "SageMaker",
        "Lambda",
        "Python",
        "Pandas",
        "Scikit-Learn",
        "XGBoost",
        "SQL",
        "Tableau",
        "Logistic Regression",
        "Random Forest",
        "PCI-DSS",
        "Git"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Extracted data from Oracle databases using Sqoop to import transactional records into Hadoop HDFS, scheduling daily incremental loads to support downstream MapReduce analytics jobs.",
        "Transformed raw data files with Informatica PowerCenter, mapping source columns to target schemas and applying business rules for data cleansing before loading into Hive tables for reporting.",
        "Validated data integrity by writing SQL scripts that compared row counts and checksums between source systems and HDFS, catching replication errors that could have corrupted monthly reports.",
        "Learned Hadoop ecosystem tools through hands-on experimentation, initially struggling with HDFS commands and MapReduce concepts before gaining confidence through peer mentoring and online tutorials.",
        "Supported production ETL jobs by monitoring Informatica workflows, restarting failed sessions when database connections timed out, and escalating persistent issues to senior engineers for root cause analysis.",
        "Documented data lineage in Excel spreadsheets, tracing how source tables flowed through Sqoop imports and Informatica transformations to final Hive tables consumed by business intelligence teams.",
        "Joined team meetings where I observed senior engineers discuss architecture decisions, absorbing best practices for error handling and logging that I later applied in my own workflow designs.",
        "Contributed to code reviews by testing Informatica mappings in development environments, verifying that transformations produced expected outputs before promoting changes to production schedulers."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "Sqoop",
        "Informatica PowerCenter",
        "Hive",
        "MapReduce",
        "Oracle",
        "SQL",
        "Linux",
        "Shell Scripting"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Certified Salesforce Developer PD1"
  ]
}