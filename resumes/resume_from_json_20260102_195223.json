{
  "name": "Yallaiah Onteru",
  "title": "Senior Software Engineer - Azure Cloud Infrastructure & AI Solutions",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bring 10 years of hands-on experience building AI-powered solutions across Insurance, Healthcare, Banking, and Consulting domains, focusing on Azure Cloud Infrastructure, Storage Management, and full-stack web applications with React and Python.",
    "Architect scalable Azure Storage management systems using React, Fluent UI, and Node.js that handle millions of users globally with weekly release cadence through CI/CD pipelines and Azure DevOps for continuous improvement.",
    "Design Agentic workflows using OpenAI GPT models, Azure AI Services, and prompt engineering techniques to create intelligent multi-agent systems with Model Context Protocol (MCP) for agent-to-agent communication.",
    "Build RAG pipelines integrating Azure Cognitive Search, vector databases, and LLMs to deliver context-aware responses with improved accuracy through fine-tuning and evaluation frameworks for enterprise customers.",
    "Develop full-stack applications with React, Node.js, Jest, and Fluent UI components following Agile methodologies and accessibility standards (WCAG) to ensure inclusive user experiences across devices.",
    "Implement authentication and authorization mechanisms meeting Microsoft Cloud security screening requirements using Azure Active Directory and role-based access controls for enterprise-grade protection.",
    "Optimize performance and scalability of Azure Storage solutions handling petabyte-scale data across globally distributed datacenters using efficient REST APIs and cloud architecture patterns.",
    "Collaborate with designers, customer teams, and engineers from diverse backgrounds to deliver revolutionary web experiences that empower organizations to manage cloud storage at scale.",
    "Guide adoption by embedding closely with customer teams, understanding their needs through regular meetings, and providing technical leadership to remove blockers early in development cycles.",
    "Apply machine learning techniques including model fine-tuning, evaluations, and speech services integration to enhance AI-powered features in production systems serving strategic enterprise customers.",
    "Maintain growth mindset by rapidly learning new concepts and technologies, adjusting plans to protect delivery, and working at startup speed with resources of a global company.",
    "Scope work and sequence delivery from prototype to production, making trade-offs between scope, speed, and quality while maintaining clarity and follow-through on outcomes that matter.",
    "Contribute directly to code when progress depends on it, participating in code reviews, debugging sessions, and troubleshooting to keep teams moving through technical challenges.",
    "Codify working patterns into reusable tools, playbooks, and components that others can use, fostering knowledge sharing and continuous improvement across teams.",
    "Integrate HTML, CSS, and JavaScript fundamentals with modern frameworks to build responsive interfaces that work seamlessly across web and mobile platforms.",
    "Deploy applications using Docker and Kubernetes in Azure cloud infrastructure, ensuring high availability and reliability through monitoring and observability tools.",
    "Engage in partnerships across Microsoft to realize shared goals, demonstrating values of respect, integrity, and accountability while creating a culture where everyone can thrive.",
    "Own technical delivery across multiple deployments, balancing customer needs with engineering excellence to build solutions that help shape the future of cloud storage management."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "JavaScript",
      "TypeScript",
      "HTML",
      "CSS",
      "SQL",
      "Bash/Shell",
      "R"
    ],
    "Web Development Frameworks": [
      "React",
      "Node.js",
      "Fluent UI",
      "Jest",
      "RESTful APIs",
      "Flask",
      "Django",
      "Fast API"
    ],
    "Generative AI & LLM Technologies": [
      "OpenAI GPT models",
      "Azure OpenAI Service",
      "Prompt Engineering",
      "Fine-tuning LLMs",
      "Model Evaluation Frameworks",
      "RAG Pipelines",
      "LangChain",
      "LangGraph",
      "Semantic Kernel"
    ],
    "Agentic Systems & Multi-Agent Frameworks": [
      "Model Context Protocol (MCP)",
      "Agent-to-Agent Communication",
      "Multi-Agent Systems",
      "Agentic Workflows",
      "Crew AI",
      "AutoGen"
    ],
    "Azure Cloud Platform": [
      "Azure Storage Services",
      "Azure AI Services",
      "Azure Cognitive Services",
      "Azure Cognitive Search",
      "Azure Active Directory",
      "Azure DevOps",
      "Azure Data Factory",
      "Azure Databricks"
    ],
    "Machine Learning & Deep Learning": [
      "TensorFlow",
      "PyTorch",
      "Scikit-Learn",
      "XGBoost",
      "Transformers",
      "BERT",
      "Transfer Learning",
      "Model Fine-Tuning"
    ],
    "Natural Language Processing": [
      "spaCy",
      "NLTK",
      "Hugging Face Transformers",
      "Speech Services",
      "TF-IDF",
      "Llama Index",
      "OpenAI APIs"
    ],
    "Big Data & Analytics": [
      "Apache Spark",
      "PySpark",
      "Databricks",
      "Apache Kafka",
      "Apache Airflow",
      "Hadoop",
      "Hive"
    ],
    "Databases & Storage": [
      "Azure Cosmos DB",
      "PostgreSQL",
      "MongoDB",
      "Snowflake",
      "Redis",
      "Vector Databases",
      "Elasticsearch"
    ],
    "DevOps & CI/CD": [
      "Git",
      "GitHub Actions",
      "Azure DevOps",
      "Docker",
      "Kubernetes",
      "Terraform",
      "CI/CD Pipelines"
    ],
    "Security & Compliance": [
      "Authentication/Authorization",
      "Azure AD",
      "Role-Based Access Control",
      "Microsoft Cloud Security",
      "WCAG Accessibility Standards"
    ],
    "Development Methodologies": [
      "Agile Development",
      "Continuous Improvement",
      "Code Reviews",
      "Test-Driven Development",
      "Performance Optimization"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Prototype multi-agent systems using LangGraph, Model Context Protocol (MCP), and agent-to-agent communication patterns to handle Insurance policy validation workflows, reducing manual review time through automated reasoning.",
        "Integrate Azure OpenAI Service with React-based web interfaces to create intelligent storage management dashboards that allow users to query petabyte-scale Insurance data using natural language prompts and receive visual insights.",
        "Configure RAG pipelines combining Azure Cognitive Search, vector databases, and prompt engineering to retrieve relevant Insurance regulations and compliance documents, ensuring accurate responses for customer inquiries.",
        "Establish CI/CD workflows using Azure DevOps and GitHub Actions to deploy React applications with Fluent UI components weekly across globally distributed datacenters, maintaining high availability for millions of users.",
        "Refactor Node.js backend services to support Agentic workflows that coordinate between multiple Azure AI Services, including Speech recognition for voice-enabled Insurance claim submissions and document processing.",
        "Test Jest suites covering React components and API endpoints to catch bugs early, participating in code reviews with teammates to maintain code quality and share knowledge about new prompt engineering techniques.",
        "Secure authentication flows using Azure Active Directory and role-based access controls that meet Microsoft Cloud security screening requirements, protecting sensitive Insurance customer data across all storage operations.",
        "Troubleshoot performance bottlenecks in React interfaces by profiling JavaScript execution and optimizing component rendering, achieving faster load times for Insurance agents accessing policy information during peak hours.",
        "Collaborate with designers to implement WCAG accessibility standards in web applications, ensuring screen readers work properly with Fluent UI components and keyboard navigation supports users with disabilities.",
        "Embed with State Farm customer teams to understand their needs around claim processing automation, guiding adoption of new AI-powered features through regular meetings and hands-on training sessions.",
        "Fine-tune OpenAI GPT models on Insurance-specific datasets using Azure Machine Learning to improve accuracy of policy recommendation systems, running evaluations to measure performance gains before production deployment.",
        "Coordinate Databricks and PySpark jobs that process Insurance claims data in real-time, feeding results into multi-agent systems that route claims to appropriate departments based on complexity and urgency.",
        "Debug Azure Storage connectivity issues by analyzing REST API logs and network traces, working with Microsoft support teams to resolve configuration problems affecting data replication across regions.",
        "Document working patterns and reusable React components in internal playbooks, making it easier for other engineers to build consistent user interfaces and reducing development time for new features.",
        "Adjust project plans when priorities shift due to regulatory changes in Insurance industry, reallocating resources and removing blockers to protect delivery timelines while maintaining quality standards.",
        "Review proof-of-concept implementations of Model Context Protocol agents, providing feedback on architecture decisions and suggesting improvements to make agent-to-agent communication more reliable under high load conditions."
      ],
      "environment": [
        "Python",
        "JavaScript",
        "React",
        "Node.js",
        "Fluent UI",
        "Jest",
        "Azure OpenAI Service",
        "Azure AI Services",
        "Azure Cognitive Search",
        "Azure Storage",
        "Azure Active Directory",
        "Azure DevOps",
        "Databricks",
        "PySpark",
        "LangGraph",
        "Model Context Protocol",
        "RAG Pipelines",
        "Vector Databases",
        "Docker",
        "Kubernetes",
        "GitHub Actions",
        "RESTful APIs",
        "HTML",
        "CSS",
        "Prompt Engineering",
        "Multi-Agent Systems",
        "Speech Services"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Constructed multi-agent systems using LangChain and Databricks to process Healthcare patient records while maintaining HIPAA compliance, coordinating agents to extract medical entities and flag potential drug interactions.",
        "Assembled React dashboards with Fluent UI that visualized patient outcomes and treatment effectiveness, connecting to Azure Cosmos DB through Node.js APIs to fetch aggregated data for Healthcare providers.",
        "Crafted prompt engineering strategies for Azure OpenAI models to generate patient discharge summaries from clinical notes, testing different prompt templates to balance accuracy with medical terminology consistency.",
        "Validated RAG pipeline outputs against Healthcare regulations by comparing generated responses to verified FDA guidelines, identifying cases where models hallucinated information and adjusting retrieval parameters.",
        "Delivered proof-of-concept applications demonstrating LangGraph workflows that automated prior authorization requests, showing Healthcare administrators how AI could reduce processing time from days to hours.",
        "Monitored Azure Storage performance metrics to ensure patient data remained accessible during peak clinic hours, addressing latency issues by implementing caching strategies and optimizing database queries.",
        "Participated in Agile sprint planning sessions where team discussed feature priorities for Healthcare portal, taking notes on customer feedback and breaking down complex AI requirements into manageable tasks.",
        "Diagnosed memory leaks in Node.js services that processed patient appointment scheduling requests, using Chrome DevTools to identify problematic event listeners and fixing issues before they affected production.",
        "Enhanced accessibility of web forms used by Healthcare staff to input patient information, ensuring WCAG compliance by adding proper ARIA labels and testing keyboard navigation with assistive technologies.",
        "Partnered with Johnson & Johnson clinical teams to understand HIPAA requirements for data handling, implementing encryption for data at rest and in transit using Azure Key Vault and TLS protocols.",
        "Experimented with fine-tuning smaller LLM models on Healthcare datasets to reduce inference costs, comparing performance against larger models and documenting trade-offs between speed and accuracy.",
        "Orchestrated PySpark jobs on Databricks that cleaned and transformed patient records from multiple hospital systems, resolving data quality issues like missing values and inconsistent date formats.",
        "Investigated CI/CD pipeline failures that blocked deployment of updated Healthcare portal, tracing problems to incompatible Node.js package versions and coordinating with DevOps team to update dependencies.",
        "Mentored junior developers on React best practices during code reviews, explaining component lifecycle methods and suggesting performance optimizations like memoization to prevent unnecessary re-renders."
      ],
      "environment": [
        "Python",
        "JavaScript",
        "React",
        "Node.js",
        "Fluent UI",
        "Azure OpenAI Service",
        "Azure AI Services",
        "Azure Cosmos DB",
        "Azure Storage",
        "Azure Key Vault",
        "Databricks",
        "PySpark",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "RAG Pipelines",
        "Prompt Engineering",
        "Docker",
        "Kubernetes",
        "Azure DevOps",
        "RESTful APIs",
        "HTML",
        "CSS",
        "HIPAA Compliance",
        "TLS Encryption"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Trained machine learning models using Scikit-Learn and XGBoost to predict patient readmission risk for Maine Healthcare facilities, validating results against historical data to ensure HIPAA-compliant handling of protected health information.",
        "Deployed Flask REST APIs on AWS EC2 instances that served model predictions to Healthcare administrators, implementing authentication using AWS IAM roles to restrict access to authorized state employees only.",
        "Processed patient records stored in AWS S3 using PySpark on EMR clusters, applying data quality checks to identify incomplete records and coordinating with data stewards to resolve issues.",
        "Created data pipelines with Apache Airflow that extracted Healthcare data from legacy systems, transformed fields to match modern schemas, and loaded results into AWS RDS for analysis by state health officials.",
        "Analyzed feature importance from trained models to explain which patient characteristics most influenced readmission predictions, presenting findings to Maine Healthcare policy teams in non-technical language they could understand.",
        "Resolved AWS Lambda timeout errors that occurred when processing large Healthcare datasets, refactoring code to use batch processing and adjusting memory allocations to handle peak loads during monthly reporting.",
        "Attended weekly stakeholder meetings where Healthcare administrators discussed public sector requirements for patient data reporting, taking feedback and adjusting model outputs to align with state regulatory standards.",
        "Verified HIPAA compliance of ML workflows by conducting security audits of data access patterns, ensuring patient identifiers were properly anonymized before datasets left secure AWS environments.",
        "Improved model accuracy by exploring different feature engineering approaches, testing polynomial features and interaction terms to capture complex relationships in Healthcare data without overfitting.",
        "Documented model development process in technical reports for state archives, including data sources, preprocessing steps, model selection rationale, and validation results to support reproducibility.",
        "Collaborated with AWS support to troubleshoot S3 bucket permission issues that prevented Airflow DAGs from reading patient files, learning about cross-account access policies and implementing proper IAM configurations.",
        "Prepared Python scripts using Pandas and NumPy to aggregate Healthcare metrics across Maine counties, generating summary statistics that informed state budget allocation decisions for rural health programs."
      ],
      "environment": [
        "Python",
        "Scikit-Learn",
        "XGBoost",
        "PySpark",
        "AWS S3",
        "AWS EC2",
        "AWS RDS",
        "AWS EMR",
        "AWS Lambda",
        "AWS IAM",
        "Apache Airflow",
        "Flask",
        "Pandas",
        "NumPy",
        "RESTful APIs",
        "HIPAA Compliance",
        "PostgreSQL"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Developed fraud detection models using Random Forest and Logistic Regression on transaction data stored in AWS Redshift, achieving high precision scores while minimizing false positives that inconvenienced Banking customers.",
        "Extracted features from transaction histories using SQL queries against AWS RDS, calculating rolling averages and transaction frequency metrics that captured customer spending patterns for risk assessment models.",
        "Visualized Banking fraud patterns using Tableau dashboards that highlighted geographic hotspots and temporal trends, presenting insights to risk management teams who used findings to adjust monitoring thresholds.",
        "Ensured PCI-DSS compliance of data handling procedures by working with security teams to encrypt credit card numbers in databases, implementing tokenization strategies that protected sensitive Banking information.",
        "Ran A/B tests on fraud alert systems to measure impact of different threshold settings, analyzing results with statistical hypothesis testing to determine optimal configurations that balanced security with customer experience.",
        "Responded to production incidents where fraud detection models flagged legitimate transactions as suspicious, investigating root causes and adjusting model parameters to reduce false alarm rates.",
        "Utilized AWS Glue to automate ETL workflows that consolidated transaction data from multiple Banking systems, resolving schema inconsistencies and ensuring data freshness for downstream ML models.",
        "Learned about Banking regulations during compliance training sessions, understanding how PCI-DSS requirements influenced model deployment decisions and data retention policies for transaction records.",
        "Fixed data pipeline failures caused by unexpected data format changes from upstream Banking systems, adding validation checks to catch issues early and alert team members before models consumed bad data.",
        "Contributed to Python codebase for model training scripts, participating in code reviews where senior engineers provided feedback on vectorization techniques that improved training performance using NumPy operations."
      ],
      "environment": [
        "Python",
        "Scikit-Learn",
        "Random Forest",
        "Logistic Regression",
        "SQL",
        "AWS Redshift",
        "AWS RDS",
        "AWS Glue",
        "Tableau",
        "Pandas",
        "NumPy",
        "PCI-DSS Compliance",
        "A/B Testing",
        "ETL Pipelines"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Built Hadoop MapReduce jobs that processed client transaction logs from retail systems, learning to optimize mapper and reducer functions to handle growing data volumes efficiently.",
        "Configured Informatica PowerCenter workflows to extract data from Oracle databases, transform records according to business rules, and load results into Hive tables for Consulting team analysis.",
        "Operated Apache Sqoop commands to import relational database tables into HDFS, troubleshooting connection errors and studying documentation to understand proper parameter configurations.",
        "Maintained Hive tables used by Consulting analysts for ad-hoc queries, adding new partitions as data arrived and helping team members write efficient HiveQL queries to avoid full table scans.",
        "Discovered data quality issues by comparing row counts between source systems and Hadoop cluster, working with client teams to identify missing files and implement checksums for validation.",
        "Supported production data pipelines by monitoring job logs for failures, attempting to restart failed tasks and escalating complex issues to senior engineers when problems persisted.",
        "Gained experience with shell scripting by automating routine Hadoop cluster maintenance tasks, gradually taking on more responsibility as comfort level with Linux command line increased.",
        "Attended team meetings where Consulting project requirements were discussed, asking questions to clarify requirements and understanding how data engineering work supported broader client objectives."
      ],
      "environment": [
        "Hadoop",
        "MapReduce",
        "Hive",
        "HiveQL",
        "Apache Sqoop",
        "Informatica PowerCenter",
        "Oracle",
        "HDFS",
        "Linux",
        "Shell Scripting"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}