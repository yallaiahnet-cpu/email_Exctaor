{
  "name": "Yallaiah Onteru",
  "title": "Senior AI Developer",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Experienced GenAI Data Scientist with 10 years in AI/ML, delivering enterprise-grade solutions using LLMs, RAG pipelines, embeddings, and vector databases across finance, healthcare, manufacturing, and public sector domains.",
    "Specialized in building cloud-native, with NLP applications including document classification, semantic search, summarization, sentiment analysis, and named entity recognition for finance, healthcare, and manufacturing.",
    "Proficient in a comprehensive range of Machine Learning algorithms such as Linear/Logistic Regression, Decision Trees, Random Forest, XGBoost, LightGBM, K-Means and Deep Learning frameworks such as PyTorch, TensorFlow, Keras for prototyping and deploying production-ready neural networks, including CNNs, RNNs, and GANs.",
    "Applied Vibe Coding principles to rapidly translate business needs into AI-native prototypes and workflows, delivering functional demos that showcased clear value and accelerated customer decision-making.",
    "Performed Exploratory Data Analysis (EDA) and applied statistical techniques (hypothesis testing, correlation, distributions, and outlier detection) to identify key patterns, validate assumptions, and generate actionable insights for data-driven decision making.",
    "Strong in A/B testing, ANOVA, regression, and hypothesis testing also Built ETL pipelines and did advanced analysis using Python, SQL, and R.",
    "Specialized in cloud-native ML & GenAI applications(RAG, embeddings, Google Gemini, agentic multi-agent AI systems), Experience in AI/ML, specializing in GitHub Copilot and Amazon Q.",
    "Created Power BI dashboards to support real-time decisions in planning and operations and Used Tableau, Power BI, and D3.js for visualizations.",
    "Developed GenAI architecture for cloud-native RAG and embedding-based search tools and also Developed RAG pipelines using LangChain, LangGraph, LlamaIndex, and OpenAI for document search.",
    "Cleaned and prepared data using Pandas, NumPy, Matplotlib, Seaborn (Python), and dplyr, ggplot2, caret (R). Strong in Python and R; also used Java and Scala for backend tasks.",
    "Extensive experience building NLP, Agentic AI, and Generative AI solutions with Claude AI and Crew AI, deploying transformers, embeddings, and domain-specific LLMs into production systems to power contextual search, personalized recommendations, and intelligent conversational interfaces.",
    "Worked with Apache Spark, Hadoop, and Kafka for big data and streaming jobs. Built data pipelines with Apache Airflow and Talend, Proficient in GPU optimization for ML/LLM pipelines, including multi-GPU training, GPU-backed inference, and GPU workload scaling across AWS, Azure, and GCP.",
    "Delivered Product Support for RCA, Defect Triaging, and Reduction for GenAI-powered assistants using Palantir AIP. Production Accuracy focusing on submission workflows, employing automation.",
    "Experienced in healthcare data science, including claims analytics, HIPAA-compliant pipelines, and time series forecasting for treatment demand and clinical operations.",
    "Strong in SQL; used PostgreSQL, MySQL, MongoDB, and Cassandra. Developed extensive GenAI and data science experience to understand complex project pipelines, contributing to targeted code refactoring and small enhancements. Supported the integration of LLMs and embeddings into AI-powered workflows for intelligent access, retrieval, and automation.",
    "Improved operational efficiency across inventory and asset-heavy systems using regression analysis, clustering, and anomaly detection to eliminate redundancies and delays.",
    "Integrated dashboards with React.js, worked with frontend teams using D3.js and REST APIs.Strong knowledge of TypeScript; familiar with Foundry plugin development.",
    "Deployed models on AWS SageMaker, Azure ML, and Google AI Platform. Used AWS, Azure, and GCP for cloud-based model deployment.",
    "Used Git for version control and worked with AI frameworks like Palantir AIP and LangChain agents. Hands-on with LLMs, embeddings, vector databases, and GenAI use cases in finance and manufacturing. Trained models using GPUs for faster performance.",
    "Always learning new tools and methods in data science. Good at explaining tech topics to non-technical teams. Mentored junior data scientists and led data teams. Promotes a productive, professional, and friendly work environment."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "Machine Learning Models": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "Keras",
      "XGBoost",
      "LightGBM",
      "H2O",
      "AutoML",
      "Mllib"
    ],
    "Deep Learning Models": [
      "Convolutional Neural Networks (CNNs)",
      "Recurrent Neural Networks (RNNs)",
      "LSTMs",
      "Transformers",
      "Generative Models",
      "Attention Mechanisms",
      "Transfer Learning",
      "Fine-tuning LLMs"
    ],
    "Statistical Techniques": [
      "A/B Testing",
      "ANOVA",
      "Hypothesis Testing",
      "PCA",
      "Factor Analysis",
      "Regression (Linear, Logistic)",
      "Clustering (K-Means)",
      "Time Series (Prophet)"
    ],
    "Natural Language Processing": [
      "spaCy",
      "NLTK",
      "Hugging Face Transformers",
      "BERT",
      "GPT",
      "Stanford NLP",
      "TF-IDF",
      "LSI",
      "Lang Chain",
      "Llama Index",
      "OpenAI APIs",
      "MCP",
      "RAG Pipelines",
      "Crew AI",
      "Claude AI"
    ],
    "Data Manipulation & Visualization": [
      "Pandas",
      "NumPy",
      "SciPy",
      "Dask",
      "Apache Arrow",
      "seaborn",
      "matplotlib",
      "Seaborn",
      "Plotly",
      "Bokeh",
      "ggplot2",
      "Tableau",
      "Power BI",
      "D3.js"
    ],
    "Big Data Frameworks": [
      "Apache Spark",
      "Apache Hadoop",
      "Apache Flink",
      "Apache Kafka",
      "HBase",
      "Spark Streaming",
      "Hive",
      "MapReduce",
      "Databricks",
      "Apache Airflow",
      "dbt"
    ],
    "ETL & Data Pipelines": [
      "Apache Airflow",
      "AWS Glue",
      "Azure Data Factory",
      "Informatica",
      "Talend",
      "Apache NiFi",
      "Apache Beam",
      "Informatica PowerCenter",
      "SSIS"
    ],
    "Cloud Platforms": [
      "AWS (S3, SageMaker, Lambda, EC2, RDS, Redshift, Bedrock)",
      "Azure (ML Studio, Data Factory, Databricks, Cosmos DB)",
      "GCP (Big Query, Vertex AI, Cloud SQL)"
    ],
    "Web Technologies": [
      "REST APIs",
      "Flask",
      "Django",
      "Fast API",
      "React.js"
    ],
    "Statistical Software": [
      "R (dplyr, caret, ggplot2, tidyr)",
      "SAS",
      "STATA"
    ],
    "Databases": [
      "PostgreSQL",
      "MySQL",
      "Oracle",
      "Snowflake",
      "MongoDB",
      "Cassandra",
      "Redis",
      "Snowflake Elasticsearch",
      "AWS RDS",
      "Google Big Query",
      "SQL Server",
      "Netezza",
      "Teradata"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes"
    ],
    "MLOps & Deployment": [
      "ML flow",
      "DVC",
      "Kubeflow",
      "Docker",
      "Kubernetes",
      "Flask",
      "Fast API",
      "Streamlit"
    ],
    "Streaming & Messaging": [
      "Apache Kafka",
      "Spark Streaming",
      "Amazon Kinesis"
    ],
    "DevOps & CI/CD": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Jenkins",
      "GitHub Actions",
      "Terraform"
    ],
    "Development Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "RStudio",
      "Google Colab",
      "Anaconda"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Collected and cleansed policyholder and claims datasets including unstructured adjuster notes using Python (Pandas, NumPy) and stored curated data in Amazon S3 for underwriting and pricing models.",
        "Automated data ingestion pipelines with AWS Glue and Amazon EMR (Spark), delivering clean, timely data into Snowflake on AWS to support rating factor analysis.",
        "Optimized SQL queries in Amazon Redshift and Snowflake on AWS using partitioning and query tuning, enabling faster insights for underwriters and actuaries.",
        "Versioned datasets and experiments using Amazon S3 with MLflow on SageMaker, ensuring reproducibility of pricing and risk models.",
        "Developed ML models (Scikit-learn, XGBoost, LightGBM) on Amazon SageMaker to predict claim frequency and severity, improving pricing accuracy and reducing loss ratios.",
        "Designed deep learning models (LSTMs, Transformers) in SageMaker with PyTorch/TensorFlow for reserve estimation and renewal forecasting.",
        "Executed statistical analyses (hypothesis testing, ANOVA, PCA) with Python notebooks in Amazon SageMaker Studio to validate rating factors and refine underwriting strategies.",
        "Automated retraining pipelines using AWS Step Functions, Lambda, and SageMaker Pipelines, ensuring continuous adaptation of pricing models to market changes.",
        "Built fraud detection pipelines combining streaming data from Amazon Kinesis with anomaly detection models in SageMaker, flagging high-risk claims at FNOL stages.",
        "Designed RAG pipelines on AWS using LangChain and SageMaker endpoints, storing embeddings in Amazon OpenSearch Service and Amazon DynamoDB for vector metadata and low-latency retrieval.",
        "Implemented Amazon Bedrock-powered Generative AI solutions, integrating foundation models into claims processing and policyholder support workflows, enabling scalable, secure, and low-latency LLM inference without managing custom infrastructure.",
        "Developed Agentic AI assistants with SageMaker and Bedrock LLM APIs to summarize adjuster notes, triage claims, and assist investigators in compliance monitoring.",
        "Designed and deployed fast, pragmatic AI solutions using a mix of LLMs, agents, APIs, and cloud tools, focusing on high-impact outcomes and reducing complexity across business processes.",
        "Built FastAPI microservices with async support to create fast and reliable AI inference endpoints. Used FastAPI's high performance and async features to handle more requests smoothly and speed up insurance and healthcare workflows.",
        "Developed Java-based microservices integrated with AWS Lambda and SageMaker endpoints, enabling secure modelserving for fraud scoring APIs.",
        "Developed multi-agent AI workflows with LangChain agents orchestrating retrieval, summarization, and compliance checks, improving adjuster productivity and audit accuracy.",
        "Integrated Anthropic Claude via Bedrock API for long-context summarization of adjuster notes and compliancesensitive documents, improving accuracy and reducing hallucinations in policy workflows."
      ],
      "environment": [
        "AWS Cloud (S3, Glue, EMR, Redshift, DynamoDB, Kinesis, OpenSearch, SageMaker, Bedrock, Lambda, Step Functions, IAM, CloudTrail, Guard Duty, QuickSight)",
        "Python (Pandas, NumPy, Scikit-learn, PyTorch, TensorFlow, Keras, LangChain, MLflow)",
        "Snowflake on AWS",
        "Docker",
        "Git",
        "Code Whisperer",
        "Code Commit",
        "SQL",
        "NLP (NLTK, SpaCy, Transformers)",
        "Data Visualization (Matplotlib, Seaborn, QuickSight)",
        "CI/CD (CodePipeline, Jenkins)",
        "Monitoring (CloudWatch, Evidently AI)"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Collected, cleaned, and structured large-scale financial datasets from internal and external systems using Cloud Data Fusion, BigQuery, and DBT, creating analysis-ready data for modeling.",
        "Engineered and optimized Apache Spark ETL workflows on Dataproc, enabling near real-time analytics on loan portfolios, investments, and transactions.",
        "Developed and deployed credit risk and forecasting models using Python, Scikit-learn, XGBoost, LightGBM, and Random Forest via Vertex AI, improving predictive accuracy and risk assessment.",
        "Built deep learning churn prediction models with TensorFlow on Vertex AI, integrating outputs into Looker dashboards for executive insights on customer retention.",
        "Used GAN-based models to simulate synthetic financial transaction sequences, supporting fraud detection pipelines and improving recall on rare fraud scenarios.",
        "Leveraged H2O AutoML and Spark MLlib to rapidly prototype credit risk models, benchmarking against custom XGBoost/LightGBM implementations to validate feature importance and accuracy gains.",
        "Designed and implemented NLP pipelines using spaCy, NLTK, and Hugging Face Transformers for ticketing automation, sentiment analysis, entity recognition, and document classification.",
        "Applied graph algorithms in Neo4j to analyze complex financial networks, enhancing fraud detection, network risk assessment, and investment strategies.",
        "Containerized ML models using Docker and deployed on GKE with Helm charts, ensuring scalable, reliable inference APIs.",
        "Built FastAPI and OpenAPI endpoints to provide downstream systems seamless access to predictive insights across risk, churn, and ticketing workflows.",
        "Automated CI/CD pipelines and infrastructure provisioning using Cloud Build and Terraform, ensuring reproducible, secure, and scalable production workflows.",
        "Implemented model monitoring and explainability with MLflow, SHAP, and LIME, tracked drift, and ensured interpretability for auditors and stakeholders.",
        "Automated model retraining with Vertex AI Pipelines, Cloud Functions, and Pub/Sub to keep models updated with evolving market and portfolio data.",
        "Built interactive Looker and Plotly dashboards to visualize model outputs, risk scores, and KPIs, and performed feature engineering/preprocessing using BigQuery ML, PySpark, and Python for efficient predictive modeling."
      ],
      "environment": [
        "GCP (BigQuery, Vertex AI, Cloud Data Fusion, Dataproc, Cloud Build, Cloud Functions)",
        "Python (Scikit-learn, XGBoost, LightGBM, TensorFlow, PyTorch, Pandas, NumPy)",
        "Apache Spark",
        "Apache Kafka",
        "Docker",
        "Kubernetes",
        "Helm",
        "FastAPI",
        "OpenAPI",
        "MLflow",
        "SHAP",
        "LIME",
        "Neo4j",
        "Looker",
        "Plotly",
        "DBT",
        "Terraform"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Gathered and processed large-scale patient data, including claims, EHR records, and member histories, using PySpark on AWS EMR, ensuring clean, structured inputs for downstream ML and forecasting models.",
        "Built scalable feature engineering pipelines with Spark DataFrames and Pandas UDFs to prepare healthcare features for forecasting, fraud detection, and claims adjudication.",
        "Developed patient volume forecasting and predictive claims models in Python using Scikit-learn and AWS SageMaker, optimizing resource allocation and operational planning.",
        "Used SAS and STATA for statistical validation of claims-based models, performing regression, survival analysis, and hypothesis testing on healthcare outcomes.",
        "Engineered deep learning models for medical imaging using Keras and TensorFlow, deployed on AWS EKS, enabling real-time inference for early diagnostic support.",
        "Enhanced claims adjudication and model inference with ONNX and NVIDIA TensorRT on AWS GPU instances, reducing response times for clinical and claims decisions.",
        "Designed fraud detection workflows with MLflow and AWS S3 for model versioning, reproducibility, and consistent experiment tracking.",
        "Developed NLP pipelines using spaCy and Hugging Face Transformers to analyze patient feedback, call transcripts, and provider notes, generating actionable insights for patient engagement and service improvement.",
        "Orchestrated ETL and retraining pipelines using AWS Data Pipeline and Lambda, integrating eligibility files, claims feeds, and policy updates, reducing retraining cycle times.",
        "Containerized ML microservices with Docker and deployed scalable APIs on AWS EKS with Helm, enabling clinical decision support and provider-facing integrations via FastAPI/OpenAPI.",
        "Implemented monitoring and observability frameworks with Prometheus, Grafana, and CloudWatch, tracking model drift, PHI/PII anomalies, and infrastructure health to ensure compliance and reliability.",
        "Ensured data security and compliance across AWS services with IAM, KMS encryption, and audit logging, collaborating with stakeholders to align analytics solutions with best practices in cost, scalability, and security."
      ],
      "environment": [
        "Python",
        "Scikit-learn",
        "PySpark",
        "AWS EMR",
        "AWS SageMaker",
        "Keras",
        "TensorFlow",
        "Docker",
        "AWS EKS",
        "MLflow",
        "AWS S3",
        "Spark DataFrames",
        "spaCy",
        "Hugging Face Transformers",
        "Databricks Notebooks",
        "ONNX",
        "TensorRT",
        "Kafka (AWS MSK)",
        "AWS Data Pipeline",
        "AWS Lambda",
        "CloudWatch",
        "Prometheus",
        "Grafana",
        "Helm",
        "FastAPI"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "Charlotte, NC",
      "responsibilities": [
        "Collected, cleaned, and structured millions of financial transaction records using Python, Pandas, and SQL Server, preparing high-quality datasets for predictive modeling.",
        "Designed and implemented big data pipelines with Apache Spark and Hadoop on Azure HDInsight, efficiently processing large-scale transaction data.",
        "Developed real-time data ingestion and streaming pipelines with Apache Kafka and Spark Streaming on Azure HDInsight, detecting spikes and anomalies in financial transactions.",
        "Automated ETL workflows using Apache Airflow, orchestrating extraction and transformation across multiple structured and unstructured data sources.",
        "Built time-series forecasting models using ARIMA and deep learning models with LSTM, TensorFlow, and Keras, improving transaction event predictions.",
        "Applied ensemble ML models, including Random Forest and XGBoost, deploying scalable predictive solutions via Azure ML Studio, reducing false positives and enhancing forecast accuracy.",
        "Enhanced Spark pipelines with Scala-based UDFs for optimized feature transformations on large-scale financial datasets.",
        "Prototyped Apache NiFi and Apache Beam pipelines for real-time transaction stream processing, benchmarking against existing Spark Streaming workflows.",
        "Performed dimensionality reduction using PCA and t-SNE, accelerating model training and improving efficiency for large datasets.",
        "Developed Power BI dashboards to visualize transaction forecasts, anomalies, and operational metrics, supporting capacity planning and executive decision-making."
      ],
      "environment": [
        "Python",
        "Pandas",
        "SQL Server",
        "Apache Spark",
        "Hadoop",
        "Azure HDInsight",
        "Apache Kafka",
        "Spark Streaming",
        "Apache Airflow",
        "Azure ML Studio (classic)",
        "TensorFlow",
        "Keras",
        "Random Forest",
        "XGBoost",
        "ARIMA",
        "PCA",
        "t-SNE",
        "Power BI",
        "Agile",
        "Git",
        "Scala",
        "Apache NiFi",
        "Apache Beam"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, India",
      "responsibilities": [
        "Configured Sqoop for data transfer between relational databases and Hadoop ecosystem, automating client data ingestion from MySQL and PostgreSQL.",
        "Deployed Hadoop clusters for fintech data processing automation, implementing HDFS for storing large volumes of financial transaction data and risk analytics change.",
        "Wrote complex SQL queries and stored procedures to derive actionable insights (DAU, MAU, churn, conversion rates) from diverse data sources including Firebase, Mixpanel, and internal databases.",
        "Automated data ingestion and transformation tasks with Python (Pandas, NumPy) and Azure Functions, reducing manual operations and supporting real-time analytics requirements.",
        "Visualized app KPIs and user behavior metrics in Power BI dashboards, leveraging DAX, and integrated JavaScriptbased custom visuals for enhanced analytics.",
        "Collaborated with stakeholders to define event tracking requirements, implemented SDK integrations, and supported mobile developer teams with API-based instrumentation.",
        "Applied marketing and product analytics techniques including A/B testing, cohort/user segmentation, and retention analysis to optimize user engagement and drive data-driven business strategies.",
        "Followed best practices for code versioning with Git and process documentation with JIRA, facilitating efficient teamwork in an Agile environment."
      ],
      "environment": [
        "Azure Data Factory",
        "Azure SQL Database",
        "Azure Virtual Machines",
        "Python",
        "Pandas",
        "NumPy",
        "Power BI",
        "DAX",
        "JavaScript",
        "SQL",
        "Firebase",
        "Mixpanel",
        "REST APIs",
        "Informatica",
        "Scikit-Learn",
        "JIRA",
        "Git",
        "Agile",
        "Visio"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2016"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Developer"
  ]
}