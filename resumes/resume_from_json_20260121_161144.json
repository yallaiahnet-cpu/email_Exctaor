{
  "name": "Yallaiah Onteru",
  "title": "Senior AI Engineer | Technical Lead - Agentic AI & Azure OpenAI Solutions",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bring 10 years of hands-on experience across Insurance, Healthcare, Banking, and Consulting sectors, building agentic AI systems with Azure OpenAI, GPT models, LangChain, and LangGraph frameworks for enterprise-scale deployments.",
    "Architect multi-agent systems using MCP protocol to coordinate autonomous AI agents that handle complex workflows, reducing manual intervention while maintaining compliance with industry regulations across insurance and healthcare domains.",
    "Build RAG pipelines integrating Azure Cognitive Search with GPT-4 models to retrieve context-aware responses from knowledge bases, improving answer accuracy for customer-facing applications in regulated industries.",
    "Design LangGraph workflows orchestrating sequential and parallel agent tasks for insurance claims processing, where agents validate documents, assess risk, and generate recommendations through structured decision trees.",
    "Deploy Azure OpenAI endpoints with custom prompt engineering strategies that balance token efficiency and response quality, testing multiple temperature settings to find optimal configurations for production chatbots.",
    "Develop proof-of-concept agentic systems demonstrating feasibility of autonomous decision-making for healthcare providers, where AI agents triage patient inquiries and route cases based on symptom analysis and urgency scoring.",
    "Integrate Model Context Protocol standards enabling seamless agent-to-agent communication across distributed systems, where insurance underwriting agents share policy data with fraud detection agents through standardized message formats.",
    "Configure Azure Databricks clusters running PySpark jobs that preprocess insurance claims data at scale, creating embeddings from unstructured text fields before feeding into vector databases for semantic search capabilities.",
    "Implement LangChain memory components maintaining conversation context across user sessions, storing chat history in Cosmos DB to provide continuity when customers return days later with follow-up questions about their policies.",
    "Optimize GPT model fine-tuning experiments using Azure ML pipelines, adjusting hyperparameters through iterative training runs to improve domain-specific language understanding for medical terminology in healthcare applications.",
    "Coordinate multi-agent frameworks like CrewAI where specialized agents collaborate on document analysis tasks, with one agent extracting entities, another validating compliance rules, and a third generating summary reports for auditors.",
    "Establish vector database infrastructure using FAISS and Milvus for storing document embeddings, enabling sub-second similarity searches across millions of insurance policy documents when customers ask questions about coverage details.",
    "Monitor LLM response quality through prompt-response logging mechanisms that capture user feedback, identifying failure patterns where models hallucinate facts or provide outdated information requiring prompt template revisions.",
    "Apply semantic search techniques combining dense embeddings with keyword filters to retrieve relevant policy clauses, balancing precision and recall metrics when customers search for specific coverage scenarios in natural language queries.",
    "Troubleshoot API rate limiting issues with Azure OpenAI services during peak traffic periods, implementing retry logic with exponential backoff and request queuing to maintain service availability without exceeding quota limits.",
    "Conduct code reviews examining prompt injection vulnerabilities in LangChain applications, ensuring input validation prevents malicious users from manipulating agent behavior or accessing restricted data through crafted queries.",
    "Participate in planning sessions defining agent capabilities and interaction protocols for multi-step reasoning tasks, mapping out state transitions where agents gather information, evaluate options, and execute actions autonomously.",
    "Document LangGraph agent architectures using flowcharts showing decision nodes and conditional branches, helping team members understand complex workflows where agents route tasks based on context variables and business rules."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "LLM & AI Frameworks": [
      "Azure OpenAI",
      "GPT-4",
      "GPT-3.5",
      "LangChain",
      "LangGraph",
      "Hugging Face Transformers",
      "OpenAI APIs",
      "Llama Models",
      "Claude AI",
      "Semantic Kernel",
      "AutoGen"
    ],
    "Agentic AI & Orchestration": [
      "Multi-Agent Systems",
      "Model Context Protocol (MCP)",
      "CrewAI",
      "Agent-to-Agent Communication",
      "Agentic Workflows",
      "LLM Orchestration",
      "Autonomous Agents",
      "Task Delegation"
    ],
    "Machine Learning & Deep Learning": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "Keras",
      "XGBoost",
      "LightGBM",
      "CNNs",
      "RNNs",
      "LSTMs",
      "Transformers",
      "Transfer Learning",
      "Fine-tuning LLMs"
    ],
    "Natural Language Processing": [
      "RAG Pipelines",
      "Prompt Engineering",
      "Intent Recognition",
      "Context-Driven Summarization",
      "Embedding Models",
      "BERT",
      "spaCy",
      "NLTK",
      "TF-IDF",
      "Semantic Search"
    ],
    "Cloud Platforms & Services": [
      "Azure OpenAI Service",
      "Azure Databricks",
      "Azure ML Studio",
      "Azure Cognitive Search",
      "Azure Data Factory",
      "Cosmos DB",
      "Azure Data Lake",
      "Azure AKS",
      "AWS (S3, SageMaker, Lambda, Bedrock)"
    ],
    "Vector Databases & Search": [
      "FAISS",
      "Milvus",
      "Weaviate",
      "Cosmos DB Vector Search",
      "Pinecone",
      "Elasticsearch",
      "Knowledge Graphs",
      "Retrieval Fusion"
    ],
    "Big Data & Processing": [
      "Apache Spark",
      "PySpark",
      "Apache Hadoop",
      "Apache Kafka",
      "Databricks",
      "Hive",
      "Apache Airflow",
      "Spark Streaming"
    ],
    "Data Engineering & ETL": [
      "Azure Data Factory",
      "Apache Airflow",
      "dbt",
      "Informatica",
      "Sqoop",
      "Apache NiFi",
      "Delta Lake",
      "Data Pipelines"
    ],
    "MLOps & Model Lifecycle": [
      "MLflow",
      "Databricks MLflow",
      "Azure ML",
      "Model Deployment",
      "Unity Catalog",
      "DVC",
      "Model Monitoring",
      "A/B Testing"
    ],
    "Databases & Storage": [
      "SQL",
      "Cosmos DB",
      "PostgreSQL",
      "MySQL",
      "Snowflake",
      "MongoDB",
      "Delta Lake",
      "Azure SQL",
      "AWS RDS"
    ],
    "APIs & Development Tools": [
      "REST APIs",
      "FastAPI",
      "Flask",
      "Docker",
      "Kubernetes",
      "Git",
      "GitHub",
      "VS Code",
      "Jupyter Notebook"
    ],
    "Data Visualization": [
      "Tableau",
      "Power BI",
      "matplotlib",
      "Seaborn",
      "Plotly"
    ],
    "Compliance & Security": [
      "HIPAA",
      "PCI-DSS",
      "GDPR",
      "FDA Compliance",
      "Data Privacy",
      "Secure APIs"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Coordinate planning sessions defining multi-agent system architecture for insurance claims automation, mapping agent roles where document analyzers extract data, risk assessors evaluate coverage, and decision agents generate approval recommendations.",
        "Build LangGraph workflows connecting Azure OpenAI GPT-4 agents that process claims through sequential stages, starting with intake validation, moving through fraud detection checks, and ending with settlement amount calculations based on policy terms.",
        "Configure Azure Databricks clusters executing PySpark transformations on claims data, aggregating historical patterns into embeddings that feed RAG systems answering underwriter questions about similar cases from past years.",
        "Implement MCP protocol standards enabling insurance agents to communicate policy updates with customer service agents, synchronizing state changes when coverage modifications occur during renewal periods or mid-term adjustments.",
        "Deploy proof-of-concept agentic systems demonstrating autonomous handling of simple claims under threshold amounts, where AI agents verify documentation completeness, validate claimant eligibility, and approve payments without human review.",
        "Integrate LangChain memory modules storing conversation context in Cosmos DB, allowing customer service chatbots to reference previous interactions when policyholders call back days later with additional questions about their claims status.",
        "Develop multi-step reasoning pipelines where GPT models first classify claim types, then route to specialized agents trained on auto, property, or life insurance domains, ensuring subject matter expertise applies to each case evaluation.",
        "Establish vector databases using FAISS to index policy documents, enabling semantic search when agents need to retrieve specific clauses about coverage limits, exclusions, or deductible terms during automated claims processing workflows.",
        "Monitor prompt-response logs capturing agent interactions with Azure OpenAI endpoints, identifying cases where models provide inconsistent risk assessments or hallucinate policy details requiring prompt template refinements.",
        "Apply context persistence techniques ensuring agents maintain awareness of customer history across multiple touchpoints, pulling data from Azure Data Lake where previous claims, payment records, and communication logs reside.",
        "Troubleshoot API throttling issues when concurrent agent requests exceed Azure OpenAI quota limits during peak claim filing periods, implementing request queuing with retry logic to prevent service disruptions.",
        "Conduct debugging sessions examining LangGraph state transitions where agents fail to progress through workflow stages, adding logging statements to trace variable values and identify conditional branches causing unexpected behavior.",
        "Participate in code review meetings evaluating prompt injection vulnerabilities in customer-facing chatbots, ensuring input sanitization prevents malicious queries from manipulating agent behavior or accessing restricted policy information.",
        "Optimize SQL queries retrieving policy data from Delta Lake tables, reducing response times when agents need to verify coverage details during real-time conversations with customers asking about their insurance benefits.",
        "Validate HIPAA compliance for healthcare claims processing agents handling medical insurance cases, ensuring encrypted data transmission and access controls prevent unauthorized disclosure of protected health information.",
        "Test A/B experiments comparing different agent orchestration strategies, measuring claims processing speed and accuracy when using parallel versus sequential agent workflows for document verification and fraud detection tasks."
      ],
      "environment": [
        "Python",
        "Azure OpenAI",
        "GPT-4",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "MCP Protocol",
        "Azure Databricks",
        "PySpark",
        "Cosmos DB",
        "Azure Data Lake",
        "FAISS",
        "RAG Pipelines",
        "Delta Lake",
        "SQL",
        "FastAPI",
        "Docker",
        "Kubernetes",
        "MLflow",
        "Azure Cognitive Search"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Planned multi-agent framework architecture for pharmaceutical research document analysis, defining agent responsibilities where extraction agents parsed clinical trial reports, validation agents checked regulatory compliance, and summarization agents generated executive briefs.",
        "Constructed LangChain pipelines integrating Azure OpenAI GPT-4 models with HIPAA-compliant data storage, ensuring patient information remained encrypted during agent processing while enabling natural language queries about medical research findings.",
        "Configured Azure Databricks workflows running PySpark jobs that preprocessed millions of adverse event reports, creating text embeddings used by RAG systems to answer safety-related questions from pharmacovigilance teams.",
        "Executed proof-of-concept projects demonstrating LangGraph agent capabilities for automating literature reviews, where agents searched medical databases, extracted relevant findings, and synthesized evidence summaries for research scientists.",
        "Assembled vector databases using Milvus to store biomedical text embeddings, enabling semantic search across research papers when scientists asked questions about drug interactions or treatment efficacy in natural language.",
        "Programmed context-aware conversational agents maintaining discussion threads about patient cases, storing dialogue history in Cosmos DB to provide continuity when clinicians returned to case reviews after interruptions.",
        "Launched intent recognition models classifying incoming research queries into categories like safety data requests, efficacy comparisons, or regulatory submissions, routing each to specialized agent teams trained on domain-specific knowledge.",
        "Structured LangGraph workflows orchestrating document approval processes where compliance agents verified regulatory requirements, medical writers drafted submission content, and review agents flagged sections needing physician approval.",
        "Embedded Azure Cognitive Search capabilities into RAG pipelines, combining keyword matching with semantic similarity to retrieve relevant clinical guidelines when agents answered questions about treatment protocols.",
        "Configured LLM fine-tuning experiments using Azure ML pipelines, adjusting training data to improve model understanding of pharmaceutical terminology and reducing hallucinations about drug names or dosage recommendations.",
        "Debugged agent communication failures in multi-agent systems where message passing between drug safety agents and clinical trial agents broke down, adding error handling to gracefully manage timeouts and retry failed transactions.",
        "Examined prompt engineering strategies during team meetings, testing different system messages and few-shot examples to improve agent accuracy when extracting adverse event severity levels from unstructured physician notes.",
        "Inspected SQL query performance retrieving patient data from Azure SQL databases, optimizing join operations when agents needed to correlate medication histories with lab results during safety signal detection workflows.",
        "Enforced HIPAA security controls on all AI agent deployments, implementing audit logging to track which users accessed patient data through conversational interfaces and encrypting data both at rest and in transit."
      ],
      "environment": [
        "Python",
        "Azure OpenAI",
        "GPT-4",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "Azure Databricks",
        "PySpark",
        "Cosmos DB",
        "Milvus",
        "Azure Cognitive Search",
        "RAG Pipelines",
        "Azure ML",
        "Azure SQL",
        "HIPAA Compliance",
        "FastAPI",
        "Docker",
        "MLflow",
        "Prompt Engineering"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Designed machine learning models predicting Medicaid eligibility based on applicant demographics and income data, training Random Forest classifiers on historical approval records to automate preliminary screening decisions.",
        "Developed NLP pipelines processing citizen inquiries submitted through state healthcare portals, using spaCy for named entity recognition to extract patient names, dates, and medical conditions from unstructured text.",
        "Architected AWS-based data infrastructure consolidating health records from multiple state agencies into centralized S3 data lakes, enabling analysts to run queries across previously siloed datasets for population health studies.",
        "Automated ETL workflows using Apache Airflow to schedule daily imports of eligibility updates from legacy systems, transforming fixed-width text files into Parquet format for efficient querying with Athena.",
        "Created predictive models forecasting emergency room utilization patterns across rural Maine hospitals, analyzing seasonal trends and demographic factors to help administrators allocate resources during anticipated surge periods.",
        "Validated HIPAA compliance across all data pipelines handling protected health information, implementing encryption for data at rest in S3 buckets and restricting API access through IAM policies tied to specific user roles.",
        "Deployed TensorFlow models on AWS SageMaker endpoints providing real-time risk scoring for patients likely to miss preventive care appointments, enabling care coordinators to prioritize outreach calls.",
        "Evaluated model performance metrics comparing logistic regression baselines against gradient boosting approaches for predicting hospital readmission risk, selecting XGBoost after achieving better precision-recall tradeoffs.",
        "Prepared training datasets by joining claims data with social determinants of health indicators, handling missing values through imputation strategies and addressing class imbalance with SMOTE oversampling techniques.",
        "Collaborated with public health officials during weekly meetings to review model predictions, explaining how features like prior hospitalization history and chronic condition counts influenced risk scores for specific patient populations.",
        "Investigated data quality issues where incomplete address information prevented accurate geospatial analysis of healthcare access disparities, working with data stewards to improve collection processes at enrollment.",
        "Tested different feature engineering approaches for time series forecasting of prescription drug costs, comparing rolling averages, lag variables, and seasonal decomposition methods to improve monthly budget predictions."
      ],
      "environment": [
        "Python",
        "Scikit-Learn",
        "TensorFlow",
        "XGBoost",
        "spaCy",
        "NLTK",
        "AWS S3",
        "AWS SageMaker",
        "AWS Athena",
        "Apache Airflow",
        "Pandas",
        "NumPy",
        "SQL",
        "Parquet",
        "HIPAA Compliance",
        "Random Forest",
        "Logistic Regression"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Analyzed transaction data identifying fraudulent credit card patterns using isolation forest algorithms, flagging anomalous spending behaviors like sudden geographic location changes or unusual merchant category sequences.",
        "Produced customer segmentation models clustering retail banking clients based on account activity profiles, applying K-means to group customers by transaction frequency, average balances, and product holdings for targeted marketing.",
        "Engineered features from transactional time series data calculating rolling statistics like thirty-day spending velocity and day-of-week patterns, improving fraud detection model recall without significantly increasing false positive rates.",
        "Trained gradient boosting classifiers predicting customer churn risk, using historical account closure data to identify early warning signals like declining balance trends or reduced login frequency on mobile banking apps.",
        "Performed exploratory data analysis on loan application datasets, visualizing approval rate disparities across different demographic segments and investigating whether credit scoring models exhibited unintended bias.",
        "Standardized data extraction processes pulling information from Oracle databases containing mortgage, credit card, and checking account records, writing SQL queries joining multiple tables to create unified customer views.",
        "Delivered presentations to business stakeholders explaining model predictions in non-technical terms, demonstrating how specific customer behaviors like missed payments or branch visit frequency influenced churn probability scores.",
        "Maintained PCI-DSS compliance when handling cardholder data in machine learning experiments, ensuring sensitive fields remained encrypted and restricting dataset access to authorized personnel through role-based permissions.",
        "Reviewed model performance during monthly meetings, comparing precision and recall metrics across different fraud detection approaches and recommending threshold adjustments balancing customer experience with risk mitigation.",
        "Cleaned transaction datasets addressing data quality problems like duplicate records, inconsistent merchant names, and missing timestamp information that impacted model training accuracy."
      ],
      "environment": [
        "Python",
        "Scikit-Learn",
        "XGBoost",
        "Pandas",
        "NumPy",
        "SQL",
        "Oracle",
        "AWS S3",
        "Matplotlib",
        "Seaborn",
        "Isolation Forest",
        "K-Means Clustering",
        "Gradient Boosting",
        "PCI-DSS Compliance",
        "Tableau"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Loaded data from multiple source systems into Hadoop clusters using Sqoop, transferring relational database tables into HDFS for batch processing by MapReduce jobs analyzing customer behavior patterns.",
        "Transformed raw log files using Informatica PowerCenter workflows, parsing unstructured web server logs into structured formats and loading cleansed data into data warehouse tables for reporting.",
        "Scheduled ETL jobs running overnight to refresh data marts with updated transaction records, monitoring job execution through Informatica logs and troubleshooting failures caused by source system downtime.",
        "Extracted data quality metrics calculating null rates, duplicate counts, and referential integrity violations across incoming datasets, reporting findings to business analysts during weekly data governance meetings.",
        "Assisted senior engineers writing Hive queries aggregating sales data across product categories, learning how to optimize query performance by partitioning tables and choosing appropriate file formats.",
        "Participated in training sessions learning Hadoop ecosystem tools, practicing MapReduce programming concepts and understanding how HDFS distributed file storage worked across cluster nodes.",
        "Validated data migration accuracy by comparing row counts and sample records between source Oracle databases and target Hadoop tables, documenting discrepancies for investigation.",
        "Supported production data pipelines by responding to incidents when scheduled jobs failed, checking logs to identify root causes like disk space issues or network connectivity problems."
      ],
      "environment": [
        "Hadoop",
        "Sqoop",
        "Informatica PowerCenter",
        "Hive",
        "MapReduce",
        "HDFS",
        "Oracle",
        "SQL",
        "Shell Scripting"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}