{
  "name": "Yallaiah Onteru",
  "title": "Senior Agentic AI Engineer - AWS & Multi-Agent Systems",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bringing 10 years of hands-on experience across Insurance, Healthcare, Banking, and Consulting domains, building agent-based AI solutions using LangGraph, Bedrock, and AWS Step Functions for real-time decision workflows.",
    "Designed autonomous planning agents with ReAct patterns and self-correction loops using Claude via Amazon Bedrock, combining tool-calling with OpenSearch vector retrieval for context-aware responses in production environments.",
    "Built multi-agent orchestration systems using LangGraph stateful flows and event-driven coordination through EventBridge, enabling supervisor agents to manage role-based agent execution across ECS clusters for insurance claim processing.",
    "Implemented RAG pipelines integrating Neo4j knowledge graphs with DynamoDB short-term memory and Aurora pgvector, using Bedrock Embeddings and Cypher queries to power reasoning agents with structured domain knowledge retrieval.",
    "Developed A2A communication patterns using async execution models on Lambda and EKS, establishing webhook-based tool adapters with FastAPI for real-time agent collaboration and streaming responses via SSE.",
    "Created long-term memory agents leveraging Amazon Neptune graph databases for relationship tracking, combining MCP tool registration with secure IAM roles to maintain agent state across multi-turn conversations.",
    "Configured Bedrock Guardrails for agent safety with PII redaction policies, setting up VPC isolation and KMS encryption for HIPAA-compliant healthcare agents while maintaining audit trails through CloudWatch and X-Ray traceability.",
    "Established CI/CD pipelines using GitHub Actions and CodePipeline for agent deployment, implementing canary rollout strategies with prompt versioning and model endpoint updates across SageMaker for A/B testing agent behaviors.",
    "Trained custom agent models using SageMaker with PEFT and LoRA fine-tuning techniques, hosting endpoints for specialized insurance domain agents and integrating them into Step Functions workflows for claim validation tasks.",
    "Orchestrated agent planning flows combining Bedrock tool invocation with OpenSearch semantic search, using Comprehend and Textract for multimodal document pre-processing in agent input pipelines with OCR capabilities.",
    "Monitored agent task success rates and hallucination detection using custom CloudWatch metrics, tracking tool-usage correctness and latency across distributed agent systems while optimizing cost-aware agent execution patterns.",
    "Structured system prompts for task decomposition and reflection loops, applying guardrail prompts to control agent behavior and implementing prompt regression testing frameworks to benchmark agent performance over iterations.",
    "Integrated CrewAI and custom agent frameworks with Bedrock models including Claude, Llama, and Mistral, establishing supervisor patterns for coordinating specialized agents handling different stages of workflow execution.",
    "Deployed Docker-containerized agents on ECR and EKS with Kubernetes orchestration, connecting SQS and SNS messaging for asynchronous agent communication and event-driven workflows across microservices architectures.",
    "Applied chunking and retrieval strategies using Titan Embeddings for knowledge base construction, building agent simulators and sandboxes for testing self-healing agent capabilities before production deployment on AWS infrastructure.",
    "Managed cross-account IAM access for agent roles using least-privilege policies, ensuring Secrets Manager integration for secure API tool access while maintaining compliance with data residency requirements and GDPR regulations.",
    "Evaluated agent workflows through hallucination detection frameworks and bias checks, running LLM evals for prompt optimization and measuring agent behavior benchmarks across insurance risk assessment and fraud detection scenarios.",
    "Connected agent systems with REST and async APIs using Flask for tool adapters, enabling database tools, file system tools, and browser automation tools while streaming agent responses through Server-Sent Events for real-time updates."
  ],
  "technical_skills": {
    "LLM Orchestration & Agentic Frameworks": [
      "LangChain",
      "LangGraph",
      "CrewAI",
      "AutoGen",
      "ReAct Patterns",
      "Plan-Execute-Reflect",
      "Multi-Agent Systems",
      "Supervisor Agents",
      "MCP",
      "A2A",
      "Self-Correction Loops"
    ],
    "AWS AI & ML Services": [
      "Amazon Bedrock",
      "Claude",
      "Llama",
      "Mistral",
      "Bedrock Agents",
      "Bedrock Guardrails",
      "Amazon SageMaker",
      "SageMaker Pipelines",
      "Amazon Comprehend",
      "Amazon Textract",
      "Amazon Rekognition"
    ],
    "Vector Databases & RAG": [
      "Amazon OpenSearch",
      "Aurora PostgreSQL",
      "pgvector",
      "DynamoDB",
      "Bedrock Embeddings",
      "Titan Embeddings",
      "Neo4j",
      "Amazon Neptune",
      "Cypher Queries",
      "Knowledge Graphs"
    ],
    "Agent Infrastructure & Compute": [
      "AWS Lambda",
      "Amazon ECS",
      "Amazon EKS",
      "EC2",
      "Docker",
      "ECR",
      "Kubernetes",
      "AWS Step Functions",
      "EventBridge",
      "SQS",
      "SNS"
    ],
    "Programming & Development": [
      "Python",
      "FastAPI",
      "Flask",
      "REST APIs",
      "Async APIs",
      "Webhooks",
      "TypeScript",
      "SQL",
      "Bash",
      "Server-Sent Events"
    ],
    "Observability & Monitoring": [
      "Amazon CloudWatch",
      "AWS X-Ray",
      "Custom Metrics",
      "Agent Traceability",
      "Hallucination Detection",
      "Task Success Rate",
      "Latency Tracking",
      "Cost Optimization"
    ],
    "Security & Governance": [
      "IAM",
      "VPC Isolation",
      "AWS Secrets Manager",
      "KMS Encryption",
      "Bedrock Guardrails",
      "PII Redaction",
      "Audit Logs",
      "Data Residency",
      "HIPAA",
      "GDPR",
      "PCI-DSS"
    ],
    "CI/CD & MLOps": [
      "GitHub Actions",
      "AWS CodePipeline",
      "Canary Deployments",
      "Model Versioning",
      "Prompt Versioning",
      "Agent Rollout Strategies",
      "SageMaker Endpoints",
      "PEFT",
      "LoRA"
    ],
    "Big Data & Processing": [
      "Apache Spark",
      "PySpark",
      "Databricks",
      "Apache Kafka",
      "Apache Airflow",
      "Spark Streaming",
      "Hadoop",
      "Hive",
      "MapReduce"
    ],
    "Machine Learning & NLP": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "Transformers",
      "BERT",
      "GPT",
      "Hugging Face",
      "spaCy",
      "NLTK",
      "RAG Pipelines",
      "Fine-tuning LLMs"
    ],
    "Databases & Storage": [
      "PostgreSQL",
      "MySQL",
      "MongoDB",
      "Redis",
      "Snowflake",
      "Amazon S3",
      "Amazon RDS",
      "Elasticsearch",
      "Cassandra"
    ],
    "Agent Tools & Integration": [
      "API Tool Adapters",
      "Database Tools",
      "File System Tools",
      "Browser Tools",
      "Scraping Tools",
      "Tool Registration",
      "Streaming Responses"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas",
      "responsibilities": [
        "Architecting multi-agent insurance claim systems using LangGraph stateful flows with Bedrock Claude models, mapping agent roles for fraud detection, risk assessment, and policy validation workflows across AWS infrastructure.",
        "Building autonomous planning agents with ReAct patterns on Lambda, integrating OpenSearch vector retrieval with Neo4j knowledge graphs for insurance regulation queries and establishing A2A communication via EventBridge.",
        "Containerizing agents using Docker and ECR, orchestrating deployments on EKS clusters with Kubernetes while configuring Step Functions for agent workflow execution and setting up IAM roles for least-privilege access control.",
        "Tracking agent task success rates through CloudWatch custom metrics, implementing X-Ray traceability for debugging multi-agent interactions and measuring hallucination rates across claim processing agents in real-time dashboards.",
        "Tuning cost-aware agent execution by adjusting Bedrock model parameters, refining prompt templates for task decomposition and applying chunking strategies to OpenSearch embeddings for faster semantic retrieval in production.",
        "Debugging agent self-correction loops when reflection patterns failed during edge cases, collaborating with team to adjust guardrail prompts and fix tool invocation errors in insurance domain-specific API adapters.",
        "Prototyping supervisor agent patterns using LangGraph to coordinate role-based sub-agents, testing MCP tool registration with Databricks PySpark jobs for real-time claim data aggregation and proof of concept validation.",
        "Connecting Bedrock Agents with DynamoDB for short-term agent memory, using Aurora pgvector for long-term embeddings storage and establishing Cypher queries on Neo4j to reason over insurance policy relationships.",
        "Applying Bedrock Guardrails with PII redaction for customer data protection, configuring VPC isolation and KMS encryption while maintaining HIPAA compliance through audit logs and Secrets Manager integration for API keys.",
        "Automating agent deployment pipelines using GitHub Actions, implementing canary rollout strategies for prompt versioning and setting up SageMaker endpoint updates with model monitoring for A/B testing agent performance.",
        "Training custom insurance domain agents on SageMaker using LoRA techniques, hosting fine-tuned Claude models for specialized fraud detection and integrating endpoints into Step Functions for automated claim validation.",
        "Establishing async agent execution with SQS messaging for job queues, triggering EventBridge rules to coordinate multi-stage workflows and streaming agent responses via FastAPI with Server-Sent Events for UI updates.",
        "Constructing RAG pipelines with Titan Embeddings for policy document retrieval, chunking PDF regulations using Textract OCR and indexing into OpenSearch for context-aware agent responses during customer inquiries.",
        "Running LLM evals for prompt regression testing, benchmarking agent behavior across fraud scenarios and implementing bias checks while tracking tool-usage correctness through custom evaluation frameworks.",
        "Designing self-healing agents with fallback mechanisms, creating agent simulators for sandbox testing before production and refining reflection loops to handle unexpected insurance claim scenarios autonomously.",
        "Working closely with compliance teams to validate agent outputs against state insurance regulations, attending daily standup meetings to discuss multi-agent system performance and participating in code reviews for tool adapter improvements."
      ],
      "environment": [
        "LangGraph",
        "Amazon Bedrock",
        "Claude",
        "AWS Step Functions",
        "OpenSearch",
        "Neo4j",
        "DynamoDB",
        "Aurora pgvector",
        "EventBridge",
        "Lambda",
        "ECS",
        "EKS",
        "Docker",
        "Kubernetes",
        "Databricks",
        "PySpark",
        "SageMaker",
        "LoRA",
        "Bedrock Guardrails",
        "IAM",
        "CloudWatch",
        "X-Ray",
        "GitHub Actions",
        "FastAPI",
        "Textract",
        "Titan Embeddings",
        "MCP",
        "A2A"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey",
      "responsibilities": [
        "Mapped multi-agent healthcare workflows using LangChain for patient data processing, defining supervisor patterns to coordinate specialized agents handling HIPAA-compliant medical record analysis and drug interaction validation.",
        "Developed proof of concept multi-agent systems with Databricks PySpark for clinical trial data aggregation, integrating LangGraph stateful flows to manage agent conversations and maintain context across patient care pathways.",
        "Configured Bedrock Agents with Claude models for medical query understanding, establishing tool invocation patterns through Lambda functions to fetch patient histories from Aurora PostgreSQL while respecting HIPAA data access policies.",
        "Implemented RAG pipelines using OpenSearch vector engine with Bedrock Embeddings, chunking medical literature PDFs and indexing clinical guidelines to provide agents with accurate reference material during diagnostic assistance workflows.",
        "Built A2A communication channels using EventBridge for asynchronous task distribution, enabling pharmacy review agents to collaborate with prescribing agents while maintaining separate contexts in DynamoDB short-term memory.",
        "Applied CrewAI framework for coordinating role-based healthcare agents, testing AutoGen patterns for task assignment and establishing ReAct reasoning loops to validate medication recommendations against patient allergy databases.",
        "Deployed containerized agents on ECS Fargate with Docker, connecting to Amazon Comprehend Medical for entity extraction from clinical notes and feeding structured data into agent decision trees for treatment plan generation.",
        "Constructed knowledge graphs in Neo4j representing drug interactions and patient conditions, writing Cypher queries for agents to traverse relationships and identify contraindications during prescription validation processes.",
        "Monitored agent hallucination rates through CloudWatch custom dashboards, tracking false positive detection in symptom analysis and adjusting system prompts to reduce medical misinformation in agent-generated responses.",
        "Established IAM policies for cross-account agent access to protected health information, integrating Secrets Manager for secure API credentials and implementing VPC endpoints to isolate healthcare agent traffic from public networks.",
        "Trained domain-specific medical agents using SageMaker with PEFT fine-tuning on clinical datasets, hosting endpoints for specialized oncology and cardiology agents that integrate into Step Functions for care coordination workflows.",
        "Created async APIs with FastAPI for streaming agent responses to healthcare providers, implementing Server-Sent Events for real-time patient risk score updates and connecting webhook endpoints for electronic health record systems.",
        "Debugged agent task failures during peak clinic hours, analyzing X-Ray traces to identify bottlenecks in OpenSearch retrieval latency and optimizing embedding queries to improve agent response times under heavy concurrent load.",
        "Participated in weekly code reviews with healthcare IT teams, discussing agent behavior patterns and collaborating on guardrail prompt adjustments to ensure agents provided medically accurate information while respecting GDPR regulations."
      ],
      "environment": [
        "LangChain",
        "LangGraph",
        "CrewAI",
        "AutoGen",
        "Amazon Bedrock",
        "Claude",
        "AWS Step Functions",
        "OpenSearch",
        "Neo4j",
        "DynamoDB",
        "Aurora PostgreSQL",
        "EventBridge",
        "Lambda",
        "ECS",
        "Docker",
        "Databricks",
        "PySpark",
        "SageMaker",
        "PEFT",
        "Bedrock Embeddings",
        "IAM",
        "CloudWatch",
        "X-Ray",
        "FastAPI",
        "Comprehend Medical",
        "A2A",
        "HIPAA",
        "GDPR"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine",
      "responsibilities": [
        "Analyzed state healthcare data using Azure Machine Learning Studio, building predictive models for Medicaid enrollment forecasting and integrating with Azure Data Factory pipelines to automate monthly report generation for program administrators.",
        "Processed patient eligibility records from Azure SQL Database using PySpark on Databricks, applying data quality checks and transforming raw claims data into structured datasets for risk stratification models.",
        "Developed classification models with Scikit-Learn and XGBoost to identify high-risk patients requiring care management interventions, deploying models through Azure Container Instances with REST API endpoints for real-time scoring.",
        "Constructed ETL workflows in Azure Data Factory to extract PHI from on-premise systems, applying encryption and tokenization before loading into Azure Cosmos DB while maintaining HIPAA compliance throughout the data pipeline.",
        "Trained time series models using Prophet to forecast seasonal healthcare demand patterns, helping state planners allocate resources for flu season surges and optimizing hospital bed capacity predictions across rural facilities.",
        "Built interactive dashboards with Power BI connected to Azure Synapse Analytics, visualizing population health metrics and enabling healthcare administrators to drill down into county-level utilization patterns for program evaluation.",
        "Implemented Azure Key Vault for managing database credentials and API keys, establishing role-based access controls to ensure only authorized ML pipelines could access sensitive patient information during model training cycles.",
        "Monitored ML model performance using Azure Monitor, tracking prediction accuracy degradation over time and scheduling quarterly model retraining jobs to adapt to changing patient demographics and healthcare utilization trends.",
        "Collaborated with state epidemiologists during outbreak investigations, quickly adapting existing models to predict COVID case trajectories and providing daily forecasts to support public health decision-making during emergency response.",
        "Debugged Azure pipeline failures caused by schema changes in source systems, working with legacy mainframe teams to understand data format updates and modifying Spark transformation logic to handle new field structures.",
        "Participated in HIPAA compliance audits, documenting data flow diagrams and demonstrating encryption at rest and in transit for all PHI processing workflows while addressing security team concerns about ML model explainability.",
        "Presented ML findings to non-technical stakeholders in monthly steering committee meetings, translating model outputs into actionable insights and explaining confidence intervals to help program directors make informed policy recommendations."
      ],
      "environment": [
        "Azure Machine Learning",
        "Azure Data Factory",
        "Databricks",
        "PySpark",
        "Azure SQL Database",
        "Azure Cosmos DB",
        "Azure Synapse Analytics",
        "Scikit-Learn",
        "XGBoost",
        "Prophet",
        "Power BI",
        "Azure Container Instances",
        "Azure Key Vault",
        "Azure Monitor",
        "HIPAA"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York",
      "responsibilities": [
        "Analyzed credit card transaction patterns using Azure Databricks with PySpark, identifying anomalous spending behaviors and building fraud detection models that reduced false positives while maintaining PCI-DSS compliance standards.",
        "Processed streaming transaction data from Azure Event Hubs, applying real-time feature engineering in Spark Streaming to calculate rolling aggregates and velocity checks for immediate fraud scoring during authorization flows.",
        "Developed ensemble models combining Random Forest and Gradient Boosting with Scikit-Learn, training on historical fraud cases and legitimate transactions to classify suspicious activity with appropriate risk scores for investigation queues.",
        "Built customer segmentation models using K-Means clustering on Azure ML Studio, grouping cardholders by spending patterns to personalize product recommendations and optimize marketing campaign targeting for premium card upgrades.",
        "Constructed data pipelines in Azure Data Factory to extract transaction records from on-premise SQL Server databases, transforming and loading into Azure SQL Database while masking PII according to banking security policies.",
        "Trained churn prediction models using Logistic Regression to identify customers at risk of closing accounts, providing retention teams with prioritized contact lists and recommended intervention strategies based on feature importance analysis.",
        "Implemented automated model monitoring through Azure Monitor, tracking fraud model precision and recall metrics daily to detect performance degradation and triggering alerts when score distributions shifted beyond expected thresholds.",
        "Collaborated with compliance officers to document model risk management processes, preparing validation reports for internal audits and explaining model decisions to satisfy regulatory requirements for algorithmic transparency.",
        "Debugged scoring pipeline failures during high-volume transaction periods, analyzing Azure Application Insights logs to identify timeout issues and optimizing SQL queries to reduce latency for real-time fraud assessment.",
        "Participated in model governance reviews with risk management teams, presenting A/B test results comparing new fraud detection approaches against baseline models and quantifying business impact through estimated loss prevention."
      ],
      "environment": [
        "Azure Databricks",
        "PySpark",
        "Azure Event Hubs",
        "Spark Streaming",
        "Azure Machine Learning",
        "Scikit-Learn",
        "Random Forest",
        "Gradient Boosting",
        "K-Means",
        "Azure Data Factory",
        "Azure SQL Database",
        "SQL Server",
        "Logistic Regression",
        "Azure Monitor",
        "Azure Application Insights",
        "PCI-DSS"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra",
      "responsibilities": [
        "Extracted data from multiple Oracle and MySQL source systems using Sqoop, transferring large transaction tables into Hadoop HDFS for batch processing and historical analysis across consulting client projects.",
        "Processed structured data files with Hive queries on Hadoop clusters, performing aggregations and joins to prepare dimensional tables for business intelligence reporting and creating partitioned tables for query performance.",
        "Built ETL workflows using Informatica PowerCenter to cleanse and standardize customer records, applying business rules for data quality validation and loading transformed datasets into data warehouse staging tables.",
        "Scheduled batch jobs through cron scripts and Informatica Workflow Manager, monitoring execution logs to ensure overnight data loads completed successfully before morning report generation deadlines for business users.",
        "Learned MapReduce programming patterns for distributed data processing, writing basic Java jobs to calculate summary statistics across large datasets and understanding parallel processing concepts on Hadoop infrastructure.",
        "Collaborated with senior engineers during troubleshooting sessions when Sqoop imports failed, examining connection errors and adjusting memory settings to successfully transfer multi-gigabyte tables without timeouts.",
        "Attended team knowledge sharing sessions about Hadoop ecosystem tools, taking notes on HDFS architecture and practicing HiveQL syntax to improve query writing skills for data transformation tasks.",
        "Documented ETL mappings in Informatica for source-to-target data flows, creating process diagrams that helped team members understand transformation logic and supporting handoff to production support teams after project delivery."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "MapReduce",
        "Oracle",
        "MySQL",
        "Java",
        "HiveQL",
        "cron"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}