{
  "name": "Yallaiah Onteru",
  "title": "Senior MLOps Engineer - GCP AI/ML Platforms & LLM Systems",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Delivered enterprise-grade MLOps solutions across Insurance, Healthcare, Banking, and Consulting domains with deep focus on operationalizing LLM systems, RAG pipelines, and multimodal AI using GCP-native services including Vertex AI and BigQuery ML.",
    "Architected end-to-end ML pipeline automation on GCP utilizing Vertex AI for model training, tuning, deployment, and monitoring while maintaining model versioning and registry governance across cloud and hybrid environments.",
    "Established CI/CD pipelines for ML models using Cloud Build, Cloud Deploy, and Artifact Registry to enable automated testing, canary deployments, blue-green deployments, and rollback strategies for production-grade AI systems.",
    "Configured Kubernetes clusters on GKE and serverless deployments on Cloud Run to serve LLM inference endpoints with GPU and CPU optimization, autoscaling strategies, and latency optimization for real-time prediction workloads.",
    "Implemented Retrieval-Augmented Generation systems integrating BigQuery for structured data retrieval, vector indexing with FAISS and Pinecone, and embedding workflows to ground LLM responses and reduce hallucination risks.",
    "Automated feature engineering pipelines using Cloud Composer and Dataflow for batch and streaming ML workflows while enforcing schema governance, data quality checks, and lineage tracking for responsible AI compliance.",
    "Deployed document intelligence and OCR pipelines processing multimodal inputs through Vertex AI Vision APIs with preprocessing in Dataflow and storage optimization in BigQuery for downstream analytics and model training.",
    "Built model drift detection and inference monitoring systems using Vertex AI Model Monitoring integrated with Cloud Logging and custom alerting workflows to ensure production model performance and reliability.",
    "Managed container hardening and secrets management using Secret Manager, IAM policies, and VPC network isolation to secure model serving endpoints and protect PII data in healthcare and financial compliance contexts.",
    "Integrated prompt engineering and prompt versioning frameworks into LLM deployment workflows enabling controlled behavior, A/B testing of prompts, and tracking of LLM evaluation metrics for grounding and bias validation.",
    "Developed cost optimization strategies analyzing BigQuery query patterns, Vertex AI training job configurations, and GKE autoscaling policies to reduce infrastructure spend while maintaining SLA targets.",
    "Collaborated with data scientists and platform engineers to define approval workflows, governance metadata standards, and model auditability requirements ensuring reproducibility and compliance with regulatory frameworks.",
    "Applied LLM evaluation frameworks measuring response quality, safety, and bias across multi-agent systems and agentic AI workflows using custom metrics stored in BigQuery and visualized through Looker dashboards.",
    "Configured error handling and alerting mechanisms using Cloud Monitoring and PagerDuty integrations to capture ML pipeline failures, model endpoint errors, and data quality anomalies in real-time production environments.",
    "Participated in debugging sessions and code reviews focused on optimizing Docker container performance, refining Kubernetes resource allocations, and troubleshooting Python-based ML service dependencies.",
    "Attended cross-functional meetings coordinating with security teams on IAM role definitions, network policies, and audit logging to meet enterprise governance and compliance standards for AI/ML platforms.",
    "Shared reference architectures and documentation on workflow chaining for LLM systems, agentic reasoning patterns, and integration of vector databases into RAG pipelines to support scalable AI initiatives.",
    "Contributed to open-source MLOps tooling and participated in internal knowledge-sharing sessions demonstrating GCP-native solutions for model deployment, observability tooling, and automation-first mindset adoption."
  ],
  "technical_skills": {
    "Cloud Platforms & Services": [
      "Google Cloud Platform (GCP)",
      "Vertex AI (Training, Tuning, Deployment, Model Registry, Monitoring)",
      "BigQuery",
      "BigQuery ML",
      "Cloud Composer",
      "Dataflow",
      "GKE (Google Kubernetes Engine)",
      "Cloud Run",
      "Artifact Registry",
      "Cloud Build",
      "Cloud Deploy",
      "IAM",
      "VPC",
      "Secret Manager",
      "Cloud Logging",
      "Cloud Monitoring"
    ],
    "MLOps & Model Lifecycle": [
      "Model CI/CD",
      "Model Versioning",
      "Model Registry",
      "Automated Testing",
      "Canary Deployments",
      "Blue-Green Deployments",
      "Rollback Strategies",
      "Model Drift Detection",
      "Inference Monitoring",
      "MLflow",
      "DVC",
      "Kubeflow"
    ],
    "LLM & Generative AI": [
      "Large Language Models (LLMs)",
      "Retrieval-Augmented Generation (RAG)",
      "Prompt Engineering",
      "Prompt Versioning",
      "LLM Evaluation Frameworks",
      "Embedding Workflows",
      "Vector Indexing",
      "Agentic AI",
      "Multi-Agent Systems",
      "Model Context Protocol (MCP)",
      "LangChain",
      "LangGraph",
      "OpenAI APIs",
      "Hugging Face Transformers"
    ],
    "Programming & Scripting": [
      "Python",
      "SQL",
      "Bash/Shell",
      "Scala",
      "PySpark"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes",
      "GKE",
      "Cloud Run",
      "Container Hardening"
    ],
    "CI/CD & DevOps": [
      "Cloud Build",
      "Cloud Deploy",
      "Jenkins",
      "GitHub Actions",
      "GitLab CI/CD",
      "Terraform",
      "Git",
      "Artifact Registry"
    ],
    "Data Engineering & Pipelines": [
      "Apache Spark",
      "Databricks",
      "PySpark",
      "Apache Airflow",
      "Cloud Composer",
      "Dataflow",
      "BigQuery",
      "Batch Pipelines",
      "Streaming Pipelines",
      "Feature Engineering",
      "ETL/ELT"
    ],
    "Vector Databases & Search": [
      "FAISS",
      "Pinecone",
      "Weaviate",
      "Vector Indexing",
      "Embedding Storage"
    ],
    "Multimodal AI & Document Processing": [
      "Document Intelligence",
      "OCR Pipelines",
      "Multimodal Processing",
      "Vertex AI Vision APIs",
      "Text Extraction",
      "Image Processing"
    ],
    "Observability & Monitoring": [
      "Cloud Logging",
      "Cloud Monitoring",
      "Vertex AI Model Monitoring",
      "Metrics Collection",
      "Tracing",
      "Alerting",
      "Error Handling",
      "PagerDuty"
    ],
    "Security & Governance": [
      "IAM",
      "Secret Manager",
      "VPC Network Isolation",
      "PII Controls",
      "Secure Model Serving",
      "Container Hardening",
      "Compliance (HIPAA, PCI-DSS)",
      "Model Auditability",
      "Lineage Tracking",
      "Governance Metadata",
      "Approval Workflows",
      "Responsible AI"
    ],
    "Optimization & Performance": [
      "GPU Optimization",
      "CPU Optimization",
      "Latency Optimization",
      "Autoscaling Strategies",
      "Cost Optimization",
      "Query Optimization",
      "Resource Allocation"
    ],
    "Machine Learning Frameworks": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "XGBoost",
      "Keras"
    ],
    "Data Quality & Validation": [
      "Schema Governance",
      "Data Quality Checks",
      "Automated Validation",
      "Anomaly Detection",
      "Drift Detection"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Architect multi-agent systems using LangGraph and Model Context Protocol on GCP Vertex AI to automate insurance claims processing, integrating agent-to-agent communication patterns with BigQuery for policy data retrieval and RAG pipelines for regulatory document grounding.",
        "Build proof-of-concept LLM applications on Databricks leveraging PySpark for distributed feature extraction and Vertex AI for model fine-tuning, demonstrating feasibility of agentic workflows in insurance underwriting and fraud detection use cases.",
        "Deploy containerized LLM inference services on GKE with canary deployment strategies using Cloud Deploy, implementing automated rollback mechanisms based on latency metrics and error rates captured through Cloud Monitoring and custom Python validation scripts.",
        "Configure Vertex AI Model Registry for versioning fine-tuned LLMs and embedding models, establishing CI/CD pipelines in Cloud Build that trigger automated testing, bias evaluation, and prompt validation before production releases.",
        "Implement RAG pipelines combining BigQuery ML for semantic search, FAISS vector indexing for document embeddings, and Vertex AI LLMs for response generation, reducing hallucination incidents in customer-facing insurance chatbot applications.",
        "Optimize GPU allocation and autoscaling policies on GKE clusters serving real-time LLM inference, analyzing Cloud Monitoring metrics to balance cost and latency while maintaining SLA compliance for peak traffic periods.",
        "Automate feature engineering workflows using Cloud Composer DAGs that orchestrate PySpark jobs on Dataflow, applying schema governance rules and data quality checks to ensure clean inputs for model training and retraining cycles.",
        "Establish model drift detection pipelines on Vertex AI Model Monitoring integrated with BigQuery for historical comparison, triggering alerts via Cloud Logging when prediction distributions deviate beyond acceptable thresholds.",
        "Secure LLM endpoints using IAM role-based access controls, Secret Manager for API key rotation, and VPC network isolation to comply with insurance industry PII protection requirements and state regulatory mandates.",
        "Develop prompt engineering frameworks with version control in Artifact Registry, enabling A/B testing of prompt templates and tracking performance metrics in BigQuery to iterate on LLM response quality and grounding accuracy.",
        "Integrate document intelligence pipelines processing policy PDFs through Vertex AI Vision OCR, storing extracted text in BigQuery and feeding structured data into downstream RAG systems for claims adjudication support.",
        "Collaborate with data scientists to define approval workflows for model deployments, creating governance metadata in BigQuery and implementing lineage tracking to ensure auditability and reproducibility for regulatory audits.",
        "Participate in code reviews focused on optimizing Docker container sizes for faster Cloud Run cold starts, refactoring Python LLM service dependencies to reduce memory footprint and improve horizontal scaling efficiency.",
        "Attend sprint planning meetings coordinating with platform engineers on Kubernetes resource quotas, IAM policy updates, and network configurations to support multi-agent system deployments across hybrid cloud environments.",
        "Troubleshoot production incidents involving LLM endpoint timeouts by analyzing Cloud Logging traces, adjusting Cloud Run concurrency settings, and implementing retry logic in Python client libraries to improve reliability.",
        "Document reference architectures for agentic AI workflows and multi-agent orchestration patterns, sharing best practices with cross-functional teams on integrating LangChain with GCP-native services for enterprise-grade AI systems."
      ],
      "environment": [
        "GCP",
        "Vertex AI",
        "BigQuery",
        "BigQuery ML",
        "GKE",
        "Cloud Run",
        "Cloud Build",
        "Cloud Deploy",
        "Artifact Registry",
        "Cloud Composer",
        "Dataflow",
        "Cloud Monitoring",
        "Cloud Logging",
        "Secret Manager",
        "IAM",
        "VPC",
        "Databricks",
        "PySpark",
        "LangGraph",
        "LangChain",
        "Model Context Protocol",
        "Multi-Agent Systems",
        "FAISS",
        "Pinecone",
        "Docker",
        "Kubernetes",
        "Python",
        "RAG Pipelines",
        "Prompt Engineering",
        "LLM Evaluation Frameworks",
        "OCR",
        "Document Intelligence"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Designed multi-agent LLM systems using LangChain and Databricks on GCP to automate clinical trial data extraction from medical literature, integrating RAG pipelines with BigQuery for structured patient cohort queries and HIPAA-compliant data handling.",
        "Constructed proof-of-concept applications demonstrating LLM-powered drug interaction analysis, leveraging Vertex AI for model fine-tuning on proprietary healthcare datasets and PySpark on Dataflow for large-scale feature preprocessing.",
        "Deployed blue-green deployment workflows for LLM inference endpoints on Cloud Run, utilizing Cloud Deploy automation and implementing health checks that validated response quality and latency before traffic cutover.",
        "Maintained model versioning in Vertex AI Model Registry with automated testing pipelines in Cloud Build, enforcing bias evaluation and safety validation through custom LLM evaluation frameworks before production releases.",
        "Assembled RAG architectures combining Vertex AI embeddings, Weaviate vector database for medical literature indexing, and BigQuery ML for semantic retrieval to ground LLM responses in FDA-approved drug information.",
        "Tuned autoscaling configurations on GKE for GPU-accelerated LLM inference, monitoring Cloud Monitoring metrics to optimize resource allocation while reducing compute costs without compromising patient-facing application performance.",
        "Orchestrated batch ML pipelines using Cloud Composer and PySpark on Dataflow to process electronic health records, applying data quality checks and schema governance to ensure HIPAA compliance and data integrity.",
        "Monitored model drift on Vertex AI Model Monitoring for clinical prediction models, storing performance metrics in BigQuery and triggering retraining workflows when accuracy degraded beyond regulatory thresholds.",
        "Secured healthcare AI systems through IAM policies restricting model access, Secret Manager for credential storage, and VPC service controls to isolate PHI data in compliance with HIPAA and GDPR regulations.",
        "Refined prompt versioning workflows storing prompt templates in Artifact Registry, conducting A/B tests with healthcare professionals to iterate on LLM prompt engineering for clinical decision support tools.",
        "Processed multimodal medical imaging data through Vertex AI Vision OCR and document intelligence pipelines, extracting structured information from radiology reports and storing results in BigQuery for downstream analytics.",
        "Coordinated with security teams to establish approval workflows for model deployments, documenting governance metadata and lineage tracking in BigQuery to support FDA audit requirements and model reproducibility.",
        "Reviewed Python codebases optimizing Docker container configurations for faster Cloud Run deployments, reducing cold start latency and improving horizontal scaling behavior for patient-facing LLM applications.",
        "Joined cross-functional meetings aligning on Kubernetes resource limits, network policies, and IAM role definitions to support multi-agent system rollouts across on-premise and GCP hybrid environments."
      ],
      "environment": [
        "GCP",
        "Vertex AI",
        "BigQuery",
        "BigQuery ML",
        "Cloud Run",
        "GKE",
        "Cloud Build",
        "Cloud Deploy",
        "Artifact Registry",
        "Cloud Composer",
        "Dataflow",
        "Cloud Monitoring",
        "Cloud Logging",
        "Secret Manager",
        "IAM",
        "VPC",
        "Databricks",
        "PySpark",
        "LangChain",
        "LangGraph",
        "Multi-Agent Systems",
        "Weaviate",
        "FAISS",
        "Docker",
        "Kubernetes",
        "Python",
        "RAG Pipelines",
        "Prompt Engineering",
        "LLM Evaluation Frameworks",
        "HIPAA Compliance",
        "OCR",
        "Document Intelligence"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Implemented AWS SageMaker pipelines for Medicaid eligibility prediction models, integrating S3 for data storage, Lambda for event-driven preprocessing, and RDS PostgreSQL for feature store management while ensuring HIPAA compliance.",
        "Developed batch ML workflows using Apache Airflow on EC2 orchestrating PySpark jobs on EMR to process healthcare claims data, applying data quality validations and schema checks before loading into Redshift for analytics.",
        "Configured model deployment automation on SageMaker with endpoint autoscaling and CloudWatch monitoring, establishing canary deployment patterns and automated rollback triggers based on prediction latency and error thresholds.",
        "Established CI/CD pipelines using AWS CodePipeline and CodeBuild for ML models, integrating unit tests for feature engineering code and model performance validation before promoting to production endpoints.",
        "Built feature engineering pipelines in PySpark on EMR reading from S3, transforming electronic health records into structured features, and writing to Redshift while maintaining lineage tracking for regulatory audit trails.",
        "Monitored production model performance through CloudWatch dashboards capturing inference latency, error rates, and data drift metrics, configuring SNS alerts to notify on-call engineers of anomalies requiring investigation.",
        "Secured AWS infrastructure using IAM policies restricting SageMaker endpoint access, Secrets Manager for database credentials, and VPC endpoints to isolate PHI data flows in compliance with state healthcare regulations.",
        "Optimized EMR cluster configurations analyzing Spark job execution plans, adjusting instance types and partition sizes to reduce processing time and compute costs for monthly Medicaid enrollment forecasting jobs.",
        "Participated in debugging sessions tracing Lambda function errors, examining CloudWatch Logs to identify timeout issues, and refactoring Python code to improve exception handling and retry logic.",
        "Collaborated with healthcare policy teams translating regulatory requirements into data quality rules, implementing validation logic in PySpark to flag incomplete or inconsistent records before model training.",
        "Attended standup meetings discussing pipeline failures, coordinating with platform engineers to resolve S3 access permission issues and SageMaker endpoint quota limits impacting production workloads.",
        "Documented ML system architecture diagrams showing data flow from S3 ingestion through EMR processing to SageMaker deployment, supporting knowledge transfer to incoming team members and compliance officers."
      ],
      "environment": [
        "AWS",
        "SageMaker",
        "S3",
        "Lambda",
        "EC2",
        "EMR",
        "RDS PostgreSQL",
        "Redshift",
        "Apache Airflow",
        "PySpark",
        "CodePipeline",
        "CodeBuild",
        "CloudWatch",
        "SNS",
        "IAM",
        "Secrets Manager",
        "VPC",
        "Docker",
        "Python",
        "HIPAA Compliance",
        "Data Quality Checks",
        "Model Monitoring"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Trained fraud detection models using XGBoost on AWS SageMaker, ingesting transaction data from S3, engineering temporal features in PySpark on EMR, and deploying real-time inference endpoints with autoscaling for peak traffic.",
        "Created ETL pipelines in Apache Airflow on EC2 orchestrating Glue jobs to extract transactional data from RDS, transform using PySpark, and load into Redshift for model training and BI dashboards.",
        "Deployed model versioning workflows in SageMaker Model Registry, integrating with CodePipeline to automate testing of model accuracy and PCI-DSS compliance checks before production endpoint updates.",
        "Configured CloudWatch alarms monitoring SageMaker endpoint latency and error rates, setting up automated SNS notifications and Lambda-triggered rollback procedures to maintain transaction processing SLAs.",
        "Engineered streaming data pipelines using Kinesis Data Streams ingesting card transaction events, processing in Lambda with anomaly detection logic, and storing results in DynamoDB for real-time fraud alerts.",
        "Analyzed model drift by comparing production prediction distributions stored in S3 against training baselines in Redshift, scheduling monthly retraining jobs on SageMaker to maintain detection accuracy.",
        "Enforced PCI-DSS security controls through IAM policies limiting SageMaker access, encrypting S3 buckets with KMS, and configuring VPC endpoints to prevent cardholder data exposure outside secure networks.",
        "Reduced infrastructure costs by analyzing CloudWatch billing metrics, rightsizing EMR cluster instance types, and implementing S3 lifecycle policies to archive infrequently accessed transaction logs.",
        "Debugged Python scripts processing transaction data, fixing data type mismatches causing Glue job failures and optimizing Spark SQL queries to improve pipeline execution time during month-end processing.",
        "Met with business stakeholders translating fraud prevention requirements into feature engineering specifications, balancing false positive rates with detection coverage to align model behavior with risk tolerance."
      ],
      "environment": [
        "AWS",
        "SageMaker",
        "S3",
        "EMR",
        "RDS",
        "Redshift",
        "Glue",
        "Apache Airflow",
        "PySpark",
        "CodePipeline",
        "CloudWatch",
        "SNS",
        "Lambda",
        "Kinesis Data Streams",
        "DynamoDB",
        "IAM",
        "KMS",
        "VPC",
        "XGBoost",
        "Python",
        "PCI-DSS Compliance",
        "Model Versioning"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Extracted data from Oracle databases using Sqoop, loading into HDFS on Hadoop clusters for downstream processing by MapReduce jobs transforming client datasets for business intelligence reporting.",
        "Loaded data into Hive tables partitioned by date, optimizing query performance for analytics teams running SQL queries against large datasets stored across Hadoop distributed file systems.",
        "Scheduled ETL workflows using cron jobs executing Sqoop import scripts, applying basic data quality checks in Bash to validate row counts and detect null values before downstream processing.",
        "Transferred files between on-premise servers and Hadoop clusters using SCP and HDFS CLI commands, documenting transfer procedures and troubleshooting network connectivity issues during overnight batch jobs.",
        "Learned Informatica PowerCenter by shadowing senior engineers, configuring source and target connections, and building simple mappings to replicate data from RDBMS sources into HDFS.",
        "Assisted in debugging MapReduce job failures by examining YARN logs, identifying out-of-memory errors, and working with team leads to adjust memory configurations and mapper/reducer counts.",
        "Participated in team meetings discussing daily ETL job statuses, reporting on Sqoop import completion times, and flagging data discrepancies requiring validation from source system owners.",
        "Wrote basic Bash scripts automating file movement and directory cleanup tasks on Hadoop edge nodes, improving efficiency of manual processes performed by operations teams."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "Oracle",
        "Bash",
        "YARN",
        "Linux"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}