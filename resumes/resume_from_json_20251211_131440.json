{
  "name": "Yallaiah Onteru",
  "title": "Senior AI/ML Developer - GenAI & RAG Platform Engineering",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Bring a decade of hands-on experience across Insurance, Healthcare, Banking, and Consulting sectors, building AI platforms that align with AWS Bedrock, SageMaker, Snowflake, and RAG workflows to modernize secure enterprise banking systems.",
    "Architected production-grade GenAI solutions using Python, PySpark, LangChain, and LangGraph on AWS infrastructure, integrating LLMs with Vertex AI and Hugging Face models to automate document processing and risk assessment workflows in regulated environments.",
    "Designed agentic frameworks applying Model Context Protocol and Agent-to-Agent patterns to orchestrate multi-step reasoning tasks, combining vector databases like Pinecone and Weaviate with embedding models for retrieval-augmented generation in financial compliance use cases.",
    "Implemented RAG architectures leveraging AWS Knowledge Bases, OpenSearch, and ChromaDB to enable semantic search across millions of policy documents, reducing manual review time while maintaining audit trails for regulatory oversight and model governance.",
    "Developed prompt engineering strategies including zero-shot, few-shot, Chain-of-Thought, and ReAct techniques to optimize LLM outputs for structured banking data extraction, ensuring consistency across customer onboarding and fraud detection pipelines.",
    "Built distributed data pipelines using Airflow and Databricks on Snowflake to orchestrate feature engineering workflows, feeding real-time embeddings into multimodal models that process text, audio, and image inputs for claims adjudication and patient records analysis.",
    "Tuned open-source LLMs with LoRA, QLoRA, and supervised fine-tuning methods on domain-specific insurance and healthcare datasets, balancing model accuracy with inference latency requirements for high-availability production APIs serving thousands of concurrent users.",
    "Established LLM evaluation frameworks using RAGAS, TruLens, and custom metrics to validate retrieval precision and generation quality, creating feedback loops that improved answer relevance scores and reduced hallucination rates in customer-facing chatbots.",
    "Deployed containerized AI services with Docker and Kubernetes on AWS EKS, configuring auto-scaling policies and load balancers to handle peak traffic during enrollment periods while maintaining sub-second response times for real-time quote generation systems.",
    "Configured MLOps workflows with MLflow and SageMaker Pipelines to version control model artifacts, track experiment runs, and automate retraining cycles, ensuring reproducibility and compliance with financial audit requirements across the model lifecycle.",
    "Integrated AWS Lambda functions with S3 event triggers to preprocess incoming documents, extract entities using NER models, and route structured outputs to downstream banking systems through RESTful APIs with OAuth authentication and encrypted data transmission.",
    "Optimized vector similarity search performance by experimenting with different embedding dimensions and indexing strategies in Qdrant and Milvus, reducing query latency from seconds to milliseconds for real-time customer support applications handling complex policy questions.",
    "Collaborated with data engineering teams to establish data governance policies around PII handling, implementing field-level encryption in Snowflake and role-based access controls that aligned with PCI-DSS and HIPAA compliance standards for secure AI platform operations.",
    "Constructed Graph RAG systems that mapped relationships between entities in healthcare claims data, enabling multi-hop reasoning queries that connected patient histories with provider networks and medication interactions for clinical decision support tools.",
    "Maintained observability across AI pipelines using OpenTelemetry, Prometheus, and Grafana dashboards to monitor token usage, model latency, and error rates, setting up alerts that triggered automatic rollbacks when performance degraded below acceptable thresholds.",
    "Refactored legacy monolithic banking applications into microservice architectures with event-driven communication patterns using Kafka, allowing independent scaling of AI inference services while preserving backward compatibility with existing core banking systems.",
    "Conducted code reviews and debugging sessions to troubleshoot edge cases in agentic workflows, identifying issues where agent state management caused infinite loops and implementing circuit breaker patterns to prevent cascading failures in production environments.",
    "Participated in cross-functional planning meetings to translate business requirements into technical specifications for AI automation initiatives, documenting API contracts and data schemas that enabled parallel development across distributed teams working on integrated solutions."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "SQL",
      "Scala",
      "R",
      "Java",
      "Bash/Shell",
      "TypeScript"
    ],
    "GenAI & LLM Frameworks": [
      "LangChain",
      "LangGraph",
      "Hugging Face Transformers",
      "OpenAI APIs",
      "Claude AI",
      "Llama Index",
      "Crew AI",
      "AutoGen",
      "Model Context Protocol (MCP)",
      "Agent-to-Agent (A2A)",
      "Vertex AI",
      "AWS Bedrock",
      "Prompt Engineering",
      "Fine-tuning (LoRA, QLoRA, RLHF)"
    ],
    "RAG & Vector Databases": [
      "Pinecone",
      "Weaviate",
      "ChromaDB",
      "Milvus",
      "Qdrant",
      "AWS Knowledge Bases",
      "OpenSearch",
      "Elasticsearch",
      "Embedding Models (bge-large, E5)",
      "Graph RAG",
      "Semantic Search"
    ],
    "Machine Learning & Deep Learning": [
      "TensorFlow",
      "PyTorch",
      "Scikit-Learn",
      "Keras",
      "XGBoost",
      "LightGBM",
      "CNNs",
      "RNNs",
      "LSTMs",
      "Transformers",
      "BERT",
      "GPT",
      "Transfer Learning",
      "Multimodal Models"
    ],
    "LLM Evaluation & Validation": [
      "RAGAS",
      "TruLens",
      "DeepEval",
      "Custom Evaluation Metrics",
      "Hallucination Detection",
      "Answer Relevance Scoring",
      "Retrieval Precision Analysis"
    ],
    "Big Data & Data Engineering": [
      "PySpark",
      "Apache Spark",
      "Databricks",
      "Apache Airflow",
      "Snowflake",
      "Apache Hadoop",
      "Apache Kafka",
      "Spark Streaming",
      "Hive",
      "MapReduce",
      "dbt",
      "Apache Flink"
    ],
    "Cloud Platforms & Services": [
      "AWS (SageMaker, Bedrock, S3, Lambda, EC2, EKS, RDS, Redshift, Glue, Kinesis)",
      "Azure (ML Studio, Data Factory, Databricks, Cosmos DB)",
      "GCP (BigQuery, Vertex AI, Cloud SQL)"
    ],
    "MLOps & Experiment Tracking": [
      "MLflow",
      "Kubeflow",
      "SageMaker Pipelines",
      "DVC",
      "Model Versioning",
      "Experiment Tracking",
      "Automated Retraining",
      "Model Governance"
    ],
    "API Design & Development": [
      "REST APIs",
      "FastAPI",
      "Flask",
      "Django",
      "GraphQL",
      "OAuth Authentication",
      "API Gateway",
      "Microservices Architecture"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes",
      "AWS EKS",
      "Helm",
      "Service Mesh",
      "Auto-scaling",
      "Load Balancing"
    ],
    "Databases & Data Storage": [
      "Snowflake",
      "PostgreSQL",
      "MySQL",
      "MongoDB",
      "Redis",
      "Cassandra",
      "AWS RDS",
      "BigQuery",
      "Teradata",
      "Oracle"
    ],
    "Observability & Monitoring": [
      "OpenTelemetry",
      "Prometheus",
      "Grafana",
      "CloudWatch",
      "Log Analysis",
      "Performance Metrics",
      "Alert Management"
    ],
    "Data Governance & Security": [
      "PCI-DSS Compliance",
      "HIPAA Compliance",
      "GDPR",
      "Field-level Encryption",
      "Role-based Access Control",
      "Data Lineage",
      "Audit Trails"
    ],
    "DevOps & CI/CD": [
      "Git",
      "GitHub",
      "GitLab",
      "Jenkins",
      "GitHub Actions",
      "Terraform",
      "Infrastructure as Code",
      "Automated Deployment"
    ],
    "Statistical & NLP Tools": [
      "spaCy",
      "NLTK",
      "Stanford NLP",
      "TF-IDF",
      "Named Entity Recognition",
      "Sentiment Analysis",
      "A/B Testing",
      "Hypothesis Testing"
    ],
    "Data Visualization": [
      "Tableau",
      "Power BI",
      "Matplotlib",
      "Seaborn",
      "Plotly",
      "Bokeh",
      "Streamlit"
    ],
    "Development Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "Google Colab",
      "Anaconda",
      "Postman"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "",
      "responsibilities": [
        "Plan multi-agent insurance claim processing workflows using LangGraph and Model Context Protocol to coordinate policy validation, fraud detection, and payout calculation agents that communicate through shared state management on AWS Bedrock infrastructure.",
        "Implement agentic frameworks combining Agent-to-Agent patterns with PySpark data transformations in Databricks, processing streaming claims data from Kafka topics and routing to specialized LLM agents that extract structured information from unstructured policy documents.",
        "Deploy proof-of-concept RAG systems integrating Pinecone vector database with AWS SageMaker endpoints, embedding insurance regulations using bge-large model to enable semantic search across policy manuals and state-specific compliance requirements for underwriting automation.",
        "Monitor production GenAI APIs serving customer service chatbots by configuring Prometheus metrics and Grafana dashboards that track token consumption, response latency, and hallucination rates, setting thresholds that trigger alerts when answer quality drops below targets.",
        "Optimize few-shot prompt templates for claims assessment tasks through systematic experimentation with Chain-of-Thought reasoning patterns, testing variations on validation datasets to improve extraction accuracy for vehicle damage descriptions and medical injury reports.",
        "Troubleshoot edge cases in multi-agent systems where state synchronization failures caused duplicate claim submissions, adding idempotency keys and distributed locking mechanisms using Redis to prevent race conditions in concurrent processing scenarios.",
        "Configure AWS Knowledge Bases with OpenSearch indexes containing millions of historical claim records, tuning retrieval parameters and reranking strategies to surface the most relevant precedents for adjusters reviewing complex liability cases.",
        "Integrate Vertex AI AutoML models with custom fine-tuned LLMs using LoRA adapters, creating hybrid pipelines where structured prediction models classify claim types before routing to specialized language models trained on insurance domain vocabulary.",
        "Establish LLM evaluation workflows with RAGAS framework to measure context precision and answer faithfulness scores, running automated tests against curated question sets that represent common customer inquiries about coverage limits and exclusions.",
        "Coordinate with compliance teams during weekly planning sessions to document model decisions for regulatory audits, implementing logging infrastructure that captures retrieval sources and generation steps for every AI-generated recommendation.",
        "Refactor legacy mainframe integration code into REST APIs using FastAPI and Docker containers on AWS EKS, enabling real-time policy lookup requests from mobile applications while maintaining backward compatibility with existing COBOL batch processing systems.",
        "Build Graph RAG architectures mapping relationships between policyholders, properties, and risk factors in Weaviate, supporting multi-hop queries that identify correlated exposures across geographically distributed assets for portfolio risk analysis.",
        "Test Snowflake integration patterns for feature engineering pipelines that aggregate telematics data, customer demographics, and claim histories into time-series embeddings fed to multimodal models predicting renewal likelihood and cross-sell opportunities.",
        "Participate in code reviews identifying memory leaks in long-running agent loops where context windows exceeded model limits, implementing sliding window strategies and summarization techniques to maintain conversation coherence without degrading performance.",
        "Experiment with QLoRA fine-tuning techniques on Llama models using domain-specific insurance datasets, balancing training costs with inference requirements to deploy quantized models that maintain accuracy while reducing GPU memory footprint for cost optimization.",
        "Automate MLOps workflows connecting MLflow experiment tracking with SageMaker Pipelines, versioning model artifacts and prompt templates in Git repositories to enable reproducible deployments and rollback capabilities when production issues arise."
      ],
      "environment": [
        "Python",
        "PySpark",
        "LangChain",
        "LangGraph",
        "AWS Bedrock",
        "AWS SageMaker",
        "Pinecone",
        "Weaviate",
        "Vertex AI",
        "Hugging Face",
        "OpenSearch",
        "Snowflake",
        "Databricks",
        "Apache Kafka",
        "Docker",
        "Kubernetes",
        "AWS EKS",
        "FastAPI",
        "Redis",
        "MLflow",
        "Prometheus",
        "Grafana",
        "Model Context Protocol",
        "Agent-to-Agent",
        "RAG",
        "LoRA",
        "QLoRA",
        "RAGAS"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "",
      "responsibilities": [
        "Planned HIPAA-compliant RAG system architecture for clinical trial document analysis, mapping data flows from encrypted S3 buckets through embedding generation pipelines to ChromaDB vector storage with field-level access controls validated against healthcare privacy regulations.",
        "Implemented LangChain-based medical literature search agents that queried PubMed APIs and internal research databases, combining retrieved abstracts with proprietary drug interaction knowledge graphs to generate evidence-based recommendations for research teams.",
        "Deployed fine-tuned BioBERT models on AWS SageMaker endpoints for adverse event extraction from patient reports, processing unstructured clinical notes to identify safety signals and automatically populate pharmacovigilance databases with structured event details.",
        "Monitored multimodal model performance analyzing radiology images and corresponding diagnostic reports, tracking precision metrics across different scan types and implementing retraining triggers when accuracy dropped for specific anatomical regions or pathology categories.",
        "Optimized ReAct prompt patterns for drug discovery workflows where LLM agents reasoned through compound property predictions, iteratively querying molecular databases and adjusting search parameters based on intermediate results to identify promising candidate molecules.",
        "Troubleshot distributed training failures when fine-tuning medical domain LLMs on multi-node GPU clusters, diagnosing gradient synchronization issues and adjusting batch sizes and learning rate schedules to stabilize convergence across heterogeneous hardware configurations.",
        "Configured Airflow DAGs orchestrating nightly ETL jobs that ingested clinical trial data from electronic health records, transformed patient timelines into standardized formats, and generated embeddings using E5 models for similarity search in patient cohort identification tools.",
        "Integrated AutoGen framework for multi-agent research assistant prototypes where specialized agents handled literature review, statistical analysis, and manuscript drafting tasks, coordinating through shared conversation history maintained in DynamoDB with versioned checkpointing.",
        "Established evaluation pipelines using TruLens to assess medical QA system outputs against clinical guidelines, measuring hallucination rates and citation accuracy to ensure generated treatment recommendations aligned with evidence-based medicine standards.",
        "Coordinated with data governance teams to implement de-identification pipelines removing protected health information before training datasets entered machine learning workflows, validating anonymization coverage through regex pattern matching and named entity recognition models.",
        "Refactored monolithic drug safety reporting applications into event-driven microservices communicating via AWS Kinesis streams, allowing independent scaling of text extraction, entity linking, and database persistence components based on fluctuating submission volumes.",
        "Built knowledge graph representations connecting drugs, diseases, genes, and proteins in Neo4j, enabling Graph RAG queries that traversed relationship paths to answer complex questions about mechanism of action and potential off-label uses for existing medications.",
        "Tested prompt injection vulnerabilities in patient-facing chatbot applications by attempting to override system instructions, implementing input validation filters and output guardrails that prevented disclosure of sensitive medical information or generation of harmful advice.",
        "Participated in weekly sprint planning sessions translating clinician feedback into technical requirements for AI-assisted diagnostic tools, creating user stories that balanced feature requests with HIPAA compliance constraints and model explainability needs for regulatory submissions."
      ],
      "environment": [
        "Python",
        "LangChain",
        "AWS SageMaker",
        "AWS S3",
        "ChromaDB",
        "BioBERT",
        "Hugging Face Transformers",
        "Apache Airflow",
        "AutoGen",
        "TruLens",
        "AWS Kinesis",
        "Neo4j",
        "DynamoDB",
        "E5 Embeddings",
        "Docker",
        "RAG",
        "HIPAA Compliance",
        "Medical NLP",
        "Named Entity Recognition"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "",
      "responsibilities": [
        "Planned Azure ML Studio pipelines for Medicaid fraud detection analyzing provider billing patterns, coordinating feature extraction from claims databases with anomaly detection models that flagged suspicious activity while maintaining patient privacy through aggregated statistics.",
        "Implemented XGBoost classifiers on Azure Databricks processing millions of healthcare transactions daily, tuning hyperparameters through grid search experiments tracked in MLflow to maximize precision in identifying fraudulent claims without overwhelming investigators with false positives.",
        "Deployed predictive models forecasting emergency department utilization across public health facilities, ingesting data from disparate hospital systems through Azure Data Factory ETL workflows and exposing forecasts via REST APIs consumed by resource allocation dashboards.",
        "Monitored model drift by comparing incoming patient demographic distributions against training baselines, calculating PSI scores monthly and triggering retraining workflows when significant population shifts occurred due to policy changes or seasonal migration patterns.",
        "Optimized SQL queries retrieving eligibility records from on-premise databases, adding indexes and restructuring joins to reduce batch processing times from hours to minutes, enabling faster turnaround for enrollment verification and coverage determination workflows.",
        "Troubleshot Azure Function timeouts during peak enrollment periods when thousands of concurrent requests overwhelmed API rate limits, implementing exponential backoff retry logic and request queuing mechanisms to handle traffic spikes without dropping submissions.",
        "Configured Apache Spark jobs performing deduplication and record linkage across multiple state agency databases, resolving entity matching challenges where name variations and missing identifiers complicated accurate patient identification for care coordination initiatives.",
        "Integrated FHIR-compliant health information exchange APIs to standardize data formats across participating healthcare providers, transforming HL7 messages into normalized schemas stored in Azure Cosmos DB for interoperable clinical data sharing.",
        "Established validation frameworks comparing model predictions against manual audit samples, calculating confusion matrices and generating classification reports that informed stakeholder decisions about deploying automated case review systems in production environments.",
        "Coordinated with legal teams reviewing HIPAA compliance documentation for new analytics platforms, conducting privacy impact assessments and implementing business associate agreements ensuring third-party vendors handling PHI met regulatory security requirements.",
        "Refactored legacy SAS programs into Python pandas workflows running on Azure VM scale sets, modernizing actuarial modeling processes while preserving calculation logic verified against historical results to maintain continuity in budget forecasting accuracy.",
        "Built PowerBI dashboards visualizing program performance metrics including enrollment trends, service utilization rates, and cost per beneficiary, aggregating data from Azure SQL Database to support executive reporting and legislative oversight requirements."
      ],
      "environment": [
        "Python",
        "Azure ML Studio",
        "Azure Databricks",
        "Azure Data Factory",
        "XGBoost",
        "Scikit-Learn",
        "Apache Spark",
        "MLflow",
        "Azure Functions",
        "Azure Cosmos DB",
        "Azure SQL Database",
        "REST APIs",
        "PowerBI",
        "Pandas",
        "SQL",
        "FHIR",
        "HIPAA Compliance",
        "Healthcare Analytics"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "",
      "responsibilities": [
        "Planned credit risk modeling framework analyzing customer transaction histories and demographic attributes, designing feature engineering pipelines that aggregated payment behaviors and account activity patterns to predict default probabilities for loan underwriting decisions.",
        "Implemented logistic regression and random forest classifiers on Azure Machine Learning comparing model performance across different customer segments, calibrating probability thresholds to balance approval rates with expected loss targets aligned with risk appetite policies.",
        "Deployed fraud detection systems processing real-time transaction streams through Azure Stream Analytics, applying rule-based filters and machine learning models simultaneously to block suspicious card activity within milliseconds while minimizing customer friction.",
        "Monitored model performance dashboards tracking precision-recall tradeoffs across geographic regions and transaction types, investigating performance degradation when emerging fraud patterns bypassed existing detection rules and required urgent model updates.",
        "Optimized feature selection processes through recursive elimination experiments identifying predictive variables while removing correlated attributes, reducing model complexity and training times without sacrificing accuracy in credit score prediction applications.",
        "Troubleshot data quality issues discovered during model validation where missing values and encoding errors corrupted training datasets, implementing upstream validation checks and imputation strategies that preserved statistical properties of cleaned records.",
        "Configured Azure DevOps pipelines automating model deployment workflows from development to production environments, incorporating approval gates and A/B testing frameworks that gradually ramped traffic to new models while monitoring business metrics.",
        "Integrated regulatory reporting requirements into model development lifecycle documenting data sources, transformation logic, and model assumptions to support Federal Reserve stress testing examinations and PCI-DSS compliance audits.",
        "Established backtesting frameworks evaluating historical model predictions against actual outcomes, measuring lift curves and Gini coefficients that quantified incremental value provided by machine learning approaches compared to traditional credit scoring methods.",
        "Coordinated with fraud operations teams providing feedback on false positive rates from production detection systems, iteratively adjusting alert thresholds and enhancing feature sets based on investigator observations about emerging attack vectors."
      ],
      "environment": [
        "Python",
        "Azure Machine Learning",
        "Azure Stream Analytics",
        "Azure DevOps",
        "Scikit-Learn",
        "Random Forest",
        "Logistic Regression",
        "Pandas",
        "SQL Server",
        "Azure SQL Database",
        "PowerBI",
        "PCI-DSS Compliance",
        "Banking Regulations",
        "Fraud Detection",
        "Credit Risk Modeling"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "",
      "responsibilities": [
        "Planned Hadoop cluster capacity requirements for client data warehousing projects estimating storage growth based on historical ingestion rates, configuring HDFS replication factors and block sizes optimized for anticipated query patterns across diverse analytical workloads.",
        "Implemented Sqoop jobs extracting data from Oracle and MySQL transactional databases into HDFS, scheduling incremental loads through cron scripts that captured changed records using timestamp columns while validating record counts matched source systems.",
        "Deployed Informatica PowerCenter workflows transforming raw data files into conformed dimensional models, mapping source fields to target schemas and applying business rules for data quality checks that rejected invalid records into error handling queues.",
        "Monitored job execution logs identifying bottlenecks in multi-stage ETL pipelines, analyzing query execution plans and adjusting partition strategies in Hive tables to improve performance when aggregating billions of rows for monthly reporting requirements.",
        "Optimized MapReduce programs processing log files by experimenting with combiner functions and compression codecs, reducing intermediate data shuffled between mapper and reducer tasks which decreased overall job runtimes and cluster resource consumption.",
        "Troubleshot connection failures between Informatica servers and Hadoop clusters caused by Kerberos authentication misconfigurations, working with infrastructure teams to renew keytabs and adjust security policies enabling seamless data movement across environments.",
        "Configured HBase tables storing real-time sensor data from IoT devices, designing row key schemas that distributed writes evenly across region servers preventing hotspotting issues that previously caused ingestion latency spikes during high-volume periods.",
        "Integrated shell scripts automating routine cluster maintenance tasks including log rotation, temporary file cleanup, and health check validations, reducing manual operational overhead and preventing disk space exhaustion incidents that interrupted batch processing."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "Sqoop",
        "Informatica PowerCenter",
        "Hive",
        "MapReduce",
        "HBase",
        "Oracle",
        "MySQL",
        "Shell Scripting",
        "ETL",
        "Data Warehousing",
        "Kerberos"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}