{
  "name": "Yallaiah Onteru",
  "title": "Gen AI / ML Engineer",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Utilized Python and TensorFlow to develop a fraud detection model for insurance claims, reducing false positives by improving accuracy in identifying fraudulent patterns.",
    "Implemented a natural language processing pipeline using spaCy and Hugging Face Transformers to process customer feedback, enhancing sentiment analysis for healthcare service improvements.",
    "Engineered a recommendation system with Apache Spark and MLlib for personalized banking product suggestions, increasing customer engagement through tailored offers.",
    "Deployed a real-time data processing pipeline using Apache Kafka and Spark Streaming to monitor healthcare claims, ensuring timely detection of anomalies and compliance with HIPAA regulations.",
    "Developed a predictive maintenance model using PyTorch for manufacturing equipment, reducing downtime by anticipating failures before they occurred.",
    "Integrated AWS SageMaker with existing workflows to automate machine learning model training and deployment, streamlining the process for insurance risk assessment models.",
    "Applied A/B testing and ANOVA to optimize marketing campaigns in the banking sector, identifying the most effective strategies for customer acquisition.",
    "Created a chatbot using GPT and LangChain for customer support in healthcare, improving response times and patient satisfaction by providing instant assistance.",
    "Implemented a data lake solution on AWS S3 for storing and analyzing large-scale insurance data, enabling efficient data retrieval and analysis for decision-making.",
    "Utilized Docker and Kubernetes to containerize and orchestrate machine learning models, ensuring scalability and reliability in production environments.",
    "Developed a credit risk scoring model using XGBoost for banking, enhancing loan approval accuracy by incorporating diverse data sources.",
    "Applied PCA and clustering techniques to segment customer data in the insurance sector, enabling targeted marketing campaigns based on customer behavior.",
    "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for machine learning projects, ensuring seamless integration and deployment of models.",
    "Utilized Tableau and Power BI to create interactive dashboards for healthcare analytics, providing stakeholders with actionable insights into patient outcomes.",
    "Developed a time series forecasting model using Prophet for sales predictions in the retail banking sector, improving inventory management and financial planning.",
    "Integrated OpenAI APIs into a consulting project to automate report generation, reducing manual effort and increasing efficiency in delivering client insights.",
    "Collaborated with cross-functional teams to ensure machine learning models met business requirements and compliance standards, such as GDPR and PCI, across various domains."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "Machine Learning Models": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "Keras",
      "XGBoost",
      "LightGBM",
      "H2O",
      "AutoML",
      "Mllib"
    ],
    "Deep Learning Models": [
      "Convolutional Neural Networks (CNNs)",
      "Recurrent Neural Networks (RNNs)",
      "LSTMs",
      "Transformers",
      "Generative Models",
      "Attention Mechanisms",
      "Transfer Learning",
      "Fine-tuning LLMs"
    ],
    "Statistical Techniques": [
      "A/B Testing",
      "ANOVA",
      "Hypothesis Testing",
      "PCA",
      "Factor Analysis",
      "Regression (Linear, Logistic)",
      "Clustering (K-Means)",
      "Time Series (Prophet)"
    ],
    "Natural Language Processing": [
      "spaCy",
      "NLTK",
      "Hugging Face Transformers",
      "BERT",
      "GPT",
      "Stanford NLP",
      "TF-IDF",
      "LSI",
      "Lang Chain",
      "Llama Index",
      "OpenAI APIs",
      "MCP",
      "RAG Pipelines",
      "Crew AI",
      "Claude AI"
    ],
    "Data Manipulation & Visualization": [
      "Pandas",
      "NumPy",
      "SciPy",
      "Dask",
      "Apache Arrow",
      "seaborn",
      "matplotlib",
      "Seaborn",
      "Plotly",
      "Bokeh",
      "ggplot2",
      "Tableau",
      "Power BI",
      "D3.js"
    ],
    "Big Data Frameworks": [
      "Apache Spark",
      "Apache Hadoop",
      "Apache Flink",
      "Apache Kafka",
      "HBase",
      "Spark Streaming",
      "Hive",
      "MapReduce",
      "Databricks",
      "Apache Airflow",
      "dbt"
    ],
    "ETL & Data Pipelines": [
      "Apache Airflow",
      "AWS Glue",
      "Azure Data Factory",
      "Informatica",
      "Talend",
      "Apache NiFi",
      "Apache Beam",
      "Informatica PowerCenter",
      "SSIS"
    ],
    "Cloud Platforms": [
      "AWS (S3, SageMaker, Lambda, EC2, RDS, Redshift, Bedrock)",
      "Azure (ML Studio, Data Factory, Databricks, Cosmos DB)",
      "GCP (Big Query, Vertex AI, Cloud SQL)"
    ],
    "Web Technologies": [
      "REST APIs",
      "Flask",
      "Django",
      "Fast API",
      "React.js"
    ],
    "Statistical Software": [
      "R (dplyr, caret, ggplot2, tidyr)",
      "SAS",
      "STATA"
    ],
    "Databases": [
      "PostgreSQL",
      "MySQL",
      "Oracle",
      "Snowflake",
      "MongoDB",
      "Cassandra",
      "Redis",
      "Snowflake Elasticsearch",
      "AWS RDS",
      "Google Big Query",
      "SQL Server",
      "Netezza",
      "Teradata"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes"
    ],
    "MLOps & Deployment": [
      "ML flow",
      "DVC",
      "Kubeflow",
      "Docker",
      "Kubernetes",
      "Flask",
      "Fast API",
      "Streamlit"
    ],
    "Streaming & Messaging": [
      "Apache Kafka",
      "Spark Streaming",
      "Amazon Kinesis"
    ],
    "DevOps & CI/CD": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Jenkins",
      "GitHub Actions",
      "Terraform"
    ],
    "Development Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "RStudio",
      "Google Colab",
      "Anaconda"
    ]
  },
  "experience": [
    {
      "role": "AI Lead Engineer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Built a fraud detection system using TensorFlow and Python, integrating AWS S3 for data storage, which reduced false positives by improving accuracy in identifying fraudulent insurance claims.",
        "Developed a customer segmentation model with Apache Spark and MLlib, enabling targeted marketing campaigns that increased customer engagement by personalizing insurance product offers.",
        "Implemented a real-time claims monitoring pipeline using Apache Kafka and Spark Streaming, ensuring timely detection of anomalies and compliance with HIPAA regulations.",
        "Utilized AWS SageMaker to automate the training and deployment of machine learning models, streamlining the process for insurance risk assessment and improving model scalability.",
        "Created a chatbot using GPT and LangChain for customer support, improving response times and reducing the workload on human agents by providing instant assistance.",
        "Applied A/B testing and ANOVA to optimize marketing campaigns, identifying the most effective strategies for customer acquisition in the insurance sector.",
        "Developed a data lake solution on AWS S3 for storing and analyzing large-scale insurance data, enabling efficient data retrieval and analysis for decision-making.",
        "Integrated Docker and Kubernetes to containerize and orchestrate machine learning models, ensuring scalability and reliability in production environments.",
        "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for machine learning projects, ensuring seamless integration and deployment of models.",
        "Collaborated with cross-functional teams to ensure machine learning models met business requirements and compliance standards, such as GDPR and PCI.",
        "Conducted code reviews and debugging sessions to maintain code quality and resolve issues in the fraud detection system, ensuring robust performance.",
        "Facilitated meetings with stakeholders to gather requirements and provide updates on project progress, ensuring alignment with business goals.",
        "Mentored junior team members on best practices in machine learning and data engineering, fostering a culture of continuous learning and improvement.",
        "Performed troubleshooting and optimization of existing models, improving their performance and reducing computational costs.",
        "Developed interactive dashboards using Tableau and Power BI for insurance analytics, providing stakeholders with actionable insights into customer behavior and claims trends.",
        "Implemented a time series forecasting model using Prophet for sales predictions, improving inventory management and financial planning in the insurance sector."
      ],
      "environment": [
        "Python, TensorFlow, AWS (S3, SageMaker), Apache Spark, MLlib, Apache Kafka, Spark Streaming, GPT, LangChain, Docker, Kubernetes, Jenkins, GitHub Actions, Tableau, Power BI, Prophet"
      ]
    },
    {
      "role": "Senior AI Engineer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Engineered a natural language processing pipeline using spaCy and Hugging Face Transformers to process customer feedback, enhancing sentiment analysis for healthcare service improvements.",
        "Developed a recommendation system for personalized healthcare product suggestions using Apache Spark and MLlib, increasing customer engagement through tailored offers.",
        "Deployed a real-time data processing pipeline using Apache Kafka and Spark Streaming to monitor healthcare claims, ensuring compliance with HIPAA regulations.",
        "Implemented a predictive maintenance model using PyTorch for medical equipment, reducing downtime by anticipating failures before they occurred.",
        "Integrated AWS SageMaker with existing workflows to automate machine learning model training and deployment, streamlining the process for healthcare risk assessment models.",
        "Applied PCA and clustering techniques to segment patient data, enabling targeted healthcare interventions based on patient behavior.",
        "Created a chatbot using GPT and LangChain for patient support, improving response times and patient satisfaction by providing instant assistance.",
        "Utilized Docker and Kubernetes to containerize and orchestrate machine learning models, ensuring scalability and reliability in production environments.",
        "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for machine learning projects, ensuring seamless integration and deployment of models.",
        "Collaborated with cross-functional teams to ensure machine learning models met business requirements and compliance standards, such as GDPR and HIPAA.",
        "Conducted code reviews and debugging sessions to maintain code quality and resolve issues in the NLP pipeline, ensuring accurate sentiment analysis.",
        "Facilitated meetings with stakeholders to gather requirements and provide updates on project progress, ensuring alignment with healthcare objectives.",
        "Mentored junior team members on best practices in machine learning and data engineering, fostering a culture of continuous learning and improvement.",
        "Performed troubleshooting and optimization of existing models, improving their performance and reducing computational costs."
      ],
      "environment": [
        "Python, spaCy, Hugging Face Transformers, Apache Spark, MLlib, Apache Kafka, Spark Streaming, PyTorch, AWS SageMaker, GPT, LangChain, Docker, Kubernetes, Jenkins, GitHub Actions"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Developed a fraud detection model using TensorFlow and Python, integrating GCP Big Query for data storage, which reduced false positives by improving accuracy in identifying fraudulent healthcare claims.",
        "Implemented a customer segmentation model with Apache Spark and MLlib, enabling targeted marketing campaigns that increased customer engagement by personalizing healthcare product offers.",
        "Deployed a real-time claims monitoring pipeline using Apache Kafka and Spark Streaming, ensuring timely detection of anomalies and compliance with HIPAA regulations.",
        "Utilized GCP Vertex AI to automate the training and deployment of machine learning models, streamlining the process for healthcare risk assessment and improving model scalability.",
        "Created a chatbot using GPT and LangChain for customer support, improving response times and reducing the workload on human agents by providing instant assistance.",
        "Applied A/B testing and ANOVA to optimize marketing campaigns, identifying the most effective strategies for customer acquisition in the healthcare sector.",
        "Developed a data lake solution on GCP Big Query for storing and analyzing large-scale healthcare data, enabling efficient data retrieval and analysis for decision-making.",
        "Integrated Docker and Kubernetes to containerize and orchestrate machine learning models, ensuring scalability and reliability in production environments.",
        "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for machine learning projects, ensuring seamless integration and deployment of models.",
        "Collaborated with cross-functional teams to ensure machine learning models met business requirements and compliance standards, such as GDPR and HIPAA.",
        "Conducted code reviews and debugging sessions to maintain code quality and resolve issues in the fraud detection system, ensuring robust performance.",
        "Facilitated meetings with stakeholders to gather requirements and provide updates on project progress, ensuring alignment with healthcare goals.",
        "Mentored junior team members on best practices in machine learning and data engineering, fostering a culture of continuous learning and improvement."
      ],
      "environment": [
        "Python, TensorFlow, GCP (Big Query, Vertex AI), Apache Spark, MLlib, Apache Kafka, Spark Streaming, GPT, LangChain, Docker, Kubernetes, Jenkins, GitHub Actions"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Developed a credit risk scoring model using XGBoost, enhancing loan approval accuracy by incorporating diverse data sources and improving risk assessment.",
        "Implemented a recommendation system for personalized banking product suggestions using Apache Spark and MLlib, increasing customer engagement through tailored offers.",
        "Applied PCA and clustering techniques to segment customer data, enabling targeted marketing campaigns based on customer behavior.",
        "Utilized Azure Machine Learning Studio to automate the training and deployment of machine learning models, streamlining the process for banking risk assessment.",
        "Created a chatbot using GPT and LangChain for customer support, improving response times and reducing the workload on human agents by providing instant assistance.",
        "Developed a time series forecasting model using Prophet for sales predictions, improving inventory management and financial planning in the banking sector.",
        "Integrated Docker and Kubernetes to containerize and orchestrate machine learning models, ensuring scalability and reliability in production environments.",
        "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for machine learning projects, ensuring seamless integration and deployment of models.",
        "Collaborated with cross-functional teams to ensure machine learning models met business requirements and compliance standards, such as PCI and GDPR.",
        "Conducted code reviews and debugging sessions to maintain code quality and resolve issues in the credit risk model, ensuring accurate risk assessment."
      ],
      "environment": [
        "Python, XGBoost, Apache Spark, MLlib, Azure (Machine Learning Studio), GPT, LangChain, Docker, Kubernetes, Jenkins, GitHub Actions, Prophet"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Built a data pipeline using Apache Airflow and AWS Glue for ETL processes, improving data flow efficiency and reducing processing time.",
        "Developed a data lake solution on AWS S3 for storing and analyzing large-scale consulting data, enabling efficient data retrieval and analysis for decision-making.",
        "Integrated Docker and Kubernetes to containerize and orchestrate data processing tasks, ensuring scalability and reliability in production environments.",
        "Implemented a CI/CD pipeline using Jenkins and GitHub Actions for data engineering projects, ensuring seamless integration and deployment of pipelines.",
        "Collaborated with cross-functional teams to ensure data pipelines met business requirements and compliance standards, such as GDPR.",
        "Conducted code reviews and debugging sessions to maintain code quality and resolve issues in the data pipeline, ensuring robust performance.",
        "Facilitated meetings with stakeholders to gather requirements and provide updates on project progress, ensuring alignment with consulting objectives.",
        "Mentored junior team members on best practices in data engineering, fostering a culture of continuous learning and improvement."
      ],
      "environment": [
        "Python, Apache Airflow, AWS Glue, AWS S3, Docker, Kubernetes, Jenkins, GitHub Actions"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Certified Salesforce Developer PD1"
  ]
}