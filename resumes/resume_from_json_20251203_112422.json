{
  "name": "Yallaiah Onteru",
  "title": "Senior AI Engineer - Enterprise RAG & Vector Systems",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "I am having 10 years of experience in building enterprise-grade AI systems, specializing in RAG architecture, vector databases, and compliant AI solutions for regulated industries like banking, healthcare, and insurance sectors.",
    "Designed secure RAG pipelines using Pinecone and OpenSearch with metadata filtering for entitlement-aware retrieval, ensuring RBAC integration and audit logging to meet financial services compliance standards across production environments.",
    "Built scalable content ingestion frameworks processing PDFs, emails, and unstructured documents using AWS Textract OCR, achieving semantic search accuracy while maintaining PII redaction protocols for sensitive banking and healthcare data.",
    "Integrated OpenAI and Azure OpenAI APIs with custom embedding models to create multi-turn conversation agents, optimizing response latency under 30 seconds while debugging vector indexing strategies for high-concurrency enterprise workloads.",
    "Developed Python-based ETL pipelines connecting SharePoint, AEM, and Content Cloud repositories to Weaviate vector stores, implementing automated metadata tagging and IAM-compliant data retrieval mechanisms for regulated content management systems.",
    "Configured Elastic Vector Store clusters with encryption standards for banking applications, troubleshooting query performance issues during code reviews and establishing CI/CD workflows for ML pipeline deployments with automated accuracy testing frameworks.",
    "Collaborated with security teams to implement RBAC controls across AI agent architectures, ensuring financial data protection through encrypted vector storage and real-time audit trails while coordinating with compliance stakeholders on explainability requirements.",
    "Architected NLP workflows using PyTorch and TensorFlow for document classification tasks, spending time in team meetings discussing embedding generation strategies and debugging semantic search recall issues across distributed vector databases in production.",
    "Established MLOps practices for RAG systems including model monitoring, prompt versioning, and drift detection mechanisms, learning through trial-and-error how to balance retrieval accuracy against system latency constraints in enterprise banking environments.",
    "Implemented Kubernetes-based container orchestration for AI workloads on AWS, working closely with cloud teams to resolve Docker configuration issues and optimize resource allocation for high-throughput vector search operations during peak transaction periods.",
    "Utilized LangChain frameworks to build proof-of-concept multi-agent systems for insurance claims processing, initially struggling with agent coordination patterns before discovering effective state management approaches through iterative testing and stakeholder feedback sessions.",
    "Consolidated unstructured data ingestion using Apache Spark and Azure Data Factory, handling schema evolution challenges and debugging transformation logic during daily standups to ensure HIPAA-compliant data flows for healthcare analytics platforms with strict regulatory oversight.",
    "Managed REST API development for AI services using Node.js and Java backends, coordinating with frontend teams during code reviews to troubleshoot authentication flows and refine GraphQL schemas for seamless enterprise application integration patterns.",
    "Applied prompt engineering techniques across Cohere and local embedding models, documenting lessons learned from failed experiments and maintaining version-controlled prompt libraries to standardize AI agent behaviors across multiple banking use cases and departments.",
    "Tuned vector similarity thresholds and chunk size parameters in OpenSearch deployments, conducting A/B tests on retrieval strategies and presenting findings to architecture teams to inform decisions on scaling vector stores for growing document repositories.",
    "Maintained CloudWatch observability dashboards for AI pipeline health monitoring, responding to production incidents by analyzing audit logs and coordinating with database administrators to resolve indexing bottlenecks affecting real-time retrieval performance metrics.",
    "Contributed to cross-functional projects integrating AI with legacy banking IAM systems, navigating complex entitlement schemas and attending weekly alignment meetings to ensure secure data access patterns met both technical requirements and compliance audit expectations.",
    "Enhanced SQL query optimization for feature store implementations supporting vector embeddings, learning data modeling best practices through hands-on troubleshooting of slow joins and indexing strategies that impacted end-to-end RAG pipeline response times."
  ],
  "technical_skills": {
    "AI & Machine Learning Frameworks": [
      "OpenAI APIs",
      "Azure OpenAI",
      "Cohere APIs",
      "TensorFlow",
      "PyTorch",
      "Scikit-Learn",
      "Hugging Face Transformers",
      "Embedding Models",
      "LLM Fine-Tuning",
      "Model Optimization"
    ],
    "RAG & Vector Technologies": [
      "Pinecone",
      "Weaviate",
      "OpenSearch",
      "Elastic Vector Store",
      "Semantic Search",
      "Vector Indexing",
      "Metadata Filtering",
      "Chunk Strategies",
      "Retrieval-Augmented Generation",
      "Similarity Search"
    ],
    "AI Agent Frameworks": [
      "LangChain",
      "LlamaIndex",
      "LangGraph",
      "Multi-Agent Systems",
      "Crew AI",
      "AutoGen",
      "Agent Orchestration",
      "Model Context Protocol",
      "Conversational AI",
      "Prompt Engineering"
    ],
    "Programming & Backend Development": [
      "Python",
      "Java",
      "Node.js",
      "Scala",
      "SQL",
      "Bash/Shell",
      "Async Python",
      "REST APIs",
      "GraphQL",
      "FastAPI",
      "Flask",
      "Django"
    ],
    "Cloud Platforms & Services": [
      "AWS S3",
      "AWS Lambda",
      "AWS SageMaker",
      "AWS Bedrock",
      "AWS Textract",
      "AWS Glue",
      "Azure ML Studio",
      "Azure Data Factory",
      "Azure Databricks",
      "CloudWatch",
      "EC2",
      "RDS"
    ],
    "Vector & Data Storage": [
      "PostgreSQL",
      "MongoDB",
      "Elasticsearch",
      "Snowflake",
      "Redis",
      "Cassandra",
      "MySQL",
      "Oracle",
      "Feature Stores",
      "Data Modeling",
      "SQL Optimization"
    ],
    "Data Engineering & ETL": [
      "Apache Spark",
      "PySpark",
      "Apache Airflow",
      "Apache Kafka",
      "Spark Streaming",
      "Data Pipelines",
      "ETL Workflows",
      "Schema Evolution",
      "Structured Data Processing",
      "Unstructured Data Ingestion"
    ],
    "OCR & Document Processing": [
      "AWS Textract",
      "OCR Tools",
      "PDF Parsing",
      "Document Text Extraction",
      "Content Ingestion",
      "Metadata Tagging",
      "PII Redaction",
      "Data Masking"
    ],
    "Enterprise Integration & Content Systems": [
      "SharePoint",
      "AEM",
      "Content Cloud",
      "Enterprise IAM",
      "RBAC",
      "Entitlement Systems",
      "Audit Logging",
      "API Security",
      "Legacy System Integration"
    ],
    "Security & Compliance": [
      "IAM",
      "RBAC",
      "PCI-DSS",
      "HIPAA",
      "GDPR",
      "Encryption Standards",
      "Data Security",
      "Financial Services Compliance",
      "Audit Trails",
      "PII Protection"
    ],
    "MLOps & DevOps": [
      "CI/CD Pipelines",
      "Docker",
      "Kubernetes",
      "MLflow",
      "Model Monitoring",
      "Drift Detection",
      "Automated Testing",
      "GitHub Actions",
      "Jenkins",
      "Terraform"
    ],
    "Observability & Monitoring": [
      "CloudWatch",
      "Splunk",
      "ELK Stack",
      "Logging Frameworks",
      "Performance Monitoring",
      "Incident Response",
      "Dashboard Creation",
      "Alert Configuration"
    ],
    "NLP & Text Processing": [
      "spaCy",
      "NLTK",
      "TF-IDF",
      "Named Entity Recognition",
      "Text Classification",
      "Sentiment Analysis",
      "Document Embeddings",
      "Token Management"
    ],
    "Development & Collaboration Tools": [
      "Git",
      "GitHub",
      "GitLab",
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "Anaconda",
      "JSON/YAML Config Management",
      "Documentation"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Build RAG-powered insurance claims agent using Pinecone vector database with LangGraph multi-agent orchestration, integrating AWS Bedrock embeddings and implementing RBAC-based retrieval filters to ensure policy document access aligns with user entitlements.",
        "Create content ingestion pipeline processing insurance policy PDFs via AWS Textract OCR, extracting structured fields and unstructured clauses into OpenSearch indexes while debugging chunk overlap parameters to improve semantic search recall for complex queries.",
        "Architect multi-turn conversation system using OpenAI GPT models with Model Context Protocol, spending time in design sessions mapping dialog flows and troubleshooting context window limitations when handling lengthy insurance regulation documents during agent interactions.",
        "Implement proof-of-concept agent-to-agent communication framework for claims validation workflows, initially struggling with state synchronization issues before discovering effective message passing patterns through iterative testing with insurance domain stakeholders and compliance officers.",
        "Integrate AWS Lambda serverless functions with PySpark batch jobs for real-time policy data transformation, coordinating with cloud architects during code reviews to resolve IAM permission conflicts and optimize execution times for high-volume claims processing periods.",
        "Configure Weaviate vector store with insurance-specific metadata schemas, attending weekly meetings with security teams to establish encryption standards and audit logging mechanisms that satisfy state insurance regulatory requirements for data protection and explainability.",
        "Develop Python-based API layer using FastAPI to expose RAG services to frontend claims applications, working closely with UX teams to refine response formatting and troubleshoot authentication flows through AWS Cognito integration for agent access control.",
        "Optimize embedding generation workflows using Azure OpenAI for policy comparison features, conducting experiments with different chunk sizes and learning that smaller segments improved retrieval precision but required careful prompt engineering to maintain contextual coherence in responses.",
        "Establish CI/CD pipeline with GitHub Actions for automated testing of multi-agent systems, documenting model accuracy metrics and latency benchmarks while collaborating with MLOps teams to set up drift detection alerts for production RAG endpoints serving customer-facing applications.",
        "Tune vector similarity thresholds in Pinecone indexes through A/B testing on historical claims data, presenting findings to insurance analysts and adjusting retrieval parameters based on feedback to balance response relevance against system performance under concurrent user loads.",
        "Monitor CloudWatch dashboards for RAG pipeline health, responding to production incidents by analyzing audit logs and coordinating with database teams to resolve indexing bottlenecks affecting real-time policy retrieval during peak business hours and seasonal surges.",
        "Collaborate with compliance officers to design explainability layers for AI agent decisions, implementing citation tracking mechanisms that reference specific policy document sections and spending time in stakeholder meetings explaining how vector search rankings inform agent recommendations.",
        "Maintain SharePoint connector for legacy insurance document repositories, troubleshooting synchronization failures during daily standups and implementing retry logic with exponential backoff to ensure consistent content availability for RAG retrieval in distributed enterprise environments.",
        "Refine prompt templates for insurance domain-specific tasks, versioning prompt libraries in Git and learning through trial-and-error which instruction patterns yielded better claim classification accuracy while avoiding hallucination issues in generated agent responses.",
        "Investigate Kubernetes resource allocation for containerized AI workloads on AWS EKS, working with DevOps teams to adjust pod configurations and resolve out-of-memory errors during stress testing of concurrent agent sessions processing complex underwriting scenarios.",
        "Conduct knowledge transfer sessions with junior developers on vector database indexing strategies, sharing lessons learned from debugging production issues and documenting best practices for metadata filtering patterns that ensure entitlement-aware retrieval aligns with insurance regulations."
      ],
      "environment": [
        "Python",
        "AWS Bedrock",
        "AWS Lambda",
        "AWS Textract",
        "AWS S3",
        "AWS EKS",
        "Pinecone",
        "Weaviate",
        "OpenSearch",
        "OpenAI GPT",
        "Azure OpenAI",
        "LangGraph",
        "Model Context Protocol",
        "Multi-Agent Systems",
        "PySpark",
        "FastAPI",
        "Docker",
        "Kubernetes",
        "GitHub Actions",
        "CloudWatch",
        "SharePoint",
        "IAM",
        "RBAC",
        "Audit Logging",
        "Encryption Standards",
        "Insurance Regulations",
        "Semantic Search",
        "Vector Embeddings",
        "RAG Pipelines",
        "Proof of Concepts"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Designed HIPAA-compliant RAG system for pharmaceutical research document retrieval using Pinecone vector database, implementing metadata-based access controls and PII redaction logic to ensure patient data security across distributed research teams and regulatory audit requirements.",
        "Constructed ETL pipeline ingesting clinical trial PDFs and medical research papers via AWS Textract OCR, parsing complex tables and extracting drug efficacy data into structured formats while troubleshooting encoding issues during team meetings for multilingual document processing.",
        "Deployed multi-agent conversation framework with LangChain for adverse event reporting workflows, coordinating with healthcare compliance teams to establish agent decision boundaries and debugging context retention problems when processing lengthy FDA submission documents during pilot testing.",
        "Automated semantic search infrastructure on AWS using OpenSearch and Azure OpenAI embeddings, spending late hours optimizing vector indexing strategies and learning that hierarchical metadata tagging significantly improved retrieval precision for specialized pharmaceutical terminology queries.",
        "Integrated AEM content management system with Weaviate vector store for regulatory document versioning, attending cross-functional meetings to align on schema designs and resolving synchronization delays that affected real-time access to updated clinical guidelines and safety protocols.",
        "Established Python REST API backend with Flask to serve AI agent responses to healthcare applications, collaborating with frontend engineers during code reviews to handle asynchronous request patterns and implement JWT authentication for secure endpoint access in production environments.",
        "Validated AI model outputs against HIPAA compliance standards through manual testing cycles, documenting edge cases where embeddings leaked sensitive information and iteratively refining PII masking rules with privacy officers to prevent data exposure in vector representations.",
        "Assembled proof-of-concept multi-agent system using Crew AI framework for clinical trial matching, initially facing challenges coordinating agent communication protocols before discovering effective task delegation patterns through experimentation with different orchestration configurations and stakeholder feedback loops.",
        "Monitored AWS CloudWatch metrics for RAG pipeline latency and throughput, responding to after-hours alerts when vector query response times exceeded service level agreements and working with infrastructure teams to scale OpenSearch cluster capacity during high-demand research periods.",
        "Customized LangChain retrieval chains for drug interaction queries, iterating on prompt templates through trial-and-error and learning that explicit instruction formatting reduced hallucination rates while maintaining conversational naturalness in agent responses to clinician questions.",
        "Enforced encryption at rest for vector embeddings stored in Pinecone, navigating AWS KMS key management complexities during implementation sprints and coordinating with security architects to ensure encryption standards met pharmaceutical industry requirements for intellectual property protection.",
        "Supported migration of legacy SQL-based document search to vector-powered semantic retrieval, attending knowledge transfer sessions with database administrators and troubleshooting SQL-to-vector translation logic that impacted query result consistency during parallel system operation phases.",
        "Analyzed embedding model performance using TensorFlow for domain-specific fine-tuning experiments, discovering through hands-on testing that medical terminology required specialized tokenization approaches to improve vector similarity accuracy for rare disease research document clustering tasks.",
        "Participated in HIPAA audit preparation activities by generating detailed logging reports from audit trail systems, explaining RAG architecture decisions to regulatory assessors and demonstrating how access control mechanisms prevented unauthorized retrieval of protected health information across agent interactions."
      ],
      "environment": [
        "Python",
        "AWS S3",
        "AWS Textract",
        "AWS Lambda",
        "AWS CloudWatch",
        "Azure OpenAI",
        "Pinecone",
        "Weaviate",
        "OpenSearch",
        "LangChain",
        "Crew AI",
        "Multi-Agent Systems",
        "Flask",
        "REST APIs",
        "JWT Authentication",
        "Docker",
        "AEM",
        "TensorFlow",
        "PII Redaction",
        "HIPAA Compliance",
        "FDA Regulations",
        "Encryption Standards",
        "Audit Logging",
        "IAM",
        "RBAC",
        "Semantic Search",
        "Vector Embeddings",
        "OCR Processing",
        "ETL Pipelines",
        "Clinical Data Security"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Developed HIPAA-compliant NLP pipeline for public health record classification using Azure ML Studio and spaCy, implementing text preprocessing workflows that sanitized patient identifiers while preserving clinical context needed for accurate disease surveillance reporting.",
        "Orchestrated data ingestion from legacy healthcare databases into Azure Data Factory, spending significant time debugging schema mapping errors during weekly synchronization runs and coordinating with state IT administrators to resolve VPN connectivity issues affecting batch job execution.",
        "Trained PyTorch-based document embedding models for medical form categorization, conducting experiments with different transformer architectures and learning through repeated validation cycles that domain-specific pretraining on state health guidelines improved classification accuracy for public assistance applications.",
        "Configured Azure Cosmos DB for storing healthcare metadata with encryption, attending security review meetings to demonstrate compliance with state data protection regulations and troubleshooting replication lag issues that occasionally delayed real-time access to patient eligibility information.",
        "Constructed SQL-based feature engineering pipelines for Medicaid claims analysis, working closely with policy analysts to understand business rules and refining join logic after discovering data quality issues during exploratory analysis sessions that impacted downstream model predictions.",
        "Automated report generation using Python scripts connecting to Azure SQL databases, initially struggling with transaction timeout errors before implementing connection pooling and retry mechanisms that stabilized scheduled reporting workflows for state health department leadership dashboards.",
        "Validated model predictions against historical healthcare outcomes through manual audit processes, documenting discrepancies found during testing phases and collaborating with domain experts to adjust classification thresholds that balanced false positive rates with operational efficiency targets.",
        "Managed Azure Databricks clusters for distributed data processing, responding to resource allocation alerts during month-end reporting periods and learning that spot instance strategies reduced compute costs without compromising job completion times for non-critical analytics workloads.",
        "Implemented REST API endpoints using Python Flask for model inference requests, participating in code reviews with application developers to refine error handling patterns and ensure consistent response formats matched frontend integration requirements across multiple state health programs.",
        "Monitored Azure Application Insights dashboards for ML pipeline health, investigating sporadic prediction failures and discovering that input data format inconsistencies from different county health systems required additional preprocessing validation steps to prevent model inference errors.",
        "Documented ML architecture decisions in technical specification documents, presenting design choices to state procurement committees and explaining how Azure services met strict public sector requirements for data sovereignty, audit trails, and system reliability during multi-year contract negotiations.",
        "Collaborated with state HIPAA compliance officers to design audit logging mechanisms, capturing model prediction events with user attribution and ensuring log retention policies aligned with federal regulations governing protected health information access in government healthcare programs."
      ],
      "environment": [
        "Python",
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure Databricks",
        "Azure Cosmos DB",
        "Azure SQL Database",
        "Azure Application Insights",
        "PyTorch",
        "spaCy",
        "NLTK",
        "Scikit-Learn",
        "SQL",
        "Flask",
        "REST APIs",
        "NLP Pipelines",
        "Document Classification",
        "Feature Engineering",
        "HIPAA Compliance",
        "State Healthcare Regulations",
        "Data Encryption",
        "Audit Logging",
        "ETL Workflows",
        "Batch Processing",
        "Report Automation"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Analyzed transaction fraud patterns using Scikit-Learn classification models on Azure ML platform, processing millions of daily payment records and implementing PCI-DSS compliant data handling procedures that masked sensitive cardholder information during model training cycles.",
        "Queried Azure SQL databases for historical banking data used in risk modeling projects, attending cross-team meetings to clarify business logic around transaction categorization and troubleshooting slow query performance that delayed scheduled model retraining jobs during quarterly updates.",
        "Prepared feature engineering workflows in Python using Pandas for credit risk assessment models, discovering through exploratory analysis that certain derived metrics improved default prediction accuracy and documenting findings in technical reports shared with senior risk management committees.",
        "Evaluated model performance metrics through A/B testing frameworks deployed in production systems, monitoring false positive rates for fraud detection algorithms and coordinating with operations teams to adjust decision thresholds based on feedback from frontline banking staff.",
        "Processed unstructured customer service transcripts using NLTK for sentiment analysis, spending time cleaning noisy text data and learning that preprocessing steps like stopword removal significantly affected downstream classification results for complaint categorization tasks.",
        "Connected to Azure Data Factory for scheduled ETL jobs moving data between operational systems and analytical databases, troubleshooting pipeline failures during daily standups and implementing email notification alerts that improved team response times to data processing errors.",
        "Generated statistical reports using R and ggplot2 for executive presentations on portfolio performance, iterating on visualization designs after receiving stakeholder feedback during review sessions and ensuring chart formatting met corporate branding standards for investor communications.",
        "Maintained Azure Data Lake storage hierarchies for regulatory reporting datasets, collaborating with compliance teams to define retention policies and encryption requirements that satisfied banking audit standards while balancing storage cost optimization strategies.",
        "Participated in code review sessions with senior data scientists, receiving guidance on statistical modeling techniques and gradually improving Python coding practices through constructive feedback that helped refine data manipulation logic for complex financial calculations.",
        "Documented analytical methodologies in Confluence wiki pages, creating reference materials for junior team members and learning that clear technical writing required multiple revision cycles to explain model assumptions and limitations in language accessible to non-technical business partners."
      ],
      "environment": [
        "Python",
        "R",
        "Azure ML Studio",
        "Azure SQL Database",
        "Azure Data Factory",
        "Azure Data Lake",
        "Scikit-Learn",
        "Pandas",
        "NumPy",
        "NLTK",
        "ggplot2",
        "SQL",
        "Statistical Analysis",
        "Feature Engineering",
        "Fraud Detection",
        "Risk Modeling",
        "PCI-DSS Compliance",
        "Banking Regulations",
        "Data Encryption",
        "ETL Pipelines",
        "A/B Testing",
        "Sentiment Analysis",
        "Reporting Automation"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Learned Hadoop ecosystem fundamentals by configuring HDFS clusters for client data warehousing projects, spending early career months understanding distributed file system concepts and troubleshooting node connectivity issues with guidance from senior engineers during onboarding period.",
        "Wrote Sqoop scripts to transfer relational data from Oracle databases into Hadoop storage, initially making mistakes with import parameter configurations before gradually improving through repeated practice and code reviews that taught proper handling of schema evolution scenarios.",
        "Assisted in Informatica PowerCenter workflow development for ETL processes, shadowing experienced developers during requirement gathering meetings and learning to translate business rules into transformation logic while debugging data quality issues found in testing phases.",
        "Queried Hive tables using SQL for basic reporting tasks requested by consulting clients, attending training sessions to understand HiveQL syntax differences from standard SQL and improving query optimization skills through trial-and-error with different join strategies.",
        "Monitored batch job executions in Hadoop clusters, responding to late-night failures by checking log files and escalating complex issues to senior team members while gradually building troubleshooting skills that improved incident resolution times over project tenure.",
        "Participated in data profiling exercises to assess source system data quality, manually reviewing sample records in Excel and documenting anomalies that informed discussions with clients about cleansing requirements before migrating data into analytical platforms.",
        "Gained exposure to Informatica metadata management by updating repository documentation for ETL workflows, learning the importance of thorough documentation when changes made during development cycles needed to be communicated to offshore maintenance teams.",
        "Contributed to client deliverables by preparing basic data pipeline monitoring reports, gradually taking on more responsibility as confidence grew and receiving positive feedback from project leads that recognized improving technical capabilities and work ethic."
      ],
      "environment": [
        "Hadoop",
        "HDFS",
        "Apache Sqoop",
        "Hive",
        "HiveQL",
        "Informatica PowerCenter",
        "Oracle Database",
        "SQL",
        "ETL Workflows",
        "Data Profiling",
        "Batch Processing",
        "Log Analysis",
        "Metadata Management",
        "Consulting Projects"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": [
    "Microsoft Certified: Azure Data Engineer Associate",
    "Microsoft Certified: Azure AI Engineer Associate",
    "AWS Certified Machine Learning Engineer \u2013 Associate",
    "Salesforce Certified Salesforce Developer PD1"
  ]
}