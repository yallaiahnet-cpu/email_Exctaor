{
  "name": "Yallaiah Onteru",
  "title": "Senior AI/ML Engineer - Public Safety & Law Enforcement Data Platforms",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Delivered AI-driven solutions across Insurance, Healthcare, Banking, and Consulting sectors while building natural language query systems and predictive analytics over complex relational datasets using RAG, LLMs, and agentic AI frameworks on AWS Cloud.",
    "Architected retrieval-augmented generation pipelines using AWS Bedrock with Claude and AWS Titan models to enable semantic search over CAD, RMS, and FRMS datasets while maintaining CJIS compliance and PII masking protocols for public safety applications.",
    "Deployed multi-agent systems using LangGraph and LangChain frameworks to orchestrate complex workflows, implementing agent-to-agent communication protocols and Model Context Protocol for coordinated task execution across distributed AI services.",
    "Constructed vector databases using Pinecone to store embeddings from public safety incident reports, enabling sub-second semantic retrieval over millions of records while integrating with Oracle and SQL Server sources through AWS data ingestion pipelines.",
    "Transformed on-premises CAD and RMS data into cloud-optimized formats using Python-based ETL workflows, applying data masking techniques to protect sensitive law enforcement information while preserving analytical utility for predictive models.",
    "Validated LLM outputs using custom evaluation frameworks that measured accuracy and relevance of natural language queries against ground truth public safety datasets, iterating on prompt engineering strategies to improve response quality.",
    "Configured AWS SageMaker endpoints to serve TensorFlow and PyTorch models for predictive analytics workloads, utilizing EC2 instances with GPU acceleration and S3 for scalable storage of training datasets and model artifacts across environments.",
    "Implemented prompt versioning systems to track changes in natural language query templates, ensuring consistent behavior across multiple deployments while enabling A/B testing of different prompting strategies for law enforcement use cases.",
    "Collaborated with law enforcement stakeholders to translate complex public safety data relationships into user-friendly AI interfaces, gathering requirements through regular meetings and incorporating feedback into iterative development cycles.",
    "Developed hybrid search capabilities combining vector embeddings with keyword matching to handle diverse query patterns in investigative scenarios, balancing recall and precision based on user feedback from field testing sessions.",
    "Monitored AI pipeline performance using cloud-native observability tools to track latency, throughput, and error rates in production environments, setting up alerts for anomalies in semantic search accuracy or data processing failures.",
    "Integrated knowledge graphs to represent entity relationships in public safety datasets, connecting persons, locations, incidents, and vehicles across multiple systems to enable sophisticated graph-based reasoning for investigative queries.",
    "Optimized RAG retrieval strategies by tuning chunk sizes, overlap ratios, and similarity thresholds based on empirical testing with real law enforcement queries, achieving high-recall results for time-sensitive investigative searches.",
    "Enforced secure data governance practices throughout the AI development lifecycle, implementing IAM policies and VPC configurations to restrict access to CJIS-regulated datasets while enabling authorized analytics workloads and monitoring.",
    "Prototyped proof-of-concept solutions demonstrating agentic AI capabilities for automated report generation, testing different LLM architectures and retrieval methods before selecting optimal approaches for scalable production deployment.",
    "Streamlined model deployment workflows using containerized services, packaging embedding models and LLM inference logic into reusable components that significantly reduced setup time for new public safety data integration projects.",
    "Debugged complex issues in multi-agent orchestration by tracing message flows between agents, identifying bottlenecks in communication protocols, and refining error handling to improve system reliability during high-load scenarios.",
    "Participated in code reviews focused on explainability and auditability of AI-driven decision support tools, ensuring that model outputs could be traced back to source data and reasoning steps for compliance with legal requirements."
  ],
  "technical_skills": {
    "Programming Languages": [
      "Python",
      "R",
      "Java",
      "SQL",
      "Scala",
      "Bash/Shell",
      "TypeScript"
    ],
    "AI/ML Frameworks & Libraries": [
      "Scikit-Learn",
      "TensorFlow",
      "PyTorch",
      "Keras",
      "XGBoost",
      "LightGBM",
      "H2O",
      "AutoML",
      "Mllib",
      "Hugging Face Transformers"
    ],
    "Agentic AI & LLM Orchestration": [
      "LangGraph",
      "LangChain",
      "Crew AI",
      "Model Context Protocol (MCP)",
      "Agent-to-Agent (A2A)",
      "Multi-Agent Systems",
      "Prompt Engineering",
      "Prompt Versioning"
    ],
    "Large Language Models": [
      "AWS Bedrock",
      "Claude AI",
      "AWS Titan",
      "AWS Nova",
      "GPT Models",
      "BERT",
      "Llama Index",
      "OpenAI APIs",
      "Fine-tuning LLMs"
    ],
    "Retrieval-Augmented Generation": [
      "RAG Pipelines",
      "Semantic Search",
      "Embedding Models",
      "Hybrid Search",
      "Vector Similarity",
      "RAG Optimization",
      "LLM Evaluation Frameworks"
    ],
    "Vector Databases & Knowledge Graphs": [
      "Pinecone",
      "Elasticsearch",
      "Knowledge Graph Design",
      "Entity Resolution",
      "Graph-Based Reasoning"
    ],
    "Cloud Platforms - AWS": [
      "AWS Bedrock",
      "AWS SageMaker",
      "AWS EC2",
      "AWS S3",
      "AWS Lambda",
      "AWS RDS",
      "AWS Redshift",
      "AWS Glue",
      "IAM",
      "VPC",
      "Amazon Kinesis"
    ],
    "Cloud Platforms - Azure": [
      "Azure ML Studio",
      "Azure Data Factory",
      "Azure Databricks",
      "Azure Cosmos DB",
      "Azure Synapse"
    ],
    "Big Data & Analytics": [
      "Apache Spark",
      "PySpark",
      "Databricks",
      "Apache Hadoop",
      "Apache Kafka",
      "Apache Flink",
      "Hive",
      "MapReduce",
      "Spark Streaming",
      "Apache Airflow",
      "dbt"
    ],
    "Databases & Data Storage": [
      "Oracle",
      "SQL Server",
      "PostgreSQL",
      "MySQL",
      "Snowflake",
      "MongoDB",
      "Cassandra",
      "Redis",
      "AWS RDS",
      "Google BigQuery",
      "Netezza",
      "Teradata"
    ],
    "ETL/ELT & Data Pipelines": [
      "Apache Airflow",
      "AWS Glue",
      "Azure Data Factory",
      "Informatica",
      "Talend",
      "Apache NiFi",
      "Apache Beam",
      "Informatica PowerCenter",
      "SSIS",
      "Sqoop"
    ],
    "Natural Language Processing": [
      "spaCy",
      "NLTK",
      "Stanford NLP",
      "TF-IDF",
      "LSI",
      "Named Entity Recognition",
      "Sentiment Analysis"
    ],
    "Data Manipulation & Visualization": [
      "Pandas",
      "NumPy",
      "SciPy",
      "Dask",
      "Apache Arrow",
      "matplotlib",
      "Seaborn",
      "Plotly",
      "Bokeh",
      "Tableau",
      "Power BI",
      "D3.js"
    ],
    "Statistical & Analytical Methods": [
      "A/B Testing",
      "ANOVA",
      "Hypothesis Testing",
      "PCA",
      "Regression Analysis",
      "Clustering",
      "Time Series Analysis",
      "Prophet"
    ],
    "Compliance & Security": [
      "CJIS Compliance",
      "PII Handling",
      "Data Masking",
      "HIPAA",
      "GDPR",
      "PCI-DSS",
      "Secure Data Governance",
      "FDA Regulations"
    ],
    "MLOps & Deployment": [
      "MLflow",
      "DVC",
      "Kubeflow",
      "Docker",
      "Kubernetes",
      "Flask",
      "FastAPI",
      "Streamlit",
      "Cloud-Native Observability"
    ],
    "DevOps & CI/CD": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Jenkins",
      "GitHub Actions",
      "Terraform"
    ],
    "Development Tools": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "RStudio",
      "Google Colab",
      "Anaconda"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas.",
      "responsibilities": [
        "Design natural language query interfaces for Insurance claim datasets by connecting AWS Bedrock with Claude models to CAD and RMS systems, enabling adjusters to retrieve policy details through conversational queries while maintaining CJIS compliance.",
        "Build multi-agent orchestration using LangGraph to coordinate document processing, fraud detection, and risk assessment tasks, implementing Model Context Protocol to enable agents to share context and collaborate on complex insurance underwriting workflows.",
        "Develop proof-of-concept solutions demonstrating agentic AI for automated claim validation, testing agent-to-agent communication patterns where one agent extracts data from Oracle databases while another performs semantic analysis using Pinecone vector search.",
        "Configure AWS data ingestion pipelines to extract policy and claim records from on-premises SQL Server instances, applying PII masking during ETL to protect customer information before loading into S3 for downstream analytics and model training.",
        "Establish RAG pipelines that retrieve relevant policy documents and regulatory guidelines from vector databases, feeding context to AWS Titan models to generate accurate responses to complex insurance coverage questions submitted by field agents.",
        "Integrate Databricks with PySpark for distributed processing of large-scale claim histories, computing embeddings for semantic search while optimizing Spark jobs to handle peak loads during monthly reporting cycles without performance degradation.",
        "Test various embedding models to identify optimal representation for insurance domain terminology, running experiments comparing AWS Titan embeddings against open-source alternatives and measuring retrieval accuracy on curated test sets of actual claims.",
        "Tune prompt templates through iterative testing with underwriters, capturing feedback on response clarity and adjusting system prompts to reduce hallucination rates while maintaining compliance with state insurance regulations governing AI-assisted decision-making.",
        "Deploy predictive analytics models using TensorFlow on AWS SageMaker to forecast claim severity, training on historical CAD incident data and deploying endpoints that serve real-time predictions to case management systems via REST APIs.",
        "Troubleshoot latency issues in semantic search by profiling Pinecone query performance, identifying bottlenecks in embedding generation and implementing caching strategies that reduced average query response time from seconds to milliseconds.",
        "Maintain version control for prompt configurations using custom tracking system, documenting changes to query templates and system instructions to ensure reproducibility and enable rollback when new prompts introduce unexpected behavior in production.",
        "Participate in weekly meetings with claims adjusters to gather requirements for new query capabilities, translating business needs into technical specifications for RAG retrieval logic and multi-agent workflows that automate repetitive tasks.",
        "Refine data transformation logic to handle edge cases in policy document formats, writing Python scripts that normalize inconsistent field names and data types before vectorization to improve embedding quality and retrieval relevance.",
        "Monitor AWS CloudWatch metrics for LLM inference endpoints, setting up alarms for elevated error rates or latency spikes and coordinating with infrastructure team to scale EC2 capacity during high-demand periods like catastrophic event responses.",
        "Apply knowledge graph techniques to model relationships between policyholders, agents, properties, and claims, enabling graph traversal queries that surface connected fraud patterns across multiple policies for investigative review.",
        "Review code submissions from junior developers focused on explainability features, ensuring that AI-generated claim summaries include citations to source documents and audit trails that satisfy regulatory requirements for algorithmic transparency."
      ],
      "environment": [
        "Python",
        "AWS Bedrock",
        "Claude AI",
        "AWS Titan",
        "LangGraph",
        "LangChain",
        "Model Context Protocol",
        "Multi-Agent Systems",
        "Agent-to-Agent",
        "Pinecone",
        "Oracle",
        "SQL Server",
        "AWS S3",
        "AWS SageMaker",
        "AWS EC2",
        "Databricks",
        "PySpark",
        "TensorFlow",
        "PyTorch",
        "RAG Pipelines",
        "Semantic Search",
        "CJIS Compliance",
        "PII Masking",
        "AWS Glue",
        "Apache Airflow",
        "Docker",
        "Kubernetes",
        "Git",
        "Jupyter Notebook"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey.",
      "responsibilities": [
        "Constructed retrieval-augmented generation system for pharmaceutical research queries by indexing clinical trial documents into Pinecone, enabling researchers to ask natural language questions about drug efficacy while adhering to HIPAA privacy standards.",
        "Orchestrated multi-agent workflows using LangGraph where specialized agents handled patient record retrieval, adverse event analysis, and regulatory compliance checks, coordinating through Model Context Protocol to deliver comprehensive medical insights.",
        "Prototyped conversational interfaces using LangChain to connect AWS Bedrock models with internal RMS databases containing patient outcomes, allowing clinicians to explore treatment patterns without writing SQL queries or navigating complex data schemas.",
        "Extracted electronic health records from on-premises Oracle systems using custom Python ETL scripts, transforming unstructured clinical notes into structured embeddings suitable for semantic search while masking patient identifiers per HIPAA requirements.",
        "Evaluated LLM response quality by comparing generated medical summaries against physician-reviewed ground truth, calculating precision and recall metrics to identify cases where models hallucinated symptoms or misinterpreted dosage information.",
        "Scheduled batch jobs in Databricks to process daily feeds of lab results and prescription data using PySpark, aggregating statistics across patient cohorts and storing preprocessed features in AWS S3 for consumption by predictive models.",
        "Analyzed adverse drug reaction patterns using PyTorch neural networks trained on historical FDA reporting data, deploying models to AWS SageMaker endpoints that flagged high-risk medication combinations for comprehensive pharmacist review.",
        "Collaborated with medical affairs teams during requirements-gathering sessions, translating clinical workflows into technical designs for AI-powered decision support tools that surfaced relevant research papers based on patient presentation.",
        "Implemented hybrid search combining vector similarity with keyword filters to handle precise medical terminology queries, ensuring that searches for specific drug names or ICD codes returned exact matches alongside semantically related results.",
        "Troubleshot embedding quality issues where medical abbreviations caused poor retrieval performance, expanding preprocessing logic to normalize acronyms and standardize medical terms before vectorization improved search accuracy noticeably.",
        "Optimized RAG retrieval by experimenting with chunk sizes for clinical documents, discovering that smaller chunks improved precision for targeted questions while larger chunks benefited broad literature review queries significantly.",
        "Monitored production AI services using AWS CloudWatch dashboards tracking inference latency and error rates, responding to alerts when API quotas approached limits during peak usage hours by coordinating immediate capacity increases.",
        "Documented prompt engineering best practices in internal wiki after discovering that including example outputs in system prompts reduced hallucination rates for medical fact extraction tasks by significant margins.",
        "Attended cross-functional meetings where data scientists, clinicians, and compliance officers discussed ethical considerations for AI-assisted diagnosis, incorporating feedback into model cards that documented training data sources and known limitations."
      ],
      "environment": [
        "Python",
        "AWS Bedrock",
        "Claude AI",
        "LangGraph",
        "LangChain",
        "Model Context Protocol",
        "Multi-Agent Systems",
        "Pinecone",
        "Oracle",
        "AWS S3",
        "AWS SageMaker",
        "Databricks",
        "PySpark",
        "PyTorch",
        "Keras",
        "RAG Pipelines",
        "HIPAA Compliance",
        "PII Masking",
        "Semantic Search",
        "AWS Glue",
        "Apache Airflow",
        "Docker",
        "Git",
        "Jupyter Notebook",
        "Scikit-Learn"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine.",
      "responsibilities": [
        "Migrated Medicaid enrollment data from legacy on-premises systems to Azure cloud infrastructure, designing ETL pipelines using Azure Data Factory to move sensitive patient information while applying encryption and access controls mandated by state regulations.",
        "Trained TensorFlow models to predict healthcare service utilization patterns across Maine populations, identifying high-risk patients likely to require emergency interventions and enabling proactive care management through early identification.",
        "Developed semantic search capabilities over medical claims databases using custom embedding models deployed on Azure ML Studio, allowing case workers to query benefits eligibility using natural language instead of complex database queries.",
        "Transformed raw FRMS incident logs into analytical datasets using Python pandas, cleaning inconsistent date formats and standardizing location codes before loading into Azure Synapse for reporting dashboards consumed by state health officials.",
        "Validated machine learning model predictions against actual patient outcomes by calculating confusion matrices and ROC curves, presenting findings to clinical stakeholders who adjusted care protocols based on model confidence intervals.",
        "Implemented data quality checks in Apache Airflow workflows that verified completeness and consistency of daily data feeds from provider systems, triggering alerts when anomalies suggested upstream integration failures requiring immediate investigation.",
        "Collaborated with state healthcare administrators during weekly status meetings, explaining technical tradeoffs between model complexity and interpretability for HIPAA-compliant analytics that informed public health policy decisions.",
        "Debugged Azure Databricks cluster failures caused by memory-intensive joins on large claims tables, refactoring Spark SQL queries to use broadcast joins and partitioning strategies that stabilized nightly batch processing jobs.",
        "Configured Azure storage accounts with lifecycle policies to archive historical claims data to cold storage tiers after retention periods expired, reducing costs while maintaining compliance with state record-keeping requirements.",
        "Analyzed prescription drug trends using time series forecasting with Prophet, identifying seasonal patterns in opioid prescriptions that informed targeted intervention programs aimed at reducing substance abuse in rural communities.",
        "Monitored model drift by comparing weekly prediction distributions against baseline metrics established during initial deployment, retraining classifiers when performance degraded beyond acceptable thresholds due to shifting demographics.",
        "Participated in security audits where external consultants reviewed data access logs and encryption configurations, remediating identified vulnerabilities in IAM role assignments that granted excessive permissions to service accounts."
      ],
      "environment": [
        "Python",
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure Databricks",
        "Azure Synapse",
        "TensorFlow",
        "Scikit-Learn",
        "Pandas",
        "NumPy",
        "Apache Airflow",
        "Apache Spark",
        "SQL Server",
        "HIPAA Compliance",
        "PII Masking",
        "Data Quality Validation",
        "Docker",
        "Git",
        "Jupyter Notebook",
        "Power BI"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York.",
      "responsibilities": [
        "Analyzed transaction datasets to detect fraudulent credit card activity using XGBoost classifiers, training models on historical fraud cases and deploying scoring logic that flagged suspicious transactions for manual review by fraud analysts.",
        "Extracted customer interaction logs from Azure SQL databases using Python SQL connectors, aggregating call center transcripts and chat histories to identify common service issues that informed process improvement initiatives.",
        "Built regression models with Scikit-Learn to forecast loan default probabilities based on applicant credit scores and employment history, calibrating thresholds to balance approval rates with risk tolerance specified by lending officers.",
        "Cleaned noisy financial datasets containing duplicate records and missing values using pandas data manipulation functions, documenting transformation logic in Jupyter notebooks that served as reference for audit compliance reviews.",
        "Visualized customer segmentation results using matplotlib and Seaborn, creating cluster plots that highlighted distinct banking behavior patterns which marketing teams used to design targeted product campaigns for different demographic groups.",
        "Configured Azure Data Factory pipelines to ingest daily transaction feeds from ATM networks and mobile banking systems, orchestrating sequential processing steps that culminated in updated risk scores loaded into operational databases.",
        "Participated in model validation exercises where independent teams reviewed training methodologies and tested predictions on holdout datasets, addressing concerns about feature leakage that initially inflated reported accuracy metrics.",
        "Troubleshot discrepancies between development and production model performance by investigating differences in data preprocessing between environments, discovering that missing normalization step caused scoring errors after deployment.",
        "Collaborated with compliance officers to ensure machine learning models met PCI-DSS requirements for cardholder data protection, implementing secure API endpoints that prevented unauthorized access to prediction explanations containing sensitive information.",
        "Monitored deployed models using Azure Application Insights, tracking prediction latencies and error rates while setting up automated alerts that paged on-call engineers when API availability dropped below service level agreements."
      ],
      "environment": [
        "Python",
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure SQL Database",
        "XGBoost",
        "Scikit-Learn",
        "Pandas",
        "NumPy",
        "Matplotlib",
        "Seaborn",
        "Jupyter Notebook",
        "PCI-DSS Compliance",
        "SQL",
        "Git",
        "Docker"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra.",
      "responsibilities": [
        "Loaded large volumes of client data into Hadoop clusters using Sqoop to transfer relational tables from Oracle databases, scheduling incremental imports through cron jobs that synchronized overnight changes for next-day analytics.",
        "Configured Informatica PowerCenter workflows to cleanse and standardize customer records from multiple source systems, mapping disparate field formats into unified schemas that enabled cross-functional reporting for business intelligence teams.",
        "Learned Hive query optimization techniques through trial and error, discovering that partitioning tables by date improved query performance significantly for common filtering patterns used in monthly aggregation reports.",
        "Assisted senior engineers during troubleshooting sessions when MapReduce jobs failed due to data skew, gaining understanding of combiner functions and custom partitioners that distributed workload more evenly across cluster nodes.",
        "Documented ETL processes in Confluence wiki pages describing data lineage from source systems through transformation steps to final warehouse tables, providing reference material for new team members onboarding onto projects.",
        "Attended training sessions on Hadoop ecosystem tools where instructors explained HDFS architecture and YARN resource management, applying classroom concepts to real projects involving terabyte-scale data processing workloads.",
        "Validated data quality by writing SQL queries comparing row counts and checksums between source and target systems, identifying discrepancies that revealed bugs in transformation logic requiring code fixes before production deployment.",
        "Collaborated with offshore teams across time zones using daily standup calls and email threads to coordinate development tasks, learning to communicate technical issues clearly despite language barriers and connectivity challenges."
      ],
      "environment": [
        "Hadoop",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "Oracle",
        "SQL",
        "MapReduce",
        "HDFS",
        "Shell Scripting",
        "Linux",
        "Git"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}