{
  "name": "Yallaiah Onteru",
  "title": "Senior Machine Learning Engineer - AI Modernization & Enterprise Systems",
  "contact": {
    "email": "yonteru.dev.ai@gmail.com",
    "phone": "9733271133",
    "portfolio": "",
    "linkedin": "https://www.linkedin.com/in/yalleshaiengineer/",
    "github": ""
  },
  "professional_summary": [
    "Delivered AI modernization projects across Insurance, Healthcare, Banking, and Consulting sectors with PyTorch, TensorFlow, and Scikit-learn for decisioning platforms that process regulatory compliance workloads under tight deadlines.",
    "Architected Retrieval-Augmented Generation systems using pgvector and Elasticsearch to enable enterprise chatbots that reduced document intelligence processing time through vector database optimization techniques.",
    "Built microservices using FastAPI and Flask for ML APIs that integrated LightGBM and XGBoost models into production environments while maintaining security standards and governance policies across cloud platforms.",
    "Designed feature engineering pipelines with Spark and Databricks that transformed legacy analytics systems into real-time processing frameworks capable of handling time series forecasting and classification tasks at scale.",
    "Implemented agentic AI workflows with Autogen and LangGraph to automate decision-making processes that required multi-agent systems coordination across distributed teams and external vendor integrations.",
    "Deployed neural networks including CNN, RNN, and LSTM architectures on AWS SageMaker for NLP tasks involving semantic search, document parsing, and BERT-based model fine-tuning for domain-specific applications.",
    "Configured MLflow and Kubeflow pipelines for model lifecycle management that tracked experiments, versioned artifacts, and automated deployment workflows through Jenkins and GitHub Actions CI/CD processes.",
    "Orchestrated Docker and Kubernetes infrastructure using Helm charts that containerized ML workloads across AWS environments while ensuring high availability and scalability for production services.",
    "Established Snowflake data warehouses integrated with Airflow DAGs to schedule ETL jobs that prepared training datasets from multiple sources including SQL databases and streaming platforms for model development cycles.",
    "Collaborated with product teams to translate business requirements into technical specifications through design reviews and documentation that communicated AI architecture decisions to executive stakeholders.",
    "Optimized Random Forest and Gradient Boosting models through hypothesis testing and A/B testing methodologies that validated statistical significance of performance improvements before production releases.",
    "Mentored ML engineers on code review practices covering Python development standards, regression analysis techniques, and clustering algorithms while fostering knowledge sharing across distributed engineering teams.",
    "Troubleshot model drift issues in production by analyzing monitoring metrics and implementing retraining strategies that maintained SVM and classification model accuracy thresholds defined in service agreements.",
    "Coordinated PoC evaluations for LLM frameworks and transformer architectures that assessed OpenAI API integration feasibility alongside Azure OpenAI for chatbot capabilities within regulated financial environments.",
    "Supported sprint planning sessions by grooming backlog items related to cloud-native ML architecture migrations that balanced technical debt reduction with new feature delivery timelines.",
    "Validated data quality using Great Expectations framework integrated into feature engineering pipelines that caught schema violations and ensured GDPR compliance for sensitive customer information processing.",
    "Participated in agile ceremonies including retrospectives where team discussed debugging sessions, platform outages, and lessons learned from failed experiments to improve delivery processes continuously.",
    "Documented RESTful API design patterns and authentication protocols including OAuth2 and JWT implementations that secured ML endpoints serving predictions to internal applications and third-party consumers."
  ],
  "technical_skills": {
    "AI/ML Frameworks & Libraries": [
      "PyTorch",
      "TensorFlow",
      "Scikit-Learn",
      "Keras",
      "XGBoost",
      "LightGBM",
      "H2O",
      "AutoML",
      "MLib",
      "Transformers",
      "BERT",
      "Hugging Face"
    ],
    "Generative AI & LLM Technologies": [
      "Retrieval-Augmented Generation (RAG)",
      "LangChain",
      "LangGraph",
      "Llama Index",
      "Autogen",
      "Crew AI",
      "OpenAI API",
      "Claude AI",
      "Vector Databases",
      "pgvector",
      "Elasticsearch",
      "Agentic AI Workflows",
      "Multi-Agent Systems",
      "Model Context Protocol"
    ],
    "Programming Languages": [
      "Python",
      "Java",
      "SQL",
      "Scala",
      "R",
      "Bash/Shell",
      "TypeScript"
    ],
    "Machine Learning Techniques": [
      "Statistical Modeling",
      "Hypothesis Testing",
      "Regression Analysis",
      "Classification",
      "Clustering",
      "Time Series Forecasting",
      "A/B Testing",
      "Random Forest",
      "Gradient Boosting",
      "Support Vector Machines (SVM)",
      "Neural Networks (CNN, RNN, LSTM)",
      "Natural Language Processing",
      "Semantic Search",
      "Document Parsing"
    ],
    "Backend & API Development": [
      "FastAPI",
      "Flask",
      "Django",
      "Microservices Architecture",
      "RESTful APIs",
      "ML APIs",
      "OAuth2",
      "JWT Authentication"
    ],
    "Data Platforms & Big Data": [
      "Apache Spark",
      "PySpark",
      "Databricks",
      "Snowflake",
      "Apache Hadoop",
      "Apache Kafka",
      "Apache Airflow",
      "Apache Flink",
      "Hive",
      "MapReduce",
      "Feature Engineering Pipelines",
      "ETL Pipelines"
    ],
    "Cloud Platforms & Services": [
      "AWS (S3, SageMaker, Lambda, EC2, RDS, Redshift, Bedrock, Glue)",
      "Azure (ML Studio, Data Factory, Databricks, Cosmos DB)",
      "GCP (BigQuery, Vertex AI, Cloud SQL)"
    ],
    "MLOps & Model Lifecycle": [
      "MLflow",
      "Kubeflow",
      "SageMaker",
      "DVC",
      "Model Monitoring",
      "Model Versioning",
      "Model Governance",
      "CI/CD Pipelines",
      "Jenkins",
      "GitHub Actions",
      "GitLab CI"
    ],
    "Containerization & Orchestration": [
      "Docker",
      "Kubernetes",
      "Helm",
      "Container Orchestration",
      "Infrastructure as Code",
      "Terraform",
      "CloudFormation"
    ],
    "Databases & Data Storage": [
      "PostgreSQL",
      "MySQL",
      "Oracle",
      "MongoDB",
      "Cassandra",
      "Redis",
      "Elasticsearch",
      "SQL Server",
      "Snowflake",
      "AWS RDS",
      "BigQuery",
      "Teradata",
      "Netezza"
    ],
    "Data Manipulation & Visualization": [
      "Pandas",
      "NumPy",
      "SciPy",
      "Dask",
      "Matplotlib",
      "Seaborn",
      "Plotly",
      "Tableau",
      "Power BI",
      "D3.js"
    ],
    "NLP & Text Processing": [
      "spaCy",
      "NLTK",
      "Hugging Face Transformers",
      "BERT",
      "GPT Models",
      "TF-IDF",
      "Word Embeddings",
      "Named Entity Recognition",
      "Sentiment Analysis",
      "Text Classification"
    ],
    "DevOps & Version Control": [
      "Git",
      "GitHub",
      "GitLab",
      "Bitbucket",
      "Jenkins",
      "GitHub Actions",
      "Terraform",
      "CI/CD Automation"
    ],
    "Development Tools & IDEs": [
      "Jupyter Notebook",
      "VS Code",
      "PyCharm",
      "Google Colab",
      "Anaconda",
      "RStudio"
    ],
    "Data Governance & Compliance": [
      "GDPR",
      "HIPAA",
      "PCI-DSS",
      "SOC2",
      "Model Risk Management",
      "Data Quality Frameworks",
      "Great Expectations",
      "Model Explainability (SHAP, LIME)"
    ],
    "Message Queues & Streaming": [
      "Apache Kafka",
      "RabbitMQ",
      "Amazon Kinesis",
      "Spark Streaming",
      "Real-time Processing"
    ]
  },
  "experience": [
    {
      "role": "Senior AI Lead Developer",
      "client": "State Farm",
      "duration": "2025-Jan - Present",
      "location": "Austin, Texas",
      "responsibilities": [
        "Plans migration strategy for legacy insurance decisioning platforms using PyTorch and TensorFlow models that require BERT-based transformers integration with existing risk assessment workflows under modernization program.",
        "Implements multi-agent systems with LangGraph and Autogen frameworks coordinating agent-to-agent communication patterns that automate claim processing workflows through Model Context Protocol integration with insurance underwriting APIs.",
        "Deploys RAG pipelines using pgvector on AWS RDS PostgreSQL databases that enable semantic search across policy documents while maintaining compliance with state insurance regulations and data governance standards.",
        "Monitors XGBoost classification models in production through MLflow tracking experiments where initial accuracy dropped during seasonal claim spikes requiring feature engineering adjustments to restore performance thresholds.",
        "Optimizes Databricks Spark clusters running PySpark jobs that process terabytes of customer interaction data for time series forecasting models predicting claim volumes across regional insurance markets.",
        "Troubleshoots Docker container orchestration issues on Kubernetes clusters where memory leaks in FastAPI microservices caused API timeouts during peak traffic requiring code refactoring and resource limit tuning.",
        "Coordinates PoC evaluations comparing Crew AI versus Autogen frameworks for document intelligence automation assessing integration complexity with existing AWS infrastructure and vendor licensing costs.",
        "Collaborates with business stakeholders translating insurance domain requirements into ML architecture diagrams that communicate LightGBM model capabilities for fraud detection use cases during executive review sessions.",
        "Refactors feature engineering pipelines in Airflow DAGs addressing data quality issues discovered during code reviews where missing value handling logic failed for edge cases in policy renewal datasets.",
        "Constructs agentic AI workflows orchestrating multiple LLM calls through LangChain that generate personalized insurance recommendations based on customer profiles retrieved from Snowflake data warehouse queries.",
        "Attends sprint planning meetings grooming backlog items for chatbot enhancements requiring NLP improvements to semantic search accuracy based on customer feedback about irrelevant policy document results.",
        "Validates hypothesis testing results from A/B experiments comparing Gradient Boosting versus Random Forest models for premium pricing optimization ensuring statistical significance before production rollout approvals.",
        "Integrates SageMaker endpoints serving neural network models including CNN architectures for image-based damage assessment automating claim adjudication workflows previously requiring manual photo review processes.",
        "Secures ML APIs implementing OAuth2 authentication protocols and JWT token validation protecting customer data transmission between microservices deployed across AWS Lambda serverless functions.",
        "Documents technical specifications for vector database architecture decisions explaining Elasticsearch versus pgvector trade-offs regarding query latency and storage costs for insurance document retrieval systems.",
        "Debugs model drift alerts investigating root causes where LSTM time series models degraded due to unexpected economic indicators not present in training data requiring emergency retraining procedures."
      ],
      "environment": [
        "PyTorch",
        "TensorFlow",
        "Scikit-Learn",
        "XGBoost",
        "LightGBM",
        "BERT",
        "Transformers",
        "LangGraph",
        "LangChain",
        "Autogen",
        "Crew AI",
        "Multi-Agent Systems",
        "Model Context Protocol",
        "RAG",
        "pgvector",
        "Elasticsearch",
        "FastAPI",
        "Flask",
        "Microservices",
        "PySpark",
        "Databricks",
        "Spark",
        "Snowflake",
        "Airflow",
        "AWS S3",
        "AWS SageMaker",
        "AWS Lambda",
        "AWS RDS",
        "Docker",
        "Kubernetes",
        "Helm",
        "MLflow",
        "Kubeflow",
        "Jenkins",
        "GitHub Actions",
        "Python",
        "Java",
        "SQL",
        "PostgreSQL",
        "OAuth2",
        "JWT",
        "CNN",
        "RNN",
        "LSTM",
        "NLP",
        "Semantic Search",
        "A/B Testing",
        "Random Forest",
        "Gradient Boosting",
        "SVM",
        "Time Series Forecasting",
        "Hypothesis Testing",
        "Feature Engineering",
        "CI/CD"
      ]
    },
    {
      "role": "Senior AI Developer",
      "client": "Johnson & Johnson",
      "duration": "2021-Aug - 2024-Dec",
      "location": "New Brunswick, New Jersey",
      "responsibilities": [
        "Architected HIPAA-compliant RAG systems using Elasticsearch vector search integrated with pharmaceutical research databases that retrieved clinical trial documentation while encrypting patient identifiable information across AWS infrastructure.",
        "Constructed LangChain workflows orchestrating OpenAI API calls for medical document summarization that reduced manual review time for regulatory submissions through automated extraction of adverse event reports.",
        "Trained TensorFlow deep learning models on Azure Databricks clusters processing medical imaging datasets where debugging memory errors required optimizing batch sizes and implementing gradient checkpointing techniques.",
        "Migrated legacy healthcare analytics from on-premise Hadoop clusters to AWS cloud-native architecture using Spark Streaming for real-time patient monitoring data ingestion from hospital IoT devices.",
        "Evaluated Crew AI framework capabilities during PoC phase comparing multi-agent coordination patterns against existing rule-based systems for drug interaction checking workflows requiring FDA compliance validation.",
        "Collaborated with HIPAA compliance team documenting data lineage for ML models serving predictions to clinical decision support tools ensuring audit trails met regulatory reporting requirements.",
        "Optimized Scikit-Learn classification models predicting patient readmission risks through feature selection experiments that improved recall metrics after initial production deployment showed bias toward certain demographics.",
        "Configured MLflow experiment tracking integrated with GitHub Actions CI/CD pipelines automating model retraining workflows when new clinical data batches arrived from hospital electronic health record systems.",
        "Participated in architecture reviews presenting LightGBM model performance trade-offs between prediction latency and accuracy for real-time drug dosage recommendation APIs serving emergency department applications.",
        "Refactored FastAPI microservices handling sensitive patient queries implementing rate limiting and request validation to prevent PHI data leaks discovered during security penetration testing exercises.",
        "Attended cross-functional meetings with pharmaceutical researchers translating biological hypotheses into statistical models while learning domain terminology for protein structure prediction tasks.",
        "Validated XGBoost model explainability using SHAP values ensuring clinical stakeholders understood feature importance rankings before deploying models affecting patient treatment pathways.",
        "Troubleshot Airflow DAG failures where upstream data quality issues from laboratory instruments caused pipeline crashes requiring error handling logic and alerting mechanisms for on-call team notifications.",
        "Integrated PyTorch BERT models fine-tuned on medical literature for semantic search enabling researchers to query drug interaction databases using natural language questions instead of structured query syntax."
      ],
      "environment": [
        "TensorFlow",
        "PyTorch",
        "Scikit-Learn",
        "XGBoost",
        "LightGBM",
        "BERT",
        "Transformers",
        "LangChain",
        "Crew AI",
        "Autogen",
        "RAG",
        "Elasticsearch",
        "Vector Databases",
        "FastAPI",
        "Flask",
        "Microservices",
        "PySpark",
        "Databricks",
        "Spark Streaming",
        "Snowflake",
        "Airflow",
        "AWS S3",
        "AWS Lambda",
        "AWS RDS",
        "AWS Glue",
        "Azure Databricks",
        "Azure Data Factory",
        "Docker",
        "Kubernetes",
        "MLflow",
        "Kubeflow",
        "Jenkins",
        "GitHub Actions",
        "Python",
        "Java",
        "SQL",
        "PostgreSQL",
        "OpenAI API",
        "NLP",
        "Semantic Search",
        "HIPAA",
        "GDPR",
        "Classification",
        "Deep Learning",
        "CNN",
        "RNN",
        "Feature Engineering",
        "Model Explainability",
        "SHAP",
        "CI/CD",
        "OAuth2"
      ]
    },
    {
      "role": "Senior ML Engineer",
      "client": "State of Maine",
      "duration": "2020-Apr - 2021-Jul",
      "location": "Augusta, Maine",
      "responsibilities": [
        "Developed HIPAA-compliant predictive models using Scikit-Learn on Azure ML Studio that forecasted Medicaid enrollment trends for state healthcare planning while encrypting demographic data at rest and in transit.",
        "Implemented Azure Data Factory pipelines ingesting electronic health records from rural clinic systems transforming unstructured clinical notes into structured features for TensorFlow regression models predicting disease prevalence.",
        "Configured PostgreSQL databases with row-level security policies restricting access to patient records based on healthcare provider roles ensuring compliance with state privacy regulations during audit reviews.",
        "Trained XGBoost classification models identifying high-risk patients for preventive care programs where initial model validation revealed data imbalance requiring SMOTE oversampling techniques to improve minority class detection.",
        "Collaborated with public health officials translating epidemiological requirements into statistical analyses using hypothesis testing and A/B testing frameworks to measure intervention effectiveness across county populations.",
        "Deployed Flask REST APIs on Azure App Service exposing ML model predictions to healthcare case management systems while implementing JWT authentication to secure API endpoints from unauthorized access attempts.",
        "Optimized Spark jobs on Azure Databricks processing claims data from state insurance exchanges where memory constraints forced code refactoring to use DataFrame operations instead of RDD transformations.",
        "Participated in code reviews catching data leakage bugs where future information accidentally included in training features caused inflated model accuracy metrics during cross-validation experiments.",
        "Troubleshot Azure pipeline failures investigating timeout errors during large batch predictions requiring optimization of model inference code and database connection pooling strategies.",
        "Documented technical architecture for Random Forest models used in opioid prescription monitoring systems explaining feature importance rankings and decision thresholds to state regulatory committee members.",
        "Attended weekly team meetings discussing challenges integrating legacy mainframe systems with cloud ML infrastructure where initial API latency issues required caching strategies and asynchronous processing implementations.",
        "Validated time series forecasting models predicting seasonal flu outbreak patterns using Prophet library ensuring predictions aligned with historical CDC surveillance data before presenting results to state health department."
      ],
      "environment": [
        "Scikit-Learn",
        "TensorFlow",
        "XGBoost",
        "Random Forest",
        "Gradient Boosting",
        "Flask",
        "Python",
        "SQL",
        "PostgreSQL",
        "Azure ML Studio",
        "Azure Data Factory",
        "Azure Databricks",
        "Azure App Service",
        "PySpark",
        "Spark",
        "Airflow",
        "Docker",
        "MLflow",
        "JWT",
        "HIPAA",
        "GDPR",
        "Classification",
        "Regression",
        "Time Series Forecasting",
        "Hypothesis Testing",
        "A/B Testing",
        "Feature Engineering",
        "Statistical Modeling",
        "NLP",
        "REST APIs",
        "CI/CD",
        "Git"
      ]
    },
    {
      "role": "Data Scientist",
      "client": "Bank of America",
      "duration": "2018-Jan - 2020-Mar",
      "location": "New York, New York",
      "responsibilities": [
        "Built PCI-DSS compliant fraud detection models using XGBoost and Random Forest classifiers on Azure Databricks processing millions of daily transaction records while masking cardholder data during model training cycles.",
        "Developed feature engineering pipelines using PySpark that aggregated customer transaction patterns from SQL Server databases creating time-windowed features for credit risk scoring models deployed to production APIs.",
        "Implemented SVM classification models detecting suspicious account activity patterns where initial false positive rates required threshold tuning and ensemble methods combining multiple algorithms to reduce alert fatigue.",
        "Configured Azure Data Factory ETL workflows extracting transaction data from legacy Teradata systems transforming currency formats and joining customer demographic tables for daily model retraining batches.",
        "Collaborated with risk management teams documenting model validation reports explaining logistic regression coefficients and statistical significance tests for regulatory compliance audits by federal banking examiners.",
        "Deployed Scikit-Learn models as Flask microservices on Azure Kubernetes Service implementing circuit breaker patterns to handle downstream database failures gracefully without impacting customer-facing payment processing systems.",
        "Optimized SQL queries retrieving training data from partitioned tables where initial query timeouts forced index creation and query rewriting reducing data preparation time from hours to minutes.",
        "Participated in sprint retrospectives discussing debugging sessions where production model scores diverged from validation metrics requiring investigation into data distribution shifts between environments.",
        "Validated clustering algorithms using K-Means for customer segmentation ensuring segment definitions aligned with business marketing strategies before launching targeted credit card offer campaigns.",
        "Attended cross-functional meetings with fraud analysts learning domain knowledge about money laundering patterns and structuring schemes that informed feature engineering decisions for anomaly detection models."
      ],
      "environment": [
        "Scikit-Learn",
        "XGBoost",
        "Random Forest",
        "SVM",
        "Logistic Regression",
        "K-Means Clustering",
        "Flask",
        "Python",
        "SQL",
        "SQL Server",
        "Teradata",
        "Azure Databricks",
        "Azure Data Factory",
        "Azure Kubernetes Service",
        "PySpark",
        "Spark",
        "Docker",
        "MLflow",
        "PCI-DSS",
        "Feature Engineering",
        "Statistical Modeling",
        "Classification",
        "Regression",
        "Anomaly Detection",
        "ETL",
        "REST APIs",
        "Git",
        "CI/CD"
      ]
    },
    {
      "role": "Data Engineer",
      "client": "Hexaware",
      "duration": "2015-Oct - 2017-Dec",
      "location": "Mumbai, Maharashtra",
      "responsibilities": [
        "Created Hadoop MapReduce jobs processing client retail sales data extracting daily transaction summaries from HDFS clusters where initial job failures taught importance of handling null values and schema validation.",
        "Configured Informatica PowerCenter workflows ingesting data from Oracle databases using Sqoop connectors that loaded tables into Hive for downstream analytics requiring coordination with DBA teams for access permissions.",
        "Developed Python scripts automating data quality checks validating row counts and duplicate detection across source and target systems generating daily reconciliation reports for client delivery managers.",
        "Participated in client meetings learning business requirements for sales forecasting dashboards translating functional specifications into technical data pipeline designs documented in Confluence wiki pages.",
        "Troubleshot Sqoop import failures investigating JDBC connection timeouts and network issues requiring collaboration with infrastructure teams to adjust firewall rules and database connection pool settings.",
        "Attended training sessions learning Spark fundamentals and practicing DataFrame operations on sample datasets preparing for upcoming project migrations from legacy MapReduce to modern big data frameworks.",
        "Validated ETL job outputs comparing aggregated metrics against business analyst calculations ensuring data accuracy before releasing pipeline changes to production environments.",
        "Supported senior engineers during on-call rotations monitoring Hadoop cluster health and restarting failed jobs while documenting troubleshooting steps in runbook wiki for knowledge sharing."
      ],
      "environment": [
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "Informatica PowerCenter",
        "Oracle",
        "Python",
        "SQL",
        "Spark",
        "ETL",
        "Data Quality",
        "Unix/Linux",
        "Shell Scripting",
        "Git"
      ]
    }
  ],
  "education": [
    {
      "institution": "KITS",
      "degree": "B.Tech",
      "field": "Computer Science",
      "year": "2015"
    }
  ],
  "certifications": []
}