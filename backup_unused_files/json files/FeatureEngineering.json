{
    "questions": [
      {
        "question": "How do you handle missing or corrupted data in a dataset?",
        "answer": "Yeah, so missing data is super common, and how I handle it depends on the situation. First, I'd figure out why it's missing—is it random, or is there a pattern? If it's just a few rows, I might drop them. If it's a whole feature with tons of missing values, I'd consider dropping the feature entirely. For numerical data, I'd use imputation—like filling with the median or mean, or using more advanced methods like KNN imputation. For categorical data, I'd create a new category like 'Unknown' or use mode imputation. One thing I learned at State Farm—sometimes the fact that data is missing is actually informative, so I'd create a binary flag like 'was_missing' as a feature. For corrupted data, I'd clean it using validation rules or remove outliers. Honestly, the key is understanding the business context before deciding."
      },
      {
        "question": "How would you handle an imbalanced dataset?",
        "answer": "Yeah, so imbalanced data is something I deal with all the time, especially in fraud detection. First, I'd never just ignore it—accuracy is misleading when 99% of your data is one class. I'd use metrics like precision, recall, F1-score, or AUC instead. For fixing it, I'd try a few things—oversampling the minority class using SMOTE, undersampling the majority class, or a combination of both. I'd also use class weights in the model—like in XGBoost or scikit-learn, you can set 'scale_pos_weight' to penalize misclassifying the minority class more. One thing I learned—sometimes collecting more minority examples is the best solution if possible. I'd also try anomaly detection models if the imbalance is extreme. Honestly, SMOTE plus class weighting works great most of the time."
      },
      {
        "question": "Can you explain the concept of 'feature selection' in machine learning?",
        "answer": "Yeah, so feature selection is about picking the most important features and dropping the irrelevant or redundant ones. It helps reduce overfitting, speeds up training, and makes models more interpretable. There are a few ways to do it—filter methods use statistical tests like correlation or chi-square to rank features. Wrapper methods like recursive feature elimination train the model multiple times and remove the least important features. Embedded methods like Lasso regression do feature selection during training. One thing I always do—check feature importance from tree-based models like XGBoost or Random Forest. At State Farm, we had datasets with hundreds of features, and dropping the bottom 50% often improved performance. Honestly, fewer features usually means a better, faster model."
      },
      {
        "question": "How do you handle categorical variables in your dataset?",
        "answer": "Yeah, so categorical variables need to be encoded because most models can't handle raw strings. For binary categories like 'yes/no,' I'd use label encoding—just map them to 0 and 1. For nominal categories with no order—like 'red, blue, green'—I'd use one-hot encoding to create separate binary columns. But if there are tons of categories, one-hot explodes the feature space, so I'd use target encoding—replace each category with the mean of the target variable for that category. One thing I learned—be careful with target encoding because it can leak information, so I'd do it inside cross-validation. For tree-based models, sometimes label encoding works fine. For high-cardinality features, I'd use embeddings or feature hashing. Honestly, the right approach depends on the model and the number of categories."
      },
      {
        "question": "How do filtering and wrapper methods work in feature selection?",
        "answer": "Yeah, so filtering methods are fast and model-agnostic—they rank features using statistical tests before training. Like, I'd use correlation to drop features that are highly correlated with each other, or chi-square tests to see which features are most associated with the target. Filtering is cheap but doesn't account for feature interactions. Wrapper methods actually train the model multiple times—like recursive feature elimination (RFE), which starts with all features, trains the model, removes the least important one, and repeats. It's slower but more accurate because it considers how features work together. One thing I do—start with filtering to quickly drop obviously bad features, then use wrappers if I need to optimize further. Honestly, filtering is my go-to for large datasets because wrapper methods can take forever."
      },
      {
        "question": "Describe a situation where you had to handle missing data. What techniques did you use?",
        "answer": "Yeah, so at State Farm, I worked on a claims dataset where some fields like 'repair cost' were missing for about 15% of records. First, I analyzed the missingness pattern—it wasn't random; it was missing when claims were still open. So I created a binary flag 'is_claim_open' to capture that info. For the missing costs, I used median imputation as a baseline, but then I tried KNN imputation, which filled values based on similar records—that worked way better. I also tried MICE (multiple imputation), but it was overkill for this use case. One thing I learned—don't just blindly impute; understand why it's missing. Sometimes the missingness itself is the signal. Honestly, the flag feature ended up being one of the top predictors."
      },
      {
        "question": "What is principal component analysis (PCA) and when is it used?",
        "answer": "Yeah, so PCA is a dimensionality reduction technique that transforms your features into a smaller set of uncorrelated components. It finds the directions of maximum variance in your data and projects the data onto those directions. I use it when I have tons of features and want to compress them—like going from 100 features to 20 components. It's great for visualization too, because you can project data down to 2D or 3D. One thing I learned—PCA works best when features are scaled, so I always standardize first. It's also useful for removing multicollinearity. At State Farm, I used it to reduce feature space before training, which sped up models. Honestly, PCA is awesome for high-dimensional data, but you lose interpretability because components are combinations of original features."
      },
      {
        "question": "What's the difference between PCA vs ICA?",
        "answer": "Yeah, so PCA and ICA both reduce dimensions, but they work differently. PCA finds directions of maximum variance and assumes the components are uncorrelated—it's about capturing the most information. ICA finds independent components and assumes the original signals are statistically independent—it's about separating mixed signals. Like, PCA is great for compression, but ICA is better for blind source separation—like separating mixed audio tracks or EEG signals. One thing I've noticed—PCA is way more common in ML because it's simpler and works for most use cases. I've only used ICA once for a signal processing task. Honestly, unless you're dealing with mixed signals, PCA is usually the better choice."
      },
      {
        "question": "How do you handle time-based features in a machine learning model?",
        "answer": "Yeah, so time-based features are super important for things like forecasting or fraud detection. First, I'd extract useful components from timestamps—like hour of day, day of week, month, or whether it's a weekend. I'd also create cyclical features using sine and cosine transformations, so the model understands that hour 23 and hour 0 are close. For sequential data, I'd use lag features—like the value from 1 day ago or 7 days ago—or rolling averages over different windows. One thing I learned—always respect the time order during train-test splits. Never shuffle time-series data randomly, or you'll leak future info into training. At State Farm, I used time-based features for claims prediction, and they were some of the strongest predictors. Honestly, feature engineering on time is where you can get huge wins."
      },
      {
        "question": "What is feature hashing? When would you use it?",
        "answer": "Yeah, so feature hashing (or the hashing trick) is when you use a hash function to map features into a fixed-size feature space. It's useful for high-cardinality categorical features—like user IDs or product SKUs—where one-hot encoding would explode the feature space. Instead of creating millions of columns, you hash them into, say, 1,000 buckets. The downside is collisions—multiple features might hash to the same bucket, which can hurt performance. But it's fast and memory-efficient. I'd use it when dealing with text data or huge categorical features in streaming systems. One thing I learned—you can't reverse the hash, so you lose interpretability. Honestly, feature hashing is great for scaling ML systems when you can't afford to store all features explicitly."
      },
      {
        "question": "How do you handle hierarchical categorical variables?",
        "answer": "Yeah, so hierarchical categoricals are like 'Country > State > City'—there's a natural structure. One way to handle them is to create features at each level—like separate columns for country, state, and city. Another approach is to use target encoding at each level, so the model learns the impact of each hierarchy. For tree-based models, I'd just encode each level separately using label encoding, and the model will figure out the hierarchy. One thing I've tried—embedding layers for hierarchical data, where the network learns a dense representation that captures the structure. At State Farm, we had policy hierarchies, and encoding each level worked great. Honestly, the key is not to lose the structure by flattening everything into one giant feature."
      },
      {
        "question": "What are embedding layers and when should you use them?",
        "answer": "Yeah, so embedding layers learn dense, low-dimensional representations of categorical features. Instead of one-hot encoding—which creates sparse, high-dimensional vectors—embeddings map each category to a learned vector of, say, 10-50 dimensions. They're super useful for high-cardinality features like user IDs, product IDs, or words in NLP. I use them a lot in deep learning models—like in neural nets or when working with tabular data. One thing I learned—embeddings capture similarity, so similar categories end up with similar vectors. At State Farm, I used embeddings for customer IDs in a recommendation system, and it worked way better than one-hot encoding. Honestly, if you have more than 50 categories, embeddings are almost always better."
      },
      {
        "question": "Explain different strategies for handling outliers in different ML algorithms",
        "answer": "Yeah, so handling outliers depends on the model. For linear models like regression, outliers can mess up the coefficients, so I'd use robust scaling or clip extreme values using IQR or z-scores. Sometimes I'd just drop them if they're clearly errors. For tree-based models like XGBoost or Random Forest, outliers aren't a big deal because trees split on thresholds—they naturally handle them. For neural networks, I'd use batch normalization or robust scaling to reduce their impact. One thing I learned—don't blindly remove outliers; sometimes they're the most interesting data points, like fraud cases. At State Farm, we kept outliers in fraud detection because they were the signal. Honestly, I'd always visualize outliers first to understand if they're errors or real."
      }
    ]
  }