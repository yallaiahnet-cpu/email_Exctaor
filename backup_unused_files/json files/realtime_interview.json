{
    "questions": [
      {
        "question": "Walk me through your end-to-end architecture for the fraud detection and underwriting AI system.",
        "answer": "In production, our architecture starts with API Gateway receiving claim submissions. These flow into Kinesis Data Streams for real-time processing. We have three parallel paths. First, the fraud detection path uses a SageMaker endpoint hosting an XGBoost model that scores claims in real-time, with results written to DynamoDB. Second, the underwriting path uses Lambda functions that invoke Bedrock Claude for document analysis and policy matching. Third, the RAG pipeline uses OpenSearch for vector storage with metadata in DynamoDB for faster filtering. All streams converge in Step Functions that orchestrate the workflow. We chose this architecture because it separates concerns, allows independent scaling of each component, and provides sub-second latency for fraud scoring while handling complex document analysis asynchronously. CloudWatch monitors everything with alarms triggering SNS notifications. The key design decision was using Kinesis for decoupling producers and consumers, which gave us the flexibility to add new fraud models without impacting the main flow."
      },
      {
        "question": "Why did you choose DynamoDB for vector metadata instead of storing everything in OpenSearch?",
        "answer": "This was a deliberate cost and performance optimization. In our production system, we store 5 million policy documents. If we put all metadata in OpenSearch, our cluster cost was 12,000 dollars per month with 10 data nodes. By moving metadata like policy number, customer ID, effective dates, and access control flags to DynamoDB, we reduced the OpenSearch cluster to 4 nodes, cutting costs to 4,500 dollars monthly. More importantly, DynamoDB gives us single-digit millisecond latency for metadata lookups and exact-match filters. When a user queries their policy, we first check DynamoDB to validate access permissions and get the document IDs, then query OpenSearch only for the semantic search. This hybrid approach reduced our P99 latency from 800 milliseconds to 250 milliseconds. We also get automatic backups and point-in-time recovery with DynamoDB, which is critical for audit compliance. The tradeoff is managing two datastores, but the operational benefits far outweigh the complexity."
      },
      {
        "question": "How did you handle back-pressure when SageMaker endpoints could not keep up with Kinesis stream throughput?",
        "answer": "We faced this exact issue during open enrollment when claim volume spiked from 500 to 3,000 requests per second. Our SageMaker endpoint with 4 instances could only handle 800 requests per second, causing Lambda functions to time out. We implemented a three-pronged solution. First, we added a Kinesis Data Firehose buffer that batches records into S3 when the endpoint is overwhelmed, then processes them asynchronously. Second, we configured Lambda with reserved concurrency and exponential backoff retries with a dead-letter queue for failed invocations. Third, we implemented auto-scaling for the SageMaker endpoint based on CloudWatch metrics, specifically ApproximateAgeOfOldestRecord in Kinesis. When this metric exceeded 60 seconds, we scaled from 4 to 12 instances within 5 minutes. We also added CloudWatch alarms that page the on-call engineer if the DLQ depth exceeds 100 messages. This architecture handles burst traffic gracefully and ensures no claim is lost, though some experience delayed processing, which is acceptable per our SLA of 5-minute P95 latency for non-critical claims."
      },
      {
        "question": "What criteria did you use to choose between Bedrock foundation models and custom SageMaker models?",
        "answer": "We use both, but the decision depends on three factors: task complexity, data sensitivity, and cost. For fraud detection, we use a custom XGBoost model on SageMaker because we need precise control over features, thresholds, and we have 3 years of labeled fraud data with 2 million examples. The model is domain-specific with features like claim velocity, provider network patterns, and historical red flags that a foundation model would not capture. For underwriting document analysis, we use Bedrock Claude Sonnet because the task requires natural language understanding of policy documents, adjuster notes, and medical records. We do not have enough labeled data to train a custom LLM, and Bedrock gives us access to a 200,000 token context window without managing infrastructure. Cost-wise, for fraud scoring at 50,000 predictions per day, SageMaker costs 180 dollars monthly for a single ml.m5.xlarge instance, whereas Bedrock would cost 2,500 dollars monthly at 500 input and 200 output tokens per request. For document summarization at 2,000 requests daily, Bedrock costs 800 dollars monthly, far cheaper than training and hosting a custom LLM. We also consider latency requirements. SageMaker gives us P99 latency under 50 milliseconds for fraud, whereas Bedrock averages 2 to 3 seconds for generative tasks, which is acceptable for non-real-time workflows."
      },
      {
        "question": "Describe the full data flow from claim submission to RAG response, which AWS services were used and why?",
        "answer": "Let me walk through a real claim scenario. When a user submits a claim through our mobile app, the request hits API Gateway with authentication via Cognito. API Gateway triggers a Lambda function that validates the payload and writes to two places: DynamoDB for the claim record and Kinesis Data Streams for real-time processing. A consumer Lambda function reads from Kinesis and invokes our fraud detection SageMaker endpoint. If the fraud score is above 0.7, we write to a high-priority SQS queue for immediate review. For normal claims, we trigger Step Functions to orchestrate the underwriting workflow. Step Functions first calls Lambda to extract documents from S3 where claim attachments are stored. These documents go through Textract for OCR if they are images, then we chunk the text using a Lambda function with LangChain. Each chunk gets embedded using Bedrock Titan Embeddings, and we store vectors in OpenSearch with metadata in DynamoDB. When an adjuster asks a question like what are the policy exclusions for this claim, API Gateway invokes a Lambda that embeds the query, searches OpenSearch for top 10 relevant chunks, retrieves full metadata from DynamoDB, then sends everything to Bedrock Claude with a prompt template. Claude generates the response, which we log to CloudWatch and return via API Gateway. We chose this architecture because Lambda scales automatically, Kinesis decouples producers and consumers, Step Functions provides visual workflow management, OpenSearch handles semantic search efficiently, and Bedrock eliminates the need to manage LLM infrastructure. Every component publishes metrics to CloudWatch, and we use X-Ray for distributed tracing across the entire flow."
      },
      {
        "question": "How do you integrate your RAG pipeline with real-time Kinesis streams for fraud detection?",
        "answer": "This is an interesting integration challenge. Our fraud detection runs in real-time on Kinesis streams, scoring claims within milliseconds. Meanwhile, RAG is typically batch-oriented for document analysis. We integrated them by creating a feedback loop. When fraud detection flags a claim as suspicious with a score above 0.8, we write an event to a separate Kinesis stream called fraud-investigation-stream. A consumer Lambda picks up these events and triggers our RAG pipeline to automatically pull all related documents: past claims from this provider, similar claim patterns from OpenSearch, policy terms around fraud indicators, and external data like provider sanction lists. The RAG system generates a fraud investigation summary using Bedrock Claude, which includes retrieved evidence and reasoning. This summary gets written back to DynamoDB with a reference to the original claim ID. The fraud analyst dashboard then displays both the real-time fraud score and the RAG-generated investigation report side-by-side. The key architectural decision was using separate Kinesis streams for different priority levels. High-priority fraud cases get dedicated throughput, while routine RAG queries go through a lower-priority stream with batch processing. We also implemented a circuit breaker: if Bedrock latency exceeds 10 seconds, we skip RAG and rely solely on the fraud model, ensuring the critical real-time path never blocks. This hybrid approach reduced false positives by 23 percent because analysts have richer context, while maintaining sub-second fraud detection latency."
      },
      {
        "question": "When do you prefer SageMaker endpoints versus Lambda versus EKS for serving your RAG APIs?",
        "answer": "We use all three, and the choice depends on latency requirements, request volume, and model size. For fraud detection, we use SageMaker endpoints because we need consistent sub-50 millisecond P99 latency for XGBoost inference, and we handle 50,000 requests daily. SageMaker endpoints keep the model warm in memory, eliminating cold starts. We use auto-scaling with a minimum of 2 instances for high availability. For the RAG retrieval logic, we use Lambda because the code is lightweight, just embedding queries and searching OpenSearch. Lambda scales automatically from 0 to 1,000 concurrent executions without managing infrastructure, and we only pay for actual invocations. Lambda cold starts are 800 milliseconds, but 90 percent of requests hit warm containers with 100 millisecond latency, which is acceptable for non-real-time queries. For the embedding model, we initially used a SageMaker endpoint, but the cost was 180 dollars monthly for a model that only serves 5,000 requests per day. We switched to Lambda with the model packaged in a container image, reducing cost to 25 dollars monthly. Now here is where EKS comes in. We explored hosting Bedrock-style workloads on EKS using open-source LLMs like Llama, but the operational overhead was significant: managing Kubernetes clusters, GPU node pools, Horizontal Pod Autoscalers, and model serving frameworks like TorchServe. For our team size of 4 engineers, Bedrock made more sense. However, if we were serving 10 million requests daily or needed sub-500 millisecond latency for LLM inference, EKS with GPU instances and aggressive caching would be worth it. In summary: SageMaker for production models needing low latency, Lambda for lightweight logic and variable workloads, EKS when you need maximum control and scale, but can afford the operational complexity."
      },
      {
        "question": "How do you architect multi-model endpoints across multiple services for fraud, underwriting, pricing, and NLP?",
        "answer": "We run four distinct models in production: fraud detection XGBoost, claim severity regression, policy pricing LightGBM, and NLP classification for document routing. Rather than deploying four separate SageMaker endpoints, which would cost 720 dollars monthly for 4 ml.m5.xlarge instances, we consolidated using SageMaker multi-model endpoints. Here is how it works. All four models are stored in S3 under separate prefixes: fraud-model slash version-3, severity-model slash version-2, and so on. We deploy a single multi-model endpoint that dynamically loads models into memory on-demand. When a request comes in, we pass a target-model header specifying which model to invoke. SageMaker loads the model if it is not in memory, caches it, and evicts the least-recently-used model if memory is full. This reduced our hosting cost to 180 dollars monthly for 1 instance, saving 75 percent. However, we learned some hard lessons. First, cold start latency when loading a model is 2 to 5 seconds, unacceptable for fraud detection. We solved this by pinning the fraud model to memory using a CloudWatch Events rule that invokes it every 5 minutes, keeping it warm. Second, we hit memory limits when all models were loaded simultaneously. We addressed this by splitting high-traffic fraud and underwriting models to a dedicated endpoint, leaving pricing and NLP on the multi-model endpoint. Third, monitoring became complex because CloudWatch metrics are aggregated. We added custom logging in Lambda to track per-model invocation counts, latencies, and errors, then built CloudWatch dashboards to visualize each model separately. The architecture now balances cost and performance: critical models on dedicated endpoints, low-traffic models on multi-model endpoints, with autoscaling configured per service based on traffic patterns."
      },
      {
        "question": "What is your strategy for multi-region model deployment and disaster recovery?",
        "answer": "We deploy our fraud detection and RAG systems in two regions: us-east-1 as primary and us-west-2 as failover. Here is the architecture. We use S3 Cross-Region Replication to sync model artifacts, training data, and policy documents to the DR region every 15 minutes. DynamoDB Global Tables replicate claim metadata and user profiles with sub-second latency. For OpenSearch, we take automated snapshots every 6 hours to S3, which replicates to us-west-2. In a disaster scenario, we restore the snapshot to a new OpenSearch domain in the DR region. This gives us a Recovery Point Objective of 6 hours for document vectors. For SageMaker endpoints, we use CloudFormation StackSets to deploy identical infrastructure in both regions. The tricky part is keeping models in sync. We have a CI/CD pipeline in CodePipeline that, after training a new model in us-east-1, automatically uploads artifacts to S3, triggers replication, then deploys to us-west-2 SageMaker endpoints. We validate both endpoints return identical predictions on a test dataset before switching traffic. For failover, we use Route 53 health checks that monitor the us-east-1 API Gateway endpoint every 30 seconds. If two consecutive checks fail, Route 53 automatically routes traffic to us-west-2. We tested this during a simulated outage: failover took 90 seconds, and we maintained 99.9 percent uptime. The biggest challenge was Bedrock, which is region-specific. We use Bedrock in both regions but accept that if us-east-1 fails, we lose any in-flight Bedrock requests. We mitigate this with retry logic in Lambda using exponential backoff. Our RTO is 2 minutes for failover, RPO is 6 hours for vectors and 15 minutes for documents. Cost is significant: running duplicate infrastructure costs an additional 40 percent, but for a production fraud detection system processing millions in claims, the business justified it."
      },
      {
        "question": "How did you ensure scalability while minimizing SageMaker costs?",
        "answer": "Balancing scalability and cost required several optimizations. Initially, our fraud detection endpoint ran 24/7 on 4 ml.m5.xlarge instances costing 720 dollars monthly, but traffic varied wildly: 2,000 requests per hour during business hours, only 200 at night. We implemented target tracking auto-scaling based on SageMakerVariantInvocationsPerInstance, scaling from 2 to 8 instances dynamically. This reduced cost to 450 dollars by rightsizing during low traffic. Next, we analyzed our model: the fraud XGBoost model was 200 MB serialized, but we were using ml.m5.xlarge with 16 GB memory, massive overkill. We downgraded to ml.m5.large with 8 GB, cutting cost to 225 dollars. We also optimized the model itself by removing low-importance features, reducing model size to 80 MB and inference time from 35 milliseconds to 18 milliseconds, allowing us to handle more requests per instance. For the embedding model, we switched from SageMaker to Lambda, saving 150 dollars monthly. For Bedrock, we implemented aggressive prompt compression: average input tokens dropped from 3,500 to 1,200 by summarizing retrieved documents before sending to Claude, cutting token costs by 65 percent. We also introduced smart routing: simple queries go to Claude Haiku costing 0.25 dollars per million tokens, complex queries to Sonnet at 3 dollars per million tokens. This reduced Bedrock spend from 2,000 dollars to 800 dollars monthly. For OpenSearch, we moved cold data older than 90 days to S3, reducing our cluster from 10 to 4 nodes, saving 7,500 dollars annually. Finally, we implemented response caching in ElastiCache: 30 percent of RAG queries are identical or near-identical, so we cache responses for 1 hour, reducing Bedrock invocations by 30 percent. Total savings: 60 percent cost reduction while supporting 3 times the traffic."
      },
      {
        "question": "Why did you choose LSTMs and Transformers for reserve estimation instead of ARIMA or Prophet?",
        "answer": "We evaluated ARIMA, Prophet, and deep learning models for predicting claim reserves, which is critical because inaccurate reserves impact balance sheets and regulatory filings. ARIMA and Prophet are great for univariate time series with clear seasonality, but claim reserves depend on multiple variables: claim type, injury severity, state regulations, legal fees, medical inflation, and settlement patterns. ARIMA struggled with multivariate dependencies and non-stationary data. Prophet worked better but could not capture complex interactions between features like how legal representation affects settlement amounts differently for auto versus workers comp claims. We built an LSTM model using TensorFlow on SageMaker. The input sequence includes 24 months of historical claim development patterns, payment velocities, and external features like medical CPI. The LSTM learns temporal dependencies, like how claims with legal involvement settle slower. We achieved 12 percent MAPE on test data versus 22 percent with Prophet. However, LSTMs were black boxes, which actuaries disliked. We added SHAP values to explain predictions, showing which features drove each reserve estimate. For very long-tail claims like asbestos, we experimented with Transformers using the Temporal Fusion Transformer architecture. Transformers use attention mechanisms to focus on relevant historical periods and handle variable-length sequences better than LSTMs. We trained on 10 years of claims data with features like exposure type, jurisdiction, and defense costs. TFT improved MAPE to 9 percent and provided interpretable attention weights, showing which past quarters influenced the prediction. The downside is training time: 6 hours for LSTM versus 18 hours for TFT on ml.p3.2xlarge instances. We deploy LSTM for routine reserve updates and TFT for annual reserve reviews. Both models retrain monthly with fresh data to adapt to inflation and regulatory changes."
      },
      {
        "question": "How did you address class imbalance in fraud detection and how did you validate synthetic data from GANs?",
        "answer": "Class imbalance was brutal: 98.5 percent of claims are legitimate, only 1.5 percent are fraud. Training on raw data, our XGBoost model achieved 99 percent accuracy but detected zero fraud cases, completely useless. We tried several techniques. First, we used SMOTE to oversample the minority class, but synthetic samples were too similar to originals, and the model overfit, achieving 95 percent recall in training but 40 percent in production. Next, we adjusted class weights in XGBoost, setting scale_pos_weight to 65, the ratio of negatives to positives. This helped, getting 75 percent recall, but precision dropped to 12 percent, flooding analysts with false positives. The breakthrough was using Conditional GANs to generate realistic synthetic fraud cases. We trained a GAN on fraud cases with conditional labels for fraud type: billing fraud, staged accidents, exaggerated injuries. The generator learned to create realistic feature distributions. We augmented our training set from 15,000 fraud cases to 100,000 synthetic cases. Here is how we validated the synthetic data. First, we compared feature distributions: real versus synthetic using Kolmogorov-Smirnov tests, ensuring distributions matched for key features like claim amount, provider network, and claim velocity. Second, we trained two models: one on real data only, one on real plus synthetic. We evaluated both on a held-out test set of real data. The synthetic-augmented model improved recall from 75 to 88 percent while maintaining 85 percent precision. Third, we had fraud analysts manually review 500 synthetic cases to check realism: 92 percent were deemed plausible. Fourth, we tracked production metrics: false positive rate stayed under 15 percent, and we caught 200 additional fraud cases in 6 months worth 1.2 million dollars. The key lesson: do not blindly trust synthetic data. Rigorous validation on real test data and continuous monitoring in production are essential."
      },
      {
        "question": "Did you model claim frequency and severity jointly or separately and how did you handle zero-inflation?",
        "answer": "We model frequency and severity separately, a standard approach in actuarial science, but we had to address zero-inflation carefully. For claim frequency, we predict how many claims a policyholder will file. The challenge is zero-inflation: 85 percent of policyholders file zero claims in a year, far more than a Poisson distribution predicts. We use a Zero-Inflated Poisson model implemented in R using the pscl package. The model has two components: a logit model predicting the probability of being a \"zero claims\" policyholder based on features like age, vehicle type, and driving history, and a Poisson model for the claim count among non-zero policyholders. We trained on 500,000 policyholders with 3 years of history. The ZIP model captured the excess zeros and predicted frequency with 18 percent MAPE versus 35 percent with vanilla Poisson. For severity, we predict the average cost per claim. Severity is continuous and right-skewed: most claims are small fender benders, but some are catastrophic. We tried Gamma regression, which assumes a Gamma distribution, but it underestimated tail risk. We switched to Tweedie regression, which handles zero-inflated continuous data better. We used H2O on SageMaker to train a Gradient Boosting Machine with Tweedie loss. Features include claim type, injury severity, state, legal representation, and vehicle damage. The Tweedie model achieved 22 percent MAPE on severity. Why separate models? Joint models assume dependence between frequency and severity, but in insurance, they are often independent. A young driver may have high frequency due to inexperience but low severity because they drive cautiously. We tested a joint Bayesian model but saw no improvement, and it was much harder to explain to actuaries. We combine predictions: expected loss equals frequency times severity. This approach is interpretable, aligns with actuarial standards, and performs well. For pricing, we aggregate across policyholders to set premiums."
      },
      {
        "question": "How did you optimize SQL queries in Redshift and Snowflake for underwriter analysis?",
        "answer": "Underwriters run ad-hoc queries on billions of rows for risk analysis, and poorly optimized queries timed out or cost a fortune. Here is what we did. In Redshift, we started with distribution and sort keys. Our claims table has 500 million rows. We set the distribution key to policy_id because most queries filter or join on policy. This co-locates related claims on the same node, avoiding expensive network shuffles. We set the sort key to claim_date because queries often filter by date ranges. This reduced query time for monthly aggregations from 90 seconds to 12 seconds. We also created materialized views for common aggregations. Underwriters frequently query total incurred loss by coverage type and state. We pre-computed this in a materialized view that refreshes nightly, reducing query time from 45 seconds to 2 seconds. For Snowflake, we leveraged clustering keys and search optimization. We clustered the policy table on policy_effective_date and state, which Snowflake uses to prune partitions. Queries like show me all policies in California issued in Q1 2024 went from 30 seconds to 4 seconds. We enabled search optimization for high-cardinality columns like policy_number and claim_id, which are often in WHERE clauses. This created internal indexes, reducing point lookups from 8 seconds to under 1 second. We also eliminated SELECT star queries. Analysts were pulling all 80 columns when they needed only 5, wasting I/O. We rewrote queries to select specific columns and saw a 40 percent reduction in compute costs. For complex joins, we used Common Table Expressions to break queries into logical steps, making them easier to optimize and debug. We also set up query monitoring: Redshift WLM queues prioritize underwriter queries over batch ETL, and Snowflake resource monitors alert when a query exceeds 100 compute credits. One big win was moving heavy aggregations to dbt models that run overnight, so underwriters query pre-aggregated tables instead of raw data. These optimizations reduced average query time from 60 seconds to 8 seconds and cut Snowflake costs by 35 percent."
      },
      {
        "question": "How did you ensure reproducibility of experiments using MLflow on SageMaker?",
        "answer": "Reproducibility is critical for audits and debugging. We use MLflow hosted on an EC2 instance with a PostgreSQL backend for metadata and S3 for artifacts. Here is our workflow. Every training job starts with a unique experiment name like fraud-detection-v3 and a run ID. We log hyperparameters, data versions, code commits, and random seeds. For data versioning, we use DVC to track datasets in S3. Each dataset has a hash, like claims-data-v2.3-abc123, which we log to MLflow. This ensures we know exactly which data trained each model. We log every hyperparameter: learning rate, max depth, num_boost_rounds for XGBoost. We also log the Docker image used for training, which includes all dependencies. This is crucial because library versions can affect results. For example, XGBoost 1.3 and 1.5 produce different models with identical parameters. We log training and validation metrics at each epoch: accuracy, precision, recall, AUC. MLflow generates plots showing metric evolution over time, which helps diagnose overfitting. We save the trained model artifact to S3 via MLflow, along with a signature that specifies input and output schemas. We also log the SageMaker training job ARN and the Git commit SHA of the training code. This creates a full audit trail from code to data to model. For deployment, we retrieve the model from MLflow by run ID, ensuring the exact model version is deployed. We tested reproducibility by rerunning a training job from 6 months ago using the logged parameters, data version, and Docker image. The model metrics matched to 4 decimal places. We also use MLflow's model registry to manage production models. Models are tagged as staging or production, and we require two approvals before promoting to production. Every prediction in production includes the model version in the response, and we log predictions to S3 for audit. If a regulator asks how we predicted a specific claim, we trace back through MLflow to the exact training run, data, and code. This setup has been a lifesaver during audits and debugging. The overhead is minimal: logging adds 2 seconds to training time, well worth it for the guarantees."
      },
      {
        "question": "How did you monitor feature drift and retraining triggers in production?",
        "answer": "Feature drift is insidious: model performance degrades silently over time. We monitor it aggressively using SageMaker Model Monitor and custom CloudWatch metrics. Here is the setup. We capture 100 percent of prediction requests and responses using SageMaker Data Capture. Captured data goes to S3, partitioned by date. We run Model Monitor jobs daily that compare production feature distributions to the baseline from training. For numerical features like claim_amount, we compute the Kolmogorov-Smirnov statistic. If the KS statistic exceeds 0.15, we flag drift. For categorical features like claim_type, we use the Chi-Square test. Model Monitor generates reports showing which features drifted and by how much. We set up CloudWatch alarms that trigger if more than 3 features drift simultaneously. For example, during COVID, claim_frequency dropped 40 percent because people drove less, causing feature drift. Our model overestimated risk, and we retrained with recent data. We also monitor prediction distribution. If the average fraud score shifts from 0.15 to 0.30, something is wrong. We log predictions to S3 and run weekly analysis comparing distributions. We use Jensen-Shannon divergence to quantify drift: values above 0.2 trigger investigation. For ground truth, we collect labels asynchronously. Fraud labels arrive 30 to 90 days after the claim is flagged, when investigators complete reviews. We join predictions with labels in Redshift and compute rolling 30-day accuracy, precision, and recall. If accuracy drops below 80 percent or recall below 75 percent, we trigger retraining. We built a custom Lambda that runs weekly, queries Redshift for performance metrics, and publishes to CloudWatch. Alarms notify the ML team via SNS if thresholds are breached. For retraining triggers, we use Step Functions. If drift or performance degradation is detected, Step Functions orchestrates: data extraction from Redshift, feature engineering in Glue, training on SageMaker, validation, and model registration in MLflow. We require manual approval before deploying to production. This automated monitoring caught a critical issue: a new claim type, electric vehicle fires, was not in training data, causing the model to misclassify them. We retrained with new labels and restored performance. Without monitoring, we would have missed it for months."
      },
      {
        "question": "What triggers did you use to start model retraining in Step Functions and SageMaker Pipelines?",
        "answer": "We have three types of retraining triggers: scheduled, event-driven, and manual. For scheduled retraining, we use EventBridge rules to trigger Step Functions monthly. This is for routine model updates with fresh data. The Step Functions workflow has stages: data extraction from Redshift, feature engineering in Glue, data validation using Great Expectations, training on SageMaker, model evaluation, and registration in MLflow. If validation fails, the pipeline stops and alerts the team. For event-driven retraining, we monitor performance metrics and feature drift as I described earlier. When CloudWatch alarms fire due to accuracy drop or drift, they publish to an SNS topic. A Lambda subscribed to SNS triggers the Step Functions retraining workflow. This happened during COVID when claim patterns shifted abruptly. Our model detected the drift within a week, and we retrained automatically, restoring performance in 3 days. For manual retraining, data scientists trigger it via a web dashboard built on API Gateway and Lambda. This is useful for experimentation: testing new features, different algorithms, or hyperparameter tuning. The dashboard passes parameters to Step Functions, which spins up SageMaker training jobs with the specified config. We also use SageMaker Pipelines for more complex workflows. Pipelines integrate with SageMaker Feature Store, allowing us to version features consistently. For example, our fraud model uses 50 features computed in Feature Store. When we add a new feature, we update the pipeline, and it retrains automatically. SageMaker Pipelines also support conditional steps: if training accuracy exceeds 90 percent, deploy to staging; otherwise, stop. We use caching in Pipelines to avoid redundant data processing. If the input data hash has not changed, the pipeline skips data prep and uses cached results, saving time and cost. The trigger logic is in EventBridge, Step Functions, and Lambda. We log all retraining events to CloudWatch Logs with metadata: trigger type, data version, hyperparameters, and results. This creates an audit trail for compliance. The system retrains models without human intervention 80 percent of the time, freeing the team to focus on model improvement rather than operational toil."
      },
      {
        "question": "When Model Monitor detected drift, how did you identify root cause and fix it without downtime?",
        "answer": "Let me walk through a real incident. Model Monitor flagged drift in three features: claim_amount, provider_id distribution, and claim_velocity. Accuracy dropped from 88 to 76 percent over two weeks. Here is how we debugged it. First, we pulled the Model Monitor reports from S3, which showed detailed statistics. Claim_amount mean shifted from 8,500 dollars to 12,000 dollars, and provider_id had 500 new IDs not in training data. This suggested external changes, not data quality issues. We checked the source data pipeline in Glue: no errors, data was flowing correctly. Next, we queried Redshift for claims in the drift period and found a pattern: 80 percent were from a new state, Texas, where we recently launched. Our model was trained on claims from 10 states, excluding Texas. Texas has different fraud patterns, higher medical costs, and provider networks we had not seen. The model was extrapolating poorly. For the fix, we needed to retrain with Texas data, but retraining takes 6 hours and deploying requires validation. We could not have 6 hours of downtime or poor performance. Here is what we did. We deployed a blue-green strategy using SageMaker endpoint configurations. We spun up a new endpoint version with the old model still running, pulled 60 days of Texas claims from Redshift, augmented the training set, and retrained. While training, the old model kept serving traffic. After training, we validated the new model on a hold-out test set: accuracy was 87 percent, recall 85 percent, good enough. We updated the endpoint to route 10 percent of traffic to the new model using variant weights. We monitored metrics for 2 hours: the new model performed better on Texas claims without regressing on other states. We gradually shifted traffic: 50 percent, then 100 percent. The old model endpoint was deleted after 24 hours. Total downtime: zero. Performance recovered within 8 hours of detecting drift. The key was having the infrastructure for blue-green deployments and automated testing. Without it, we would have had to take the endpoint down, deploy, and hope for the best. We also updated our monitoring: we now track performance by state and alert if any state drops below thresholds, catching regional issues faster."
      },
      {
        "question": "What was your P99 latency for fraud scoring APIs and how did you achieve sub-second inference?",
        "answer": "Our SLA is P99 latency under 200 milliseconds for fraud scoring. Initially, we were at 850 milliseconds, way too high. Here is how we got to 180 milliseconds. First, we profiled the API end-to-end using X-Ray. The breakdown was: API Gateway 20 milliseconds, Lambda cold start 600 milliseconds, Lambda execution 50 milliseconds, SageMaker inference 150 milliseconds, DynamoDB write 30 milliseconds. The biggest culprit was Lambda cold starts. We fixed this by enabling provisioned concurrency for the Lambda function: 10 instances always warm, scaling up to 100 during traffic spikes. Cold start latency dropped to zero for 95 percent of requests. Cost increased by 50 dollars monthly, but latency improved dramatically. Next, we optimized the SageMaker endpoint. The XGBoost model was 200 MB, and loading it into memory took 100 milliseconds per request because we were not batching. We enabled SageMaker batch transform for offline scoring, but for real-time, we increased the instance size to ml.m5.xlarge with 16 GB memory, keeping the model resident. We also compiled the model using SageMaker Neo, which optimized it for the target instance type, reducing inference time from 150 milliseconds to 60 milliseconds. We disabled JSON serialization overhead by using binary payloads: CSV input instead of JSON reduced serialization time by 20 milliseconds. For DynamoDB, we switched from consistent reads to eventually consistent reads, cutting latency from 30 milliseconds to 10 milliseconds. This was acceptable because fraud scores do not require immediate consistency. We also implemented caching using ElastiCache Redis. We cache fraud scores for 5 minutes: if a user submits the same claim twice, we return the cached score instantly. Cache hit rate is 15 percent, saving 15 percent of SageMaker invocations. Finally, we optimized feature computation. Some features required aggregations over 90 days of history, querying Redshift, which took 200 milliseconds. We pre-computed these features nightly and stored them in DynamoDB. At inference time, we do a single DynamoDB lookup 8 milliseconds instead of querying Redshift. The tradeoff is slightly stale features, but fraud patterns do not change daily, so it is acceptable. With all optimizations, our P99 latency is now 180 milliseconds: API Gateway 20 milliseconds, Lambda 5 milliseconds, SageMaker 60 milliseconds, DynamoDB 10 milliseconds, caching and feature lookup 85 milliseconds. We monitor latency continuously in CloudWatch, and alarms fire if P99 exceeds 250 milliseconds. This has been stable for 8 months, handling 50,000 requests daily."
      },
      {
        "question": "How did you implement version control for datasets, models, and experiments?",
        "answer": "Version control is essential for reproducibility and compliance. We use a combination of tools. For datasets, we use DVC, Data Version Control, which is like Git for data. Raw data lives in S3, and DVC tracks pointers in Git. When we create a new dataset, say claims-data-v3, we run dvc add, which computes a hash and stores it in Git. The actual data stays in S3, but the hash ensures we can retrieve the exact version later. This is critical: during an audit, we needed to reproduce a model from 18 months ago. Using DVC, we checked out the Git commit, ran dvc pull, and retrieved the exact dataset, even though the raw data had been updated since. For models, we use MLflow Model Registry. Every trained model is logged with its hyperparameters, metrics, and artifacts. Models are versioned: fraud-detection-v1, v2, v3. We tag models as staging, production, or archived. When we deploy to production, we register the model version in MLflow, and the deployment script fetches it by version tag. This ensures no ambiguity: production always points to a specific model version. We also use Git for code version control. Every training script, feature engineering pipeline, and API handler is in Git. We tag releases: v1.0.0 for initial deployment, v1.1.0 for new features. The SageMaker training job logs the Git commit SHA to MLflow, linking code to models. For experiments, we use MLflow Experiments. Each experiment like fraud-hyperparameter-tuning contains multiple runs with different parameters. We compare runs using MLflow UI, which shows metrics side-by-side. We also export experiment results to CSV for presentations to stakeholders. For feature engineering, we use SageMaker Feature Store, which versions feature groups. When we add a new feature, we create a new feature group version, and all downstream models reference it by version. This prevents feature skew between training and inference. We also version Docker images. Training and inference containers are built with tags like fraud-model:v3, stored in ECR. The SageMaker job references the exact image tag, ensuring consistent environments. Finally, we maintain a changelog in Confluence documenting every model deployment: date, version, changes, and performance metrics. This is a lifesaver for audits. Every component, data, code, models, features, and containers, is versioned and tracked. The overhead is minimal, and the benefits for reproducibility and compliance are enormous."
      },
      {
        "question": "How do you handle rollbacks if a new model underperforms in production?",
        "answer": "Rollbacks are a critical safety net. We have a multi-layered strategy. First, we deploy models using SageMaker endpoint configurations with variant weights. When we deploy a new model, say fraud-detection-v4, we create a new endpoint variant alongside the existing v3. We route 10 percent of traffic to v4 and 90 percent to v3. We monitor metrics like accuracy, latency, and error rate for both variants in CloudWatch. If v4 performs well after 2 hours, we shift to 50-50, then 100 percent to v4 over 24 hours. If v4 underperforms, accuracy below 85 percent or latency above 200 milliseconds, we immediately shift traffic back to v3 by updating variant weights. This rollback takes 30 seconds, and we lose no data because both models are running. For infrastructure, we use CloudFormation with change sets. Before deploying a new SageMaker endpoint, we create a change set showing exactly what will change. If something breaks, we roll back the CloudFormation stack to the previous version, which redeploys the old endpoint. This takes 5 minutes. We also maintain model artifacts in S3 with versioning enabled. If we accidentally delete a model, we restore from S3 versions. For database schema changes, we use backward-compatible migrations. If we add a new feature column to DynamoDB, we ensure the old model can still run without it. This decouples model and schema deployments. Here is a real example. We deployed fraud-detection-v5 with a new feature: provider_network_score. In production, this feature was missing for 20 percent of claims because the upstream pipeline had a bug. The model threw errors for these claims. We detected this in CloudWatch: error rate spiked to 15 percent within 10 minutes. We rolled back traffic to v4, which did not need that feature, restoring error rate to zero. We fixed the pipeline bug, redeployed v5 the next day, and it worked. The rollback prevented downtime and incorrect predictions. We also have automated rollback triggers. A Lambda monitors CloudWatch metrics, and if error rate exceeds 10 percent or P99 latency exceeds 500 milliseconds for 5 minutes, it automatically triggers a rollback by calling the SageMaker API to update variant weights. This has saved us twice when deploys went wrong outside business hours. The key is having multiple safety layers: canary deployments, automated monitoring, fast rollback mechanisms, and backward compatibility. We can roll back within minutes, minimizing impact."
      },
      {
        "question": "Describe your CI/CD pipeline for ML model deployment using CodePipeline or Jenkins.",
        "answer": "Our CI/CD pipeline automates everything from code commit to production deployment. We use AWS CodePipeline with multiple stages. Here is the workflow. It starts with a Git commit to the main branch in CodeCommit. This triggers CodePipeline. Stage 1 is Source: CodePipeline pulls the latest code and stores it in an S3 artifact bucket. Stage 2 is Build: CodeBuild runs unit tests on the training code using pytest. If tests fail, the pipeline stops. We also run linting with flake8 and security scans with Bandit. Stage 3 is Train: CodeBuild triggers a SageMaker training job using the SageMaker Python SDK. The training job uses a custom Docker image from ECR with all dependencies. Training runs on an ml.m5.xlarge instance, and the trained model is saved to S3. We log metrics to MLflow and CloudWatch. Stage 4 is Evaluate: a Lambda function runs the trained model on a hold-out test set. It computes accuracy, precision, recall, and AUC. If accuracy is below 85 percent, the pipeline stops, and we get an SNS notification. If it passes, the model is registered in MLflow Model Registry with a staging tag. Stage 5 is Manual Approval: a data scientist reviews the model metrics in MLflow and approves or rejects deployment via the CodePipeline console. This ensures human oversight before production. Stage 6 is Deploy to Staging: CodeBuild deploys the model to a staging SageMaker endpoint. We run integration tests using a test harness that sends 100 sample requests and validates responses. Stage 7 is Deploy to Production: CodeBuild updates the production endpoint using SageMaker endpoint configurations with variant weights, routing 10 percent of traffic to the new model. We monitor for 2 hours, and if metrics look good, a Lambda automatically shifts to 100 percent. The entire pipeline, from commit to production, takes 3 hours. For infrastructure, we use CloudFormation templates stored in Git. The pipeline deploys both the model and the infrastructure as code. We also version Docker images: every build creates a new image tagged with the Git commit SHA. For rollbacks, we revert the Git commit and rerun the pipeline. We use Jenkins for on-prem components, like data extraction from legacy systems. Jenkins jobs run nightly ETL, pull data to S3, then trigger CodePipeline for retraining. The key is automation: no manual steps except approval, which reduces errors and speeds up deployments. We deploy models weekly, and the pipeline has been reliable for 18 months."
      },
      {
        "question": "How do you perform A/B testing between two SageMaker models?",
        "answer": "A/B testing is critical for validating model improvements. We use SageMaker multi-variant endpoints to split traffic between models. Here is a real example. We had fraud-detection-v3 in production with 88 percent accuracy. We developed v4 with new features and wanted to test if it improved performance. We deployed both models to a single SageMaker endpoint using variant weights: v3 gets 50 percent of traffic, v4 gets 50 percent. Each request is randomly routed to one variant. We tagged each prediction with the variant name in the response metadata and logged it to S3 via SageMaker Data Capture. For analysis, we ran a daily Glue job that joined predictions with ground truth labels from our fraud investigation system. We computed accuracy, precision, recall, false positive rate, and latency for each variant. After 7 days with 10,000 requests per variant, we had statistically significant results. V4 had 91 percent accuracy versus 88 percent for v3, and recall improved from 85 to 89 percent. False positives were slightly higher but within acceptable limits. Latency was identical. Based on this, we shifted 100 percent of traffic to v4 and deprecated v3. For statistical rigor, we used a Chi-Square test to compare accuracy between variants, with p less than 0.05. We also computed confidence intervals for precision and recall. We built a dashboard in CloudWatch using custom metrics that visualized variant performance in real-time. If v4 had underperformed, we would have rolled back by updating variant weights. For business metrics, we tracked cost savings: v4 reduced false positives by 10 percent, saving 50,000 dollars annually in wasted investigation costs. We presented this to stakeholders to justify the v4 deployment. One challenge is ensuring randomization. We hash the request ID to assign variants deterministically, ensuring the same claim always goes to the same variant. This prevents inconsistent predictions if a user retries the same request. Another challenge is sample size: we needed 10,000 requests to detect a 3 percent accuracy improvement. For low-traffic models, A/B tests take weeks. We mitigate this by running tests during high-traffic periods. A/B testing has become standard: every new model goes through it. It builds confidence and catches regressions before full rollout."
      },
      {
        "question": "How do you monitor and alert for inference failures in real-time pipelines?",
        "answer": "Inference failures can be catastrophic: claims get stuck, fraud goes undetected, and customers get angry. We monitor aggressively. First, we enable SageMaker endpoint metrics in CloudWatch: ModelLatency, Invocations, InvocationErrors. We set alarms on Invocation4XXErrors and Invocation5XXErrors. If error rate exceeds 1 percent for 5 minutes, we get paged via PagerDuty. We also monitor ModelLatency: if P99 exceeds 500 milliseconds, we investigate. For Lambda functions that invoke the endpoint, we track Lambda Errors, Duration, and Throttles. We set alarms for error spikes. For Kinesis streams, we monitor GetRecords.IteratorAgeMilliseconds, which measures how far behind consumers are. If it exceeds 60,000 milliseconds, data is piling up, indicating the endpoint is overloaded or failing. We also track custom business metrics. For fraud detection, we log every prediction to CloudWatch Logs with metadata: claim ID, fraud score, model version, and latency. A Lambda parses logs and publishes metrics like average_fraud_score and predictions_per_minute. If average fraud score shifts from 0.15 to 0.30 suddenly, something is wrong, maybe the model is broken or input data changed. We set anomaly detection alarms using CloudWatch Anomaly Detection, which learns normal patterns and alerts on deviations. For distributed tracing, we use X-Ray. Every request from API Gateway through Lambda to SageMaker is traced. If a request fails, X-Ray shows exactly which component failed and why. This is invaluable for debugging. We also implement retry logic with exponential backoff. If a SageMaker invocation fails due to throttling or transient errors, Lambda retries up to 3 times. Persistent failures go to an SQS dead-letter queue, which we monitor. If the DLQ depth exceeds 100, we get alerted. For alerting, we use a tiered approach. Low-severity issues like latency spikes send Slack notifications. High-severity issues like error rates above 5 percent page the on-call engineer. Critical issues like endpoint unavailability trigger an incident response with multiple engineers. We also have a runbook in Confluence: for each alert, there is a documented troubleshooting process. Here is a real incident. At 3 AM, we got paged: SageMaker endpoint returning 500 errors at 20 percent rate. X-Ray showed the endpoint was timing out. We checked CloudWatch Logs: the model was loading slowly due to a corrupted artifact in S3. We rolled back to the previous model version, which restored service in 5 minutes. We then investigated the corrupted artifact: a deployment script had a bug that partially uploaded the model. We fixed the script and redeployed. Without monitoring and alerting, this issue could have gone unnoticed for hours, causing massive data loss."
      },
      {
        "question": "How did you evaluate and reduce hallucinations when using Claude via Bedrock for adjuster notes?",
        "answer": "Hallucinations are a major risk in insurance: Claude might invent policy terms or claim details, leading to incorrect decisions. We tackled this with prompt engineering, retrieval, and evaluation. First, we use RAG to ground Claude in actual documents. When an adjuster asks what are the policy exclusions, we retrieve relevant sections from OpenSearch and pass them in the prompt with explicit instructions: answer only using the provided documents. If the information is not in the documents, respond I do not have that information. This reduces hallucinations significantly. Second, we tuned the prompt. Initially, Claude was creative, filling in gaps with plausible but incorrect info. We added constraints: Be concise. Do not infer beyond the provided text. If uncertain, say so explicitly. Stick to facts in the documents. We also set temperature to 0.3 for deterministic responses. Third, we implemented citation. Claude must cite the document chunk it used for each answer. We modified the prompt: For each statement, include the document ID and section number. This lets adjusters verify claims and catches hallucinations. If Claude cites a non-existent section, we know it hallucinated. For evaluation, we created a test set of 200 adjuster questions with ground truth answers. Two actuaries manually labeled correct answers using actual policy documents. We ran Claude on each question and compared outputs to ground truth. We measured factual accuracy, hallucination rate, and citation correctness. Initially, Claude hallucinated 18 percent of the time, mostly on ambiguous questions. After prompt tuning and RAG improvements, we reduced it to 6 percent. We also used LLM-as-judge. We prompted a separate Claude instance to evaluate the first Claude's output: Does the answer contradict the provided documents? Is the answer supported by evidence? This meta-evaluation caught hallucinations we missed. For production monitoring, we track user feedback. If an adjuster flags an answer as incorrect, we log it and retrain the retrieval system. We also implemented confidence scoring: Claude estimates its confidence 0 to 1. Answers with confidence below 0.7 are flagged for human review. We analyzed 1000 low-confidence responses and found 40 percent were hallucinations or uncertain. Finally, we added a disclaimer: responses are AI-generated and should be verified. Adjusters are trained to treat Claude as a research assistant, not gospel. This reduces liability. Despite these measures, hallucinations still happen. The key is layering defenses: RAG, prompt constraints, citations, evaluation, monitoring, and user training. We reduced hallucinations by 70 percent, making Claude safe for production. Continuous evaluation and iteration are essential."
      },
      {
        "question": "What was your strategy for prompt engineering and controlling model responses, for example, saying I do not know?",
        "answer": "Prompt engineering is an art. Our goal was to make Claude helpful but cautious, avoiding hallucinations and speculation. We started with a baseline prompt: You are an AI assistant for insurance adjusters. Answer questions based on policy documents. This was too vague. Claude over-inferred, giving opinions instead of facts. We refined the prompt iteratively. First, we added explicit constraints: Answer only using the provided documents. If the information is not in the documents, respond I do not have that information. Do not guess or infer beyond the text. Be concise and factual. This improved accuracy, but Claude still struggled with ambiguous questions. Next, we provided examples, few-shot learning. We included 3 examples in the prompt: a question with a clear answer, a question requiring I do not know, and a question needing multiple document citations. Claude learned the desired behavior from examples. For controlling responses, we used system and user roles carefully. The system prompt set the overall behavior: You are a cautious AI assistant. Avoid speculation. The user prompt included the question and retrieved documents. We also used delimiters: documents were wrapped in triple backticks to clearly separate them from the question. For I do not know responses, we crafted specific triggers. If no documents are retrieved, respond I do not have relevant information on that topic. If documents are retrieved but do not answer the question, respond The provided documents do not contain that information. Please consult your policy manual. We tested this with edge cases: obscure questions, multi-part questions, and questions outside the policy scope. For multi-part questions, we added: If the question has multiple parts, answer each part separately. If you can only answer some parts, clearly state which parts you cannot answer. We also tuned temperature and top-p. Initially, we used temperature 0.7, which was too creative. Lowering to 0.3 made responses more deterministic and factual. We set max_tokens to 500 to prevent overly long responses that might drift off-topic. For compliance, we added a final instruction: Do not provide legal advice or definitive interpretations. Responses are informational only. This protects us legally. We tested prompts on a validation set of 100 questions, measuring accuracy, hallucination rate, and I do not know correctness. We iterated 15 times, improving accuracy from 72 percent to 91 percent. The final prompt is 300 tokens, but it is worth it. We version prompts in Git, and every production deployment uses a specific prompt version. We also A/B tested prompts: 50 percent of traffic used the old prompt, 50 percent the new one. We measured user satisfaction and accuracy, choosing the better prompt. Prompt engineering is ongoing: as we encounter new edge cases, we refine the prompt. The key is systematic evaluation and iteration, not guessing."
      },
      {
        "question": "Explain your implementation of Model Context Protocol for multi-agent orchestration.",
        "answer": "Model Context Protocol is a framework for coordinating multiple AI agents, each with specialized roles. We implemented it for complex claim processing where no single model can handle everything. Here is our setup. We have four agents: a fraud detection agent using SageMaker XGBoost, a document extraction agent using Textract and Claude, an underwriting agent using Claude with RAG, and a pricing agent using a custom regression model. These agents need to work together, passing context and results between each other. We built the orchestration layer using Step Functions and Lambda. The workflow starts when a claim is submitted. Step Functions invokes the fraud agent first, passing the claim data. The fraud agent returns a fraud score and flags. If the score is above 0.7, we route to a high-priority queue and skip further processing. For normal claims, Step Functions invokes the document extraction agent. This agent calls Textract to OCR claim photos, then uses Claude to extract structured data: injury type, damage description, and estimated cost. The agent returns a JSON object with extracted fields. This JSON becomes the context for the next agent. Step Functions invokes the underwriting agent, passing the claim data plus extracted document fields. The underwriting agent uses RAG to retrieve policy terms, checks coverage, and determines if the claim is covered. It returns a decision: approve, deny, or manual review. Finally, Step Functions invokes the pricing agent, which estimates the claim payout based on injury severity, historical data, and policy limits. For context management, we use a shared state object in Step Functions. Each agent reads from this object, performs its task, and writes results back. The state object is versioned and logged to S3 for auditability. For inter-agent communication, we use an event-driven pattern. Agents publish events to EventBridge: fraud agent publishes fraud.score.calculated, underwriting agent publishes claim.decision.made. Other agents or downstream systems subscribe to these events. This decouples agents and allows adding new agents without changing existing ones. For error handling, if an agent fails, Step Functions retries up to 3 times with exponential backoff. If all retries fail, the workflow routes to a manual review queue, and we get alerted. We also implemented checkpoints: if the workflow fails at step 3, we can resume from step 3 without reprocessing steps 1 and 2. For monitoring, we use CloudWatch and X-Ray. Every agent invocation is traced, showing latency, errors, and data flow. We built a dashboard visualizing the entire workflow: claim submission, agent invocations, decisions, and final outcome. The MCP pattern has been powerful: we process 5,000 claims daily with 95 percent automation, only 5 percent require manual review. Adding new agents is easy: deploy the agent, add a step in Step Functions, and update the state schema. The key is having a clear protocol for context passing and error handling. Without it, multi-agent systems become spaghetti code."
      },
      {
        "question": "How do agents communicate and recover when one fails in a multi-agent pipeline?",
        "answer": "Agent failures are inevitable: API throttling, transient errors, or bugs. We designed for resilience. For communication, agents do not call each other directly. Instead, they use async messaging via SQS and EventBridge. Here is the flow. The fraud agent completes its task and publishes an event to EventBridge: fraud.score.calculated with the claim ID and fraud score. The underwriting agent subscribes to this event. EventBridge triggers a Lambda that invokes the underwriting agent. If the underwriting agent is busy or throttled, the event goes to an SQS queue, and the Lambda retries later. This decoupling prevents cascading failures: if the underwriting agent is down, the fraud agent continues working. For retries, we use exponential backoff. If an agent fails, Lambda retries after 1 second, then 2, 4, 8, up to 64 seconds. After 5 retries, the task goes to a dead-letter queue. We monitor the DLQ depth: if it exceeds 50, we get paged. For idempotency, every agent operation is idempotent. If we retry a fraud score calculation, the agent checks DynamoDB: is there already a score for this claim ID? If yes, return the cached score. If no, compute and store. This prevents duplicate processing and ensures consistency. For state management, we use DynamoDB as a central state store. Each claim has a record with its current status: pending, fraud_checked, underwriting_done, priced. Agents update this status atomically using DynamoDB conditional writes. If two agents try to update simultaneously, one fails and retries. For recovery, we implemented a dead-letter queue handler. A Lambda monitors the DLQ, inspects failed tasks, and attempts to fix them. For example, if an agent failed due to a missing field, the handler fills in a default value and retries. If the error is unfixable, the handler routes the claim to manual review and notifies the team. We also implemented circuit breakers. If an agent fails repeatedly, say Bedrock is timing out, we open the circuit: stop invoking Bedrock for 5 minutes, route claims to a fallback path like simplified underwriting without RAG. After 5 minutes, we retry Bedrock. If it works, close the circuit. This prevents overwhelming a failing service. For monitoring, we track agent success rate, latency, and error types in CloudWatch. We set alarms for error rates above 5 percent. We also use X-Ray to trace end-to-end workflows: if a claim takes 10 minutes to process, X-Ray shows which agent was slow. Here is a real incident. Bedrock hit rate limits, causing the underwriting agent to fail. Within 2 minutes, we had 500 failed tasks in the DLQ. The circuit breaker kicked in, routing claims to a simplified underwriting path using a rule-based system. We contacted AWS support, got rate limits increased, and restored Bedrock after 30 minutes. We then reprocessed the DLQ tasks. Total claims lost: zero. The key is designing for failure: async messaging, retries, idempotency, state management, circuit breakers, and monitoring. Multi-agent systems are complex, but with these patterns, they are robust."
      },
      {
        "question": "Why did you choose to build custom RAG with LangChain instead of using Bedrock Knowledge Bases?",
        "answer": "We evaluated both. Bedrock Knowledge Bases are great for quick prototypes: upload documents, Bedrock handles chunking, embeddings, and vector search. But we needed more control. First, our chunking requirements are complex. Insurance policies have nested sections, cross-references, and exclusions that modify main clauses. Bedrock's default chunking is semantic or fixed-size, which breaks these relationships. We built custom chunking in LangChain that respects document structure: each section is a chunk, and we preserve parent-child relationships in metadata. This improved retrieval accuracy by 30 percent. Second, we needed hybrid search: dense vector search for semantic matching plus sparse BM25 for keyword matching. Bedrock Knowledge Bases only support dense search. We integrated OpenSearch with both vector and BM25, then blended results using rank fusion. This was critical for queries like what are exclusions for water damage, where both semantics and exact keyword matching matter. Third, we needed fine-grained access control. Our system is multi-tenant: policyholders should only see their own documents. Bedrock Knowledge Bases support metadata filters, but we needed more: role-based access, policy-level permissions, and audit logging. We implemented this in Lambda with checks against DynamoDB before querying OpenSearch. Fourth, we needed custom reranking. After retrieval, we rerank using a cross-encoder model that scores query-document relevance. This improved Precision at 5 from 0.6 to 0.8. Bedrock does not support custom reranking. Fifth, cost. Bedrock Knowledge Bases charge per query and per document indexed. For 5 million documents and 10,000 queries daily, Bedrock would cost 3,000 dollars monthly. Our custom RAG with OpenSearch costs 1,200 dollars monthly. The tradeoff is operational overhead: we manage OpenSearch, Lambda, and embeddings. But we have 4 engineers, and the control is worth it. Sixth, we needed to integrate with existing systems: Redshift for analytics, DynamoDB for metadata, and SageMaker for embeddings. Bedrock Knowledge Bases is a closed ecosystem. LangChain gave us flexibility to plug in any component. That said, Bedrock Knowledge Bases improved: they added metadata filters, hybrid search, and custom chunking. If starting today, we might use Bedrock for simpler use cases. But for production at scale with complex requirements, custom RAG with LangChain was the right choice. We iterate faster, optimize costs, and have full control."
      },
      {
        "question": "How did you handle long-context summarization with 200K token limits using Claude?",
        "answer": "Claude Opus has a 200,000 token context window, which is massive, but some policy documents exceed even that. A master insurance policy with annexes, endorsements, and claims history can be 300,000 tokens. Here is how we handled it. First, we chunk the document hierarchically. The policy has sections: coverage, exclusions, conditions, endorsements. We split at section boundaries, creating chunks of 20,000 to 50,000 tokens each. For shorter queries, we retrieve only relevant sections, avoiding the need to process the entire policy. For example, if the user asks about deductibles, we retrieve the coverage section, not the entire policy. This fits within context limits 95 percent of the time. For cases where the user needs a full policy summary, we use map-reduce summarization. We split the policy into 10 chunks of 30,000 tokens each. We invoke Claude 10 times in parallel, each summarizing one chunk. Each summary is 2,000 tokens. We then concatenate the summaries into a 20,000 token document and invoke Claude again to create a final summary of 3,000 tokens. This approach is robust but expensive: 11 Bedrock invocations. To optimize, we cache intermediate summaries in S3. If the policy has not changed, we reuse cached summaries. Second, we use prompt compression. For long documents, we remove boilerplate: legal disclaimers, standard clauses, and redundant text. We built a preprocessing step using regex and NLP to strip out non-essential text, reducing token count by 40 percent. We also summarize tables: instead of passing a 50-row table, we pass a 5-sentence summary. Third, we experimented with sliding windows. For long claims history, we process the first 100,000 tokens, extract key points, then process the next 100,000 tokens, appending to the key points. This avoids exceeding context limits. Claude maintains coherence across windows because we pass the key points as context. Fourth, we explored alternative models. For use cases not requiring Claude's depth, we use Claude Haiku with a 100,000 token limit, which is cheaper and faster. Haiku works well for straightforward summarization. For extremely long documents, we considered Anthropic's long-context fine-tuning, but it was overkill for our needs. Finally, we implemented a hybrid approach: RAG plus summarization. Instead of summarizing the entire policy, we retrieve the most relevant sections using RAG, then summarize those. This balances coverage and cost. In production, 92 percent of queries fit within the 200,000 token context using retrieval and chunking. The remaining 8 percent use map-reduce. The key is designing workflows that avoid hitting context limits rather than relying on the limit itself. Context is expensive: every token costs money, so we minimize token usage without sacrificing quality."
      },
      {
        "question": "How did you evaluate LLM output quality for compliance-sensitive insurance documents?",
        "answer": "Evaluating LLM output in insurance is challenging: hallucinations, bias, or errors can have legal consequences. We use a multi-layered evaluation approach. First, we created a golden dataset. Two actuaries manually annotated 300 questions with ground truth answers using actual policy documents. Questions covered coverage, exclusions, claims process, and edge cases. This dataset is our north star for evaluation. Second, we measure factual accuracy. We run Claude on each question and compare outputs to ground truth. We use exact match, F1 score on tokens, and BERTScore for semantic similarity. Initially, Claude achieved 68 percent exact match and 0.82 BERTScore. After prompt tuning and RAG improvements, we reached 89 percent exact match and 0.94 BERTScore. Third, we evaluate hallucination rate. We manually reviewed 200 responses, labeling hallucinations: statements not supported by retrieved documents. We computed hallucination rate: initially 15 percent, now 4 percent. We also use LLM-as-judge: a separate Claude instance evaluates whether the first Claude's answer is supported by the documents. This automated hallucination detection achieves 85 percent accuracy compared to human labels. Fourth, we measure citation accuracy. Claude must cite sources for each statement. We check if cited documents actually support the statement. Initially, 25 percent of citations were incorrect or missing. We improved prompts and retrieval, reducing incorrect citations to 8 percent. Fifth, we evaluate compliance. We have a checklist: no legal advice, no speculation, clear disclaimers, and appropriate I do not know responses. We manually review 100 responses monthly for compliance. We also conduct adversarial testing: we ask tricky or ambiguous questions designed to elicit hallucinations or inappropriate responses. For example, can I sue my insurer? Claude should not provide legal advice. We document failures and refine prompts. Sixth, we collect user feedback. Adjusters rate responses thumbs up or thumbs down. We log feedback and analyze patterns. If a topic has low ratings, we investigate and improve retrieval or prompts. Seventh, we use business metrics. We track claim processing time: RAG reduced it from 45 minutes to 12 minutes per claim. We also track incorrect decisions: claims wrongly denied or approved. RAG reduced errors from 8 percent to 3 percent, saving 500,000 dollars annually. Finally, we conduct quarterly audits. A compliance officer reviews 50 random RAG responses for accuracy, compliance, and appropriateness. This ensures continuous quality. Evaluation is not one-time: we run tests after every model update, prompt change, or data refresh. Without rigorous evaluation, we would not trust Claude in production. The key is combining automated metrics with human review and business impact analysis."
      }
    ]
  }
